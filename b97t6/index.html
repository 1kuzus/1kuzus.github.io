<!doctype html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><script async src="https://www.googletagmanager.com/gtag/js?id=G-45BYSZ6WPY"></script><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-45BYSZ6WPY")</script><title>铃木的网络日记</title><script defer="defer" src="/static/js/main.8e48f425.js"></script><link href="/static/css/main.39d68e71.css" rel="stylesheet"></head><body><noscript><div><h1>行为识别R(2+1)D网络</h1><p>论文<a href="https://arxiv.org/pdf/1711.11248.pdf" target="_blank" rel="noreferrer">A Closer Look at Spatiotemporal Convolutions for ActionRecognition</a>讨论了几种用于行为识别的时空卷积网络。文中提出了R(2+1)D网络，即将R3D网络中的3D卷积拆分成2D空间卷积+1D时间卷积。</p><p>PyTorch版本复现代码来自于Github仓库 <a href="https://github.com/irhum/R2Plus1D-PyTorch" target="_blank" rel="noreferrer">Github:R2Plus1D-PyTorch</a></p><h2>(2+1)D卷积</h2><p>普通3D卷积的核大小为$(C_i,C_o,K_t,K_w,K_h)$，$C_i,C_o$为输入通道数、$K_t,K_w,K_h$是3D卷积核的尺寸。<br/>R(2+1)D网络最核心的改动就是将普通的Conv3d替换为SpatioTemporalConv卷积。</p><div>[IMAGE]</div><p>SpatioTemporalConv卷积从外部看与Conv3d相同，均有5个超参数，区别在于其内部的结构：</p><div>大小为$(C_i,C,1,K_w,K_h)$的`Conv3d`</div><div>`BatchNorm`+`ReLU`</div><div>大小为$(C,C_o,K_t,1,1)$的`Conv3d`</div><p>如果二者作用于相同的输出，得到的结果shape是一样的：</p><div>[CODEBLOCK]</div><p>注意到拆分成两次卷积后，会出现一个中间通道数$C$。论文中给出估计$C$值的公式是：</p>C=C_i C_oK_tK_wK_h/(C_iK_wK_h+C_oK_t)<p>这样做的目的是使得R(2+1)D卷积的参数量和R3D相近。也就是说，拆分时间和空间两次卷积并不是为了减少计算量。文中给出了使用R(2+1)D卷积两个优点：</p><div>增加了网络非线性层的层数</div><div>优化更容易（实验结果：R(2+1)D网络的训练和测试误差都更小）</div><div>[IMAGE]</div></div></noscript><div id="root"></div></body></html>