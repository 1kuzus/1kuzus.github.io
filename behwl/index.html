<!doctype html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><script async src="https://www.googletagmanager.com/gtag/js?id=G-45BYSZ6WPY"></script><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-45BYSZ6WPY")</script><title>铃木的网络日记</title><script defer="defer" src="/static/js/main.8e48f425.js"></script><link href="/static/css/main.39d68e71.css" rel="stylesheet"></head><body><noscript><div><h1>【模式识别】统计决策方法</h1><h2>贝叶斯公式</h2><div>P(\omega_i|x) = \frac{P(x|\omega_i) \cdot P(\omega_i)}{P(x)}<p>$P(\omega_i)$为先验概率，表示没有进行任何观测时的主观推测概率<br/>$P(x|\omega_i)$为类条件密度，已知<br/>$P(\omega_i|x)$为后验概率，希望得到其值，并用于决策</p></div><p>考虑如下例子，记A为抓到方形，B为抓到实心图形：</p><div className="behwl-boxes"><div className="behwl-box behwl-fill" /><div className="behwl-box" /><div className="behwl-box" /><div className="behwl-box behwl-circle behwl-fill" /><div className="behwl-box behwl-circle" /></div><div></div><div>P(AB) = \frac{1}{5}</div><div>P(A|B，抓到实心图形时抓到的是方形) = \frac{1}{2}</div><div>P(B|A，抓到方形时抓到的是实心图形) = \frac{1}{3}</div><p><br/>结合上述例子理解贝叶斯公式的推导过程：</p>P(AB) = P(A|B) \cdot P(B)P(BA) = P(B|A) \cdot P(A)P(AB) = P(BA) \; \Rightarrow \; P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}<h2>最小错误率贝叶斯决策</h2><p>研究的类别有c个。</p><div>若P(\omega_i|x) = \max_{j=1,...,c} P(\omega_j|x)，则x属于\omega_i类</div><h3>两分类情况下，最小错误率决策的四种等价规则</h3><div><p>后验概率判决</p>P(\omega_1|x) > P(\omega_2|x)</div><div><p>后验概率判决，分母相同看分子</p>P(x|\omega_1)P(\omega_1) > P(x|\omega_2)P(\omega_2)</div><div><p>似然比$l$、似然比阈值$\lambda$</p>l(x) = \frac{P(x|\omega_1)}{P(x|\omega_2)}，\lambda = \frac{P(\omega_2)}{P(\omega_1)}，l(x)>\lambda选\omega_1</div><div><p>对数似然比</p>h(x)=-\ln [l(x)] \; 与 \; \ln\frac{P(\omega_1)}{P(\omega_2)} \; 比较，h(x)小选\omega_1</div><div><p>可以把每一类的后验概率$P(\omega_i|x)$或者$P(x|\omega_i)P(\omega_i)$看作该类的一个判别函数$g(x)$，决策的过程就是各类的判别函数比较大小</p></div><h2>最小风险贝叶斯决策</h2><h3>条件风险</h3>R(\alpha_i|x) = \sum_j\lambda(\alpha_i,\omega_j)P(\omega_j|x)<p>$\lambda(\alpha_i,\omega_j)$表示样本$x \in \omega_j$但被决策为$\omega_i$类的损失，$i=j$表示正确决策</p><h3>最小风险贝叶斯决策</h3><p>研究的类别有c个，做了k个决策。</p><div>若R(\alpha_i|x) = \min_{j=1,...,k} R(\alpha_j|x)，则采用决策\alpha_i，即x属于\omega_i类</div><h3>两分类情况下的最小风险贝叶斯决策</h3><p>简记$\lambda(\alpha_i,\omega_j)$为$[backslash]lambda_{ij}$：</p>R(\alpha_1|x) = \lambda_{11}P(\omega_1|x) + \lambda_{12}P(\omega_2|x)R(\alpha_2|x) = \lambda_{21}P(\omega_1|x) + \lambda_{22}P(\omega_2|x)若R(\alpha_1|x) < R(\alpha_2|x)则x属于\omega_1类<h3>最小风险贝叶斯决策的另两种形式</h3>若(\lambda_{21} - \lambda_{11})P(\omega_1|x) > (\lambda_{12} - \lambda_{22})P(\omega_2|x)，则决策x \in \omega_1若l(x)=\frac{P(x|\omega_1)}{P(x|\omega_2)} > \frac{\lambda_{12} - \lambda_{22}}{\lambda_{21} - \lambda_{11}} \cdot \frac{P(\omega_2)}{P(\omega_1)}，则决策x \in \omega_1<div>设损失函数为\lambda(\alpha_i,\omega_j)=\begin{cases}0 \quad i=j \\ 1 \quad i \neq j\end{cases}<p>最小错误率贝叶斯决策就是0/1损失函数条件下的最小风险贝叶斯决策。</p></div><h2>正态分布的统计决策</h2><h3>从一维正态分布到高维正态分布</h3><p>单变量正态分布函数：</p>P(x) = \frac{1}{\sqrt{2\pi}\sigma} \exp\{-\frac{1}{2} \cdot (\frac{x-\mu}{\sigma})^2\}<p>双变量正态分布函数：</p>P(x_1,x_2) = \frac{1}{2\pi\sigma_1\sigma_2\sqrt{1-\rho^2}} \exp\{-\frac{1}{2} \cdot \frac{1}{1-\rho^2} \cdot [(\frac{x_1-\mu_1}{\sigma_1})^2 - 2\rho\frac{(x_1-\mu_1)(x_2-\mu_2)}{\sigma_1\sigma_2} + (\frac{x_2-\mu_2}{\sigma_2})^2]\}<p>对于上述二维正态分布，记：</p>\bm{X}=\begin{bmatrix}x_1 \\x_2\end{bmatrix}，\bm{\mu}=\begin{bmatrix}\mu_1 \\\mu_2\end{bmatrix}，\bm{\Sigma}=\begin{bmatrix}\sigma_1^2 & \rho\sigma_1\sigma_2 \\\rho\sigma_2\sigma_1 & \sigma_2^2\end{bmatrix}<p>则用矩阵形式表示为：</p><div>P(\bm{X}) = \frac{1}{(2\pi)^{\frac{d}{2}}|\bm{\Sigma}|^\frac{1}{2}} \exp \{-\frac{1}{2} \cdot (\bm{X}-\bm{\mu})^T \bm{\Sigma}^{-1} (\bm{X}-\bm{\mu}) \}<p>此式通用于高维正态分布。</p></div><p>理解这个长公式，先从两个概念入手：$\Sigma$矩阵代表什么？$\rho$似乎没有在通式中体现，其含义是什么？<br/>对于二维正态分布例子中的两个变量来说，他们的协方差定义为：</p>cov(x_1,x_2)=E[(x_1-\mu_1)(x_2-\mu_2)]=E(x_1x_2)-\mu_1\mu_2<p>$\Sigma$矩阵的含义其实是两个变量的协方差矩阵，也就是：</p>\bm{\Sigma} =\begin{bmatrix}cov(x_1,x_1) & cov(x_1,x_2) \\cov(x_2,x_1) & cov(x_2,x_2)\end{bmatrix}，有cov(x_1,x_1)=\sigma_1^2<p>$\rho$的含义为两个变量的相关系数，以上述变量x1,x2为例，计算公式为：</p>\rho = \frac{cov(x_1,x_2)}{\sigma_1\sigma_2} \; \Rightarrow \; cov(x_1,x_2)=\rho\sigma_1\sigma_2<p>至此就可以理解$\Sigma$,$\rho$两个参数的含义了！</p><h3>正态分布概率模型下的最小错误率贝叶斯决策</h3>P(\bm{X}|\omega_i) = \frac{1}{(2\pi)^{\frac{d}{2}}|\bm{\Sigma_i}|^\frac{1}{2}} \exp \{-\frac{1}{2} \cdot (\bm{X}-\bm{\mu_i})^T \bm{\Sigma_i}^{-1} (\bm{X}-\bm{\mu_i})\}<p>注意这个类概率密度函数$P([backslash]bm{X}|[backslash]omega_i)$的含义为，第$\omega_i$类的样本$[backslash]bm{X}$的概率密度。<br/>使用后验概率的变形作为判别函数：</p>g_i(\bm{X}) = \ln [P(\bm{X}|\omega_i)P(\omega_i)] =-\frac{d}{2}\ln 2\pi -\frac{1}{2}\ln |\bm{\Sigma_i}| -\frac{1}{2}(\bm{X}-\bm{\mu_i})^T\bm{\Sigma_i}^{-1}(\bm{X}-\bm{\mu_i}) +\ln P(\omega_i)<p>从以下三种情况考虑决策面：</p><div>\bm{\Sigma_1} = \bm{\Sigma_2}=...=\bm{\Sigma_c} = \sigma^2\bm{I}</div><p>各类模式分布的协方差矩阵相等，样本统计独立且方差相同，协方差均为0。<br/>此时$g([backslash]bm{X})$的前两项与类别无关，后两项化简为：</p>g_i(\bm{X}) = -\frac{1}{2\sigma^2}\Vert\bm{X}-\bm{\mu_i}\Vert^2 +\ln P(\omega_i)<div><p>如果先验概率相等，则决策只与欧氏距离有关。此时决策为：</p>若\Vert\bm{X}-\bm{\mu_i}\Vert^2 = \min_{j=1,...,c} \Vert\bm{X}-\bm{\mu_j}\Vert^2，则x属于\omega_i类</div><div><p>从几何的视角来看，以上决策规则实际就是比较样本点和各类的中心点距离，并且选择距离最近的类别作为决策结果。<br/>以上分类器也称最小距离分类器，把每个均值看作一个典型的样本，则这种分类方法也称为模板匹配技术。</p></div><p>如果对于上述判别函数$g([backslash]bm{X})$的欧式距离项展开，并删掉与类别无关的二次项，得：</p>g_i(\bm{X}) = \bm{W_i}^T\bm{X} + b，其中\bm{W_i} = \frac{1}{\sigma^2}\bm{\mu_i}，b = -\frac{1}{2\sigma^2}\bm{\mu_i}^T\bm{\mu_i} + \ln P(\omega_i)<p>判别函数是$[backslash]bm{X}$的线性函数，称为线性分类器。<br/>接下来考虑决策面方程：</p>g_i(\bm{X}) = g_j(\bm{X}) \; \Rightarrow \; \bm{W}^T(\bm{X}-\bm{X_0}) = 0其中\bm{W} = \bm{\mu_i} - \bm{\mu_j}，\bm{X_0} = \frac{1}{2}(\bm{\mu_i} + \bm{\mu_j}) - \sigma^2\frac{\bm{\mu_i}-\bm{\mu_j}}{\Vert \bm{\mu_i}-\bm{\mu_j} \Vert^2}\ln\frac{P(\omega_i)}{P(\omega_j)}<p>这个方程确定了决策面是通过$[backslash]bm{X}_0$并正交于向量$[backslash]bm{W}$的一个超平面。如果是二维平面上的点的分类问题，决策线过$[backslash]bm{X}_0$点并且垂直于样本中心的连线。当先验概率相等时，$[backslash]bm{X}_0$的后项为0，此时决策线就是样本中心连线的中垂线。<br/>来看一个具体的例子！我们需要对平面上的点进行分类任务，第一堆样本点中心为(2,3)，第二堆样本点中心为(4,4)：</p><div>[IMAGE]</div><p>红色和蓝色分别标记了两类样本点的分布情况，其中加粗的红点和蓝点表示样本中心的位置；<br/>生成两组样本时使用的方差均为0.8；样本的$x,y$坐标相关性为0。<br/>现在对平面上的所有点计算判别函数g_red和g_blue。g_red更大的区域用红色阴影表示，g_blue更大的区域用蓝色阴影表示。<br/>两个区域的交界处即为自然生成的决策线。结果如下：</p><div><div>[IMAGE]</div><div>[IMAGE]</div><div>[IMAGE]</div></div><p>决策线垂直于样本中心的连线并且在先验概率相等的前提下过样本中心连线中点(左图)。<br/>如果先验概率不相等，决策线会偏向先验概率小的一侧(中图)，而且有可能超过端点(右图)。</p><p>对于三分类问题，得到的结果类似：</p><div>[IMAGE]</div><div>\bm{\Sigma_1} = \bm{\Sigma_2}=...=\bm{\Sigma_c} = \bm{\Sigma}</div><div><h4>马氏距离</h4><p>马氏距离可以看作对欧氏距离的修正。考虑下面的例子：<br/>黑色的点距离green类样本中心更近，与red、blue类样本中心等距。它应该被归为哪一类？<br/></p><div>[IMAGE]</div><p>按照欧氏距离判别，它应该被归为green类；然而从直觉上判断，它更可能属于red类。<br/>欧式距离并没有考虑样本的方差，以及样本各个维度之间的相关性。<br/>定义马氏距离：</p>r^2 = (\bm{X}-\bm{\mu})^T \bm{\Sigma}^{-1} (\bm{X}-\bm{\mu})<p>如果样本是二维的，且协方差矩阵$\sigma$为对角矩阵，马氏距离表示为：</p>r^2 =\begin{bmatrix}x_1 - \mu_1 & x_2 - \mu_2\end{bmatrix}\begin{bmatrix}1/\sigma_1^2 & 0 \\0 & 1/\sigma_2^2\end{bmatrix}\begin{bmatrix}x_1 - \mu_1 \\x_2 - \mu_2\end{bmatrix}=\frac{(x_1-\mu_1)^2}{\sigma_1^2} + \frac{(x_2-\mu_2)^2}{\sigma_2^2}<p>如果样本方差一样，则就是欧式距离；如果方差不一样，可以看作是标准化之后的欧氏距离。</p></div><p>判别函数：</p>g_i(\bm{X}) = -\frac{1}{2}r_i^2 + \ln P(\omega_i)<p>决策面方程：</p>\bm{W}^T(\bm{X}-\bm{X_0}) = 0其中\bm{W} =\bm{\Sigma}^{-1} (\bm{\mu_i} - \bm{\mu_j})，\bm{X_0} = \frac{1}{2}(\bm{\mu_i} + \bm{\mu_j}) - \frac{\bm{\mu_i}-\bm{\mu_j}}{(\bm{\mu_i}-\bm{\mu_j})^T \bm{\Sigma}^{-1} (\bm{\mu_i}-\bm{\mu_j})}\ln\frac{P(\omega_i)}{P(\omega_j)}<p>此时，先验概率相等的前提下，决策线仍然过样本中心连线中点；但不一定垂直于样本中心的连线。</p><div>[IMAGE]</div><div>\bm{\Sigma_i} \neq \bm{\Sigma_j}</div><p>此为最一般情况。决策面为超二次曲面。对于二维样本，决策线为二次曲线。<br/>判别函数：</p>g_i(\bm{X}) = -\frac{1}{2}r_i^2 - \frac{1}{2}\ln |\bm{\Sigma_i}| + \ln P(\omega_i)<div><div>[IMAGE]</div><div>[IMAGE]</div><div>[IMAGE]</div></div></div></noscript><div id="root"></div></body></html>