2:I[4500,["2202","static/chunks/2202-efd885462b29519a.js","244","static/chunks/244-cd228004bd8a77c9.js","3487","static/chunks/app/(blogs)/longtime/papers/page-aa28a2df5ada86fc.js"],""]
4:I[4365,["2202","static/chunks/2202-efd885462b29519a.js","244","static/chunks/244-cd228004bd8a77c9.js","3487","static/chunks/app/(blogs)/longtime/papers/page-aa28a2df5ada86fc.js"],""]
5:I[5613,[],""]
6:I[1778,[],""]
7:I[9806,["8792","static/chunks/8792-72030bcaaad25c8d.js","9135","static/chunks/app/(blogs)/layout-f0f62162d7e2bce9.js"],""]
8:I[3393,["8792","static/chunks/8792-72030bcaaad25c8d.js","9135","static/chunks/app/(blogs)/layout-f0f62162d7e2bce9.js"],""]
9:I[5694,["8792","static/chunks/8792-72030bcaaad25c8d.js","3185","static/chunks/app/layout-7a07309042b190b6.js"],"GlobalProvider"]
a:I[397,["8792","static/chunks/8792-72030bcaaad25c8d.js","3185","static/chunks/app/layout-7a07309042b190b6.js"],""]
3:T645,从<code class="x-inline-highlight">(<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">ϕ</span></span></span></span>)</code>到<code class="x-inline-highlight">(<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">G</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span></span>)</code>0:["Z88cIXpAGJTeT4Lmz41Ex",[[["",{"children":["(blogs)",{"children":["longtime",{"children":["papers",{"children":["__PAGE__",{}]}]}]}]},"$undefined","$undefined",true],["",{"children":["(blogs)",{"children":["longtime",{"children":["papers",{"children":["__PAGE__",{},["$L1",[["$","h1",null,{"className":"x-title","children":"论文速记"}],["$","h2",null,{"className":"x-h1","children":["$","$L2",null,{"excludeFromContents":"$undefined","children":"研究"}]}],["$","h3",null,{"className":"x-h2","children":["$","$L2",null,{"excludeFromContents":"$undefined","children":["$","a",null,{"href":"https://arxiv.org/pdf/2003.08934.pdf","target":"_blank","rel":"noreferrer","children":"【NeRF】NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis (2020)"}]}]}],["$","div",null,{"className":"x-highlightblock highlight-background-gray","children":[false,[["$","div",null,{"className":"x-oli","children":[["$","div",null,{"className":"x-oli-number","children":"1."}],["$","div",null,{"className":"x-oli-content-wrapper","children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">文章的主题 / 文章要解决什么问题？</span>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"使用稀疏输入2D图片集实现场景的3D视图合成"}}]]}]]}],["$","div",null,{"className":"x-oli","children":[["$","div",null,{"className":"x-oli-number","children":"2."}],["$","div",null,{"className":"x-oli-content-wrapper","children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">文章的核心方法 / 具体是如何做的？</span>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"$3"}}]]}]]}],["$","div",null,{"className":"x-oli","children":[["$","div",null,{"className":"x-oli-number","children":"3."}],["$","div",null,{"className":"x-oli-content-wrapper","children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">做了什么实验，效果怎么样？</span>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"测试数据集是<code class=\"x-inline-highlight\">Diffuse Synthetic 360</code>、<code class=\"x-inline-highlight\">Realistic Synthetic 360</code>和<code class=\"x-inline-highlight\">Real Forward-Facing</code>"}}]]}]]}],["$","div",null,{"className":"x-oli","children":[["$","div",null,{"className":"x-oli-number","children":"4."}],["$","div",null,{"className":"x-oli-content-wrapper","children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">研究的创新点</span>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"将输入坐标位置编码，帮助MLP表示高频函数<br/>分层采样"}}]]}]]}],["$","div",null,{"className":"x-oli","children":[["$","div",null,{"className":"x-oli-number","children":"5."}],["$","div",null,{"className":"x-oli-content-wrapper","children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">有什么限制或可以改进的地方？</span>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"有效地优化和渲染神经辐射场<br/>可解释性"}}]]}]]}]]]}],["$","div",null,{"className":"x-highlightblock highlight-background-blue","children":[["$","h4",null,{"className":"x-h3","children":"更多笔记"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"神经辐射场用于从2D的图片重建3D的场景。"}}],["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"文中出现的三个指标：PSNR、SSIM、LPIPS"}}],["$","div",null,{"className":"x-uli","children":[["$","div",null,{"className":"x-uli-marker","children":["$","div",null,{"className":"x-uli-marker-dot"}]}],["$","div",null,{"className":"x-uli-content-wrapper","children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">峰值信噪比</span><code class=\"x-inline-highlight\">(Peak Signal to Noise Ratio, PSNR)</code>：用于衡量图像恢复的质量，数值越高表示图像质量越好。接近<code class=\"x-inline-highlight\">50 dB</code>代表误差非常小，大于<code class=\"x-inline-highlight\">30 dB</code>人眼难察觉差异。"}}]}]]}],["$","div",null,{"className":"x-uli","children":[["$","div",null,{"className":"x-uli-marker","children":["$","div",null,{"className":"x-uli-marker-dot"}]}],["$","div",null,{"className":"x-uli-content-wrapper","children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">结构相似性</span><code class=\"x-inline-highlight\">(Structural Similarity Index Measure, SSIM)</code>：用于衡量图像的结构相似性，得分通常在<code class=\"x-inline-highlight\">0</code>~<code class=\"x-inline-highlight\">1</code>之间，数值越高表示图像结构越相似。相较于PSNR在图像质量的衡量上更能符合人眼对图像质量的判断。"}}]}]]}],["$","div",null,{"className":"x-uli","children":[["$","div",null,{"className":"x-uli-marker","children":["$","div",null,{"className":"x-uli-marker-dot"}]}],["$","div",null,{"className":"x-uli-content-wrapper","children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">基于学习的感知图像质量评价</span><code class=\"x-inline-highlight\">(Learned Perceptual Image Patch Similarity, LPIPS)</code>：测量从预训练网络中提取的两个图像的特征之间的相似性，得分通常在<code class=\"x-inline-highlight\">0</code>~<code class=\"x-inline-highlight\">1</code>之间，数值越低表示感知质量越高。"}}]}]]}]]}],["$","h3",null,{"className":"x-h2","children":["$","$L2",null,{"excludeFromContents":"$undefined","children":["$","a",null,{"href":"https://arxiv.org/pdf/2308.04079.pdf","target":"_blank","rel":"noreferrer","children":"【3DGS】3D Gaussian Splatting for Real-Time Radiance Field Rendering (2023)"}]}]}],["$","div",null,{"className":"x-highlightblock highlight-background-gray","children":[false,[["$","div",null,{"className":"x-oli","children":[["$","div",null,{"className":"x-oli-number","children":"1."}],["$","div",null,{"className":"x-oli-content-wrapper","children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">文章的主题 / 文章要解决什么问题？</span>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"实现实时辐射场渲染，同时保持高质量的视觉效果，并且保持较短的训练时间"}}]]}]]}],"$undefined","$undefined","$undefined","$undefined"]]}],["$","div",null,{"className":"x-highlightblock highlight-background-blue","children":[["$","h4",null,{"className":"x-h3","children":"更多笔记"}],["$","h4",null,{"className":"x-h3","children":"文章的相关工作部分"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"传统的场景重建与渲染：基于光场的，密集采样、非结构化捕获；<span class=\"x-inline-strong\">运动恢复结构</span><code class=\"x-inline-highlight\">(Structure from Motion, SFM)</code>用一组照片估计稀疏点云合成新视图；<span class=\"x-inline-strong\">多视点立体视觉</span><code class=\"x-inline-highlight\">(Multi-View Stereo, MVS)</code>；<br/> 神经渲染和辐射场：用CNN估计混合权重，用于纹理空间；Soft3D提出<code class=\"x-inline-highlight\">Volumetric representations</code>；NeRF提出重要性采样和位置编码来提高质量，但使用了大型多层感知器，对速度有负面影响；"}}],["$","h4",null,{"className":"x-h3","children":"稀疏重建和稠密重建"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"稀疏重建主要用于定位，得到每张图片的相机参数，提取特征点，例如SFM；稠密重建是假设相机参数已知的情况下，从不同视角的图像中找到匹配的对应点，对整个图像或图像中绝大部分像素进行重建。"}}]]}],["$","h3",null,{"className":"x-h2","children":["$","$L2",null,{"excludeFromContents":"$undefined","children":["$","a",null,{"href":"https://arxiv.org/pdf/2201.05989.pdf","target":"_blank","rel":"noreferrer","children":"【Instant NGP】Instant Neural Graphics Primitives with a Multiresolution Hash Encoding (2022)"}]}]}],["$","div",null,{"className":"x-highlightblock highlight-background-gray","children":[false,[["$","div",null,{"className":"x-oli","children":[["$","div",null,{"className":"x-oli-number","children":"1."}],["$","div",null,{"className":"x-oli-content-wrapper","children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">文章的主题 / 文章要解决什么问题？</span>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"实现实时辐射场渲染，同时保持高质量的视觉效果，并且保持较短的训练时间"}}]]}]]}],"$undefined","$undefined","$undefined","$undefined"]]}],["$","div",null,{"className":"x-highlightblock highlight-background-blue","children":[["$","h4",null,{"className":"x-h3","children":"更多笔记"}],["$","h4",null,{"className":"x-h3","children":"Instant NGP与NeRF的异同"}],["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"转载自<a href=\"https://zhuanlan.zhihu.com/p/631284285\" target=\"_blank\" rel=\"noreferrer\" class=\"x-inline-link\">知乎：从NeRF到Instant-NGP</a>"}}],["$","div",null,{"className":"x-uli","children":[["$","div",null,{"className":"x-uli-marker","children":["$","div",null,{"className":"x-uli-marker-dot"}]}],["$","div",null,{"className":"x-uli-content-wrapper","children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"同样基于体渲染"}}]}]]}],["$","div",null,{"className":"x-uli","children":[["$","div",null,{"className":"x-uli-marker","children":["$","div",null,{"className":"x-uli-marker-dot"}]}],["$","div",null,{"className":"x-uli-content-wrapper","children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"不同于NeRF的MLP，Instant NGP使用稀疏的参数化的<code class=\"x-inline-highlight\">voxel grid</code>作为场景表达"}}]}]]}]]}],["$","h3",null,{"className":"x-h2","children":["$","$L2",null,{"excludeFromContents":"$undefined","children":["$","a",null,{"href":"https://arxiv.org/pdf/2211.11646.pdf","target":"_blank","rel":"noreferrer","children":"【NeRF RPN】NeRF-RPN: A general framework for object detection in NeRFs (2022)"}]}]}],["$","div",null,{"className":"x-highlightblock highlight-background-gray","children":[false,[["$","div",null,{"className":"x-oli","children":[["$","div",null,{"className":"x-oli-number","children":"1."}],["$","div",null,{"className":"x-oli-content-wrapper","children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">文章的主题 / 文章要解决什么问题？</span>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"在NeRF中直接进行3D物体检测"}}]]}]]}],["$","div",null,{"className":"x-oli","children":[["$","div",null,{"className":"x-oli-number","children":"2."}],["$","div",null,{"className":"x-oli-content-wrapper","children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">文章的核心方法 / 具体是如何做的？</span>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"第一部分：特征提取器<br/>从NeRF采样的辐射度和密度网格作为输入，生成特征金字塔作为输出。<br/> 第二部分：RPN头<br/>对特征金字塔进行操作并生成对象建议。"}}]]}]]}],"$undefined",["$","div",null,{"className":"x-oli","children":[["$","div",null,{"className":"x-oli-number","children":"3."}],["$","div",null,{"className":"x-oli-content-wrapper","children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">研究的创新点</span>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"第一次将RPN引入NeRF以进行3D物体检测和相关任务<br/>利用Hypersim和3D-FRONT数据集构建了第一个用于3D目标检测的NeRF数据集"}}]]}]]}],"$undefined"]]}],["$","h3",null,{"className":"x-h2","children":["$","$L2",null,{"excludeFromContents":"$undefined","children":["$","a",null,{"href":"https://arxiv.org/pdf/2304.04395v3.pdf","target":"_blank","rel":"noreferrer","children":"【Instance NeRF】Instance Neural Radiance Field (2023)"}]}]}],["$","div",null,{"className":"x-highlightblock highlight-background-gray","children":[false,[["$","div",null,{"className":"x-oli","children":[["$","div",null,{"className":"x-oli-number","children":"1."}],["$","div",null,{"className":"x-oli-content-wrapper","children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">文章的主题 / 文章要解决什么问题？</span>"}}],[["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"输入一个以多视图RGB图像预训练的NeRF，学习给定场景的3D实例分割"}}],["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"文章的主要贡献："}}],["$","div",null,{"className":"x-uli","children":[["$","div",null,{"className":"x-uli-marker","children":["$","div",null,{"className":"x-uli-marker-dot"}]}],["$","div",null,{"className":"x-uli-content-wrapper","children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"第一个在NeRF中进行3D实例分割的尝试之一，而没有使用真实分割标签作为输入"}}]}]]}],["$","div",null,{"className":"x-uli","children":[["$","div",null,{"className":"x-uli-marker","children":["$","div",null,{"className":"x-uli-marker-dot"}]}],["$","div",null,{"className":"x-uli-content-wrapper","children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"提出<code class=\"x-inline-highlight\">Neural Instance Field</code>的结构和训练方法，可以产生<span class=\"x-inline-strong\">多视图一致</span>的2D分割和连续的3D分割"}}]}]]}],["$","div",null,{"className":"x-uli","children":[["$","div",null,{"className":"x-uli-marker","children":["$","div",null,{"className":"x-uli-marker-dot"}]}],["$","div",null,{"className":"x-uli-content-wrapper","children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"对合成室内NeRF数据集进行实验和消融研究"}}]}]]}]]]}]]}],["$","div",null,{"className":"x-oli","children":[["$","div",null,{"className":"x-oli-number","children":"2."}],["$","div",null,{"className":"x-oli-content-wrapper","children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">文章的核心方法 / 具体是如何做的？</span>"}}],[["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"Instance NeRF有两个组件：预训练的NeRF模型、和文中提出的<code class=\"x-inline-highlight\">Instance Field</code>。"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">Instance Field</code>的训练过程如下："}}],["$","$L4",null,{"src":"instance_field.jpg","width":"96%","invertInDarkTheme":true}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"NeRF-RCNN用预训练的NeRF提取的辐射场和密度场，为每个检测到的对象输出3D掩码；Mask2Former生成从NeRF渲染的图像的二维全景分割图（跨视图的实例标签并不一定一致）。然后按照相机位置，投影3D掩码去匹配不同视图中的相同实例，去产生多视图一致的2D分割图。CascadePSP用于细化2D mask。"}}]]]}]]}],"$undefined","$undefined","$undefined"]]}],["$","div",null,{"className":"x-highlightblock highlight-background-blue","children":[["$","h4",null,{"className":"x-h3","children":"更多笔记"}],["$","h4",null,{"className":"x-h3","children":"包围体：AABB和OBB"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">AABB</span>：轴对齐包围盒<code class=\"x-inline-highlight\">(Axis-Aligned Bounding Box)</code><br/><span class=\"x-inline-strong\">OBB</span>：有向包围盒<code class=\"x-inline-highlight\">(Oriented Bounding Box)</code>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"下图展示了更多种类的包围体："}}],["$","$L4",null,{"src":"bounding_volumes.png","width":"100%","invertInDarkTheme":true}]]}],["$","h3",null,{"className":"x-h2","children":["$","$L2",null,{"excludeFromContents":"$undefined","children":["$","a",null,{"href":"https://openaccess.thecvf.com/content_CVPR_2020/papers/Cheng_CascadePSP_Toward_Class-Agnostic_and_Very_High-Resolution_Segmentation_via_Global_and_CVPR_2020_paper.pdf","target":"_blank","rel":"noreferrer","children":"【CascadePSP】CascadePSP: Toward Class-Agnostic and Very High-Resolution Segmentation via Global and Local Refinement (2020)"}]}]}],["$","div",null,{"className":"x-highlightblock highlight-background-gray","children":[false,[["$","div",null,{"className":"x-oli","children":[["$","div",null,{"className":"x-oli-number","children":"1."}],["$","div",null,{"className":"x-oli-content-wrapper","children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">文章的主题 / 文章要解决什么问题？</span>"}}],[["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"提出一种不使用高分辨率训练数据，解决高分辨率分割问题的方法<br/>右图是改进后的结果："}}],["$","$L4",null,{"src":"cascadepsp.jpg","width":"400px","invertInDarkTheme":true}]]]}]]}],"$undefined","$undefined","$undefined","$undefined"]]}],["$","h3",null,{"className":"x-h2","children":["$","$L2",null,{"excludeFromContents":"$undefined","children":["$","a",null,{"href":"https://arxiv.org/pdf/2312.00860.pdf","target":"_blank","rel":"noreferrer","children":"【SAGA】Segment Any 3D Gaussians (2023)"}]}]}],["$","div",null,{"className":"x-highlightblock highlight-background-gray","children":[false,[["$","div",null,{"className":"x-oli","children":[["$","div",null,{"className":"x-oli-number","children":"1."}],["$","div",null,{"className":"x-oli-content-wrapper","children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">文章的主题 / 文章要解决什么问题？</span>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"交互式3D分割"}}]]}]]}],"$undefined","$undefined","$undefined","$undefined"]]}],["$","h2",null,{"className":"x-h1","children":["$","$L2",null,{"excludeFromContents":"$undefined","children":"学习"}]}],["$","h3",null,{"className":"x-h2","children":["$","$L2",null,{"excludeFromContents":"$undefined","children":["$","a",null,{"href":"https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf","target":"_blank","rel":"noreferrer","children":"【R-CNN】Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation (2014)"}]}]}],["$","div",null,{"className":"x-highlightblock highlight-background-gray","children":[false,[["$","div",null,{"className":"x-oli","children":[["$","div",null,{"className":"x-oli-number","children":"1."}],["$","div",null,{"className":"x-oli-content-wrapper","children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">文章的主题 / 文章要解决什么问题？</span>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"提出<code class=\"x-inline-highlight\">Regions with CNN features, R-CNN</code>提高目标检测性能"}}]]}]]}],["$","div",null,{"className":"x-oli","children":[["$","div",null,{"className":"x-oli-number","children":"2."}],["$","div",null,{"className":"x-oli-content-wrapper","children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">文章的核心方法 / 具体是如何做的？</span>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"区域提议<code class=\"x-inline-highlight\">(Region Proposals)</code>：使用<code class=\"x-inline-highlight\">selective search</code>生成候选框"}}]]}]]}],["$","div",null,{"className":"x-oli","children":[["$","div",null,{"className":"x-oli-number","children":"3."}],["$","div",null,{"className":"x-oli-content-wrapper","children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">做了什么实验，效果怎么样？</span>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"在<code class=\"x-inline-highlight\">PASCAL VOC 2012</code>取得<code class=\"x-inline-highlight\">mAP 53.3%</code>，在<code class=\"x-inline-highlight\">ILSVRC 2013</code>竞赛数据集取得<code class=\"x-inline-highlight\">mAP 31.4%</code>"}}]]}]]}],"$undefined","$undefined"]]}],["$","div",null,{"className":"x-highlightblock highlight-background-blue","children":[["$","h4",null,{"className":"x-h3","children":"更多笔记"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"转载自<a href=\"https://zh-v2.d2l.ai/chapter_computer-vision/rcnn.html\" target=\"_blank\" rel=\"noreferrer\" class=\"x-inline-link\">动手学深度学习 - 区域卷积神经网络系列</a>"}}],["$","h4",null,{"className":"x-h3","children":"R-CNN"}],["$","div",null,{"className":"x-uli","children":[["$","div",null,{"className":"x-uli-marker","children":["$","div",null,{"className":"x-uli-marker-dot"}]}],["$","div",null,{"className":"x-uli-content-wrapper","children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"R-CNN使用启发式搜索算法<code class=\"x-inline-highlight\">selective search</code>（之前人们通常也是这样做的）来选择锚框"}}]}]]}],["$","div",null,{"className":"x-uli","children":[["$","div",null,{"className":"x-uli-marker","children":["$","div",null,{"className":"x-uli-marker-dot"}]}],["$","div",null,{"className":"x-uli-content-wrapper","children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"用预训练的模型，对每一个锚框抽取特征"}}]}]]}],["$","div",null,{"className":"x-uli","children":[["$","div",null,{"className":"x-uli-marker","children":["$","div",null,{"className":"x-uli-marker-dot"}]}],["$","div",null,{"className":"x-uli-content-wrapper","children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"训练一个SVM来对类别分类"}}]}]]}],["$","div",null,{"className":"x-uli","children":[["$","div",null,{"className":"x-uli-marker","children":["$","div",null,{"className":"x-uli-marker-dot"}]}],["$","div",null,{"className":"x-uli-content-wrapper","children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"训练一个线性回归模型来预测边缘框"}}]}]]}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"R-CNN的速度很慢，因为可能从一张图像中选出上千个提议区域，这需要上千次的卷积神经网络的前向传播来执行目标检测。这种庞大的计算量使得R-CNN在现实世界中难以被广泛应用。"}}],["$","$L4",null,{"src":"rcnn.jpg","width":"600px"}],["$","h4",null,{"className":"x-h3","children":"Fast R-CNN"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"R-CNN的主要性能瓶颈在于，对每个提议区域，卷积神经网络的前向传播是独立的，而没有共享计算。由于这些区域通常有重叠，独立的特征抽取会导致重复的计算。Fast R-CNN的主要改进之一，是仅在整张图象上执行卷积神经网络的前向传播，并且引入<span class=\"x-inline-strong\">兴趣区域池化</span><code class=\"x-inline-highlight\">(ROI Pooling)</code>，将卷积神经网络的输出和提议区域作为输入，输出连结后的各个提议区域抽取的特征。"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"兴趣区域池化层可以给出固定大小的输出：把给定的锚框均匀分割成<span class=\"katex\"><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\">n</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">m</span></span></span></span>块，输出每块里的最大值，这样无论锚框多大，总是输出<span class=\"katex\"><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">nm</span></span></span></span>个值。"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"Fast R-CNN先对图片用CNN抽取特征，然后将<code class=\"x-inline-highlight\">selective search</code>给出的原图上的提议区域映射到CNN特征图上，再经过<code class=\"x-inline-highlight\">ROI Pooling</code>就可以得到维度对齐的特征。"}}],["$","$L4",null,{"src":"fastrcnn.jpg","width":"400px"}],["$","h4",null,{"className":"x-h3","children":"Faster R-CNN"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"与Fast R-CNN相比，Faster R-CNN将生成提议区域的方法从<code class=\"x-inline-highlight\">selective search</code>改为了<span class=\"x-inline-strong\">区域提议网络</span><code class=\"x-inline-highlight\">(Region Proposal Network, RPN)</code>，模型的其余部分保持不变。区域提议网络作为Faster R-CNN模型的一部分，是和整个模型一起训练得到的。换句话说，Faster R-CNN的目标函数不仅包括目标检测中的类别和边界框预测，还包括区域提议网络中锚框的二元类别和边界框预测。作为端到端训练的结果，区域提议网络能够学习到如何生成高质量的提议区域，从而在减少了从数据中学习的提议区域的数量的情况下，仍保持目标检测的精度。"}}],["$","$L4",null,{"src":"fasterrcnn.jpg","width":"600px"}]]}],["$","h3",null,{"className":"x-h2","children":["$","$L2",null,{"excludeFromContents":"$undefined","children":["$","a",null,{"href":"https://arxiv.org/pdf/1703.06870.pdf","target":"_blank","rel":"noreferrer","children":"【Mask R-CNN】Mask R-CNN (2017)"}]}]}],["$","div",null,{"className":"x-highlightblock highlight-background-gray","children":[false,[["$","div",null,{"className":"x-oli","children":[["$","div",null,{"className":"x-oli-number","children":"1."}],["$","div",null,{"className":"x-oli-content-wrapper","children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">文章的主题 / 文章要解决什么问题？</span>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"目标检测&实例分割"}}]]}]]}],["$","div",null,{"className":"x-oli","children":[["$","div",null,{"className":"x-oli-number","children":"2."}],["$","div",null,{"className":"x-oli-content-wrapper","children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">文章的核心方法 / 具体是如何做的？</span>"}}],[["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"基于Faster R-CNN，添加一个对每个ROI预测分割mask的分支。这个分支可以与分类和边界框预测分支并行。"}}],["$","$L4",null,{"src":"maskrcnn1.jpg","width":"600px"}]]]}]]}],["$","div",null,{"className":"x-oli","children":[["$","div",null,{"className":"x-oli-number","children":"3."}],["$","div",null,{"className":"x-oli-content-wrapper","children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">做了什么实验，效果怎么样？</span>"}}],[["$","$L4",null,{"src":"maskrcnn2.jpg","width":"600px","invertInDarkTheme":true}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"在COCO数据集上表现优异，超过了<code class=\"x-inline-highlight\">2015</code>和<code class=\"x-inline-highlight\">2016</code>年COCO分割任务的冠军。"}}]]]}]]}],["$","div",null,{"className":"x-oli","children":[["$","div",null,{"className":"x-oli-number","children":"4."}],["$","div",null,{"className":"x-oli-content-wrapper","children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">研究的创新点</span>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":" <code class=\"x-inline-highlight\">ROI Pooling</code>中发生了两次浮点数取整，第一次是将锚框均匀分割成<span class=\"katex\"><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\">n</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">m</span></span></span></span>块时，第二次是把提议区域映射回CNN特征图时。<br/> 分割任务的精细程度更高，因此文章提出了<code class=\"x-inline-highlight\">ROI Align</code>，使用双线性插值来保留特征图上的空间信息。 "}}]]}]]}],"$undefined"]]}],["$","div",null,{"className":"x-highlightblock highlight-background-blue","children":[["$","h4",null,{"className":"x-h3","children":"更多笔记"}],["$","h4",null,{"className":"x-h3","children":"语义分割与实例分割"}],["$","$L4",null,{"src":"ss_and_is.jpg","width":"600px"}],["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">语义分割</span><code class=\"x-inline-highlight\">(semantic segmentation)</code>：为每一个像素分配一个类别，但不区分同一类别之间的对象。"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">实例分割</span><code class=\"x-inline-highlight\">(instance segmentation)</code>：会区分属于同一类别的不同实例。"}}]]}],["$","h3",null,{"className":"x-h2","children":["$","$L2",null,{"excludeFromContents":"$undefined","children":["$","a",null,{"href":"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf","target":"_blank","rel":"noreferrer","children":"【FCN】Fully Convolutional Networks for Semantic Segmentation (2015)"}]}]}],["$","div",null,{"className":"x-highlightblock highlight-background-gray","children":[false,[["$","div",null,{"className":"x-oli","children":[["$","div",null,{"className":"x-oli-number","children":"1."}],["$","div",null,{"className":"x-oli-content-wrapper","children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">文章的主题 / 文章要解决什么问题？</span>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"使用全卷积网络进行语义分割"}}]]}]]}],["$","div",null,{"className":"x-oli","children":[["$","div",null,{"className":"x-oli-number","children":"2."}],["$","div",null,{"className":"x-oli-content-wrapper","children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">文章的核心方法 / 具体是如何做的？</span>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":" 是将现有的分类网络（AlexNet、VGG Net、GoogLeNet）改造为全卷积网络，以便在语义分割任务上进行端到端（输入图像，输出分割掩码）的训练。<br/> 改造的方式是将最后的全连接层替换成卷积层。 "}}]]}]]}],"$undefined","$undefined","$undefined"]]}],["$","div",null,{"className":"x-highlightblock highlight-background-blue","children":[["$","h4",null,{"className":"x-h3","children":"更多笔记"}],["$","h4",null,{"className":"x-h3","children":"转置卷积"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"卷积通常不会增大输入的高宽，而是保持不变或降低。由于语义分割任务需要像素级别的输出，转置卷积被用来增大输入的高宽。"}}],["$","$L4",null,{"src":"transpose_convolution.jpg","width":"600px","invertInDarkTheme":true}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"图片转载自<a href=\"https://zh-v2.d2l.ai/chapter_computer-vision/transposed-conv.html\" target=\"_blank\" rel=\"noreferrer\" class=\"x-inline-link\">动手学深度学习 - 转置卷积</a>"}}],["$","h4",null,{"className":"x-h3","children":"FCN中的转置卷积"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"例如对于ImageNet的图片输入，大小通常为<code class=\"x-inline-highlight\">224</code>&#42;<code class=\"x-inline-highlight\">224</code>&#42;<code class=\"x-inline-highlight\">3</code>（RGB通道）；经过卷积后缩小宽高缩小<code class=\"x-inline-highlight\">32</code>倍，通道增加到<code class=\"x-inline-highlight\">512</code>，变成<code class=\"x-inline-highlight\">7</code>&#42;<code class=\"x-inline-highlight\">7</code>&#42;<code class=\"x-inline-highlight\">512</code>的特征图。此时FCN会先通过一个<code class=\"x-inline-highlight\">1x1conv</code>进行通道降维，然后通过转置卷积将特征图的高度和宽度增加<code class=\"x-inline-highlight\">32</code>倍，<span class=\"x-inline-strong\">输出通道数等于类别数</span>，相当于储存了对每一类的预测结果。"}}]]}]],null]]},["$","$L5",null,{"parallelRouterKey":"children","segmentPath":["children","(blogs)","children","longtime","children","papers","children"],"loading":"$undefined","loadingStyles":"$undefined","loadingScripts":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/99d35e134c5cca0c.css","precedence":"next","crossOrigin":""}]]}]]},["$","$L5",null,{"parallelRouterKey":"children","segmentPath":["children","(blogs)","children","longtime","children"],"loading":"$undefined","loadingStyles":"$undefined","loadingScripts":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}]]},[null,["$","div",null,{"id":"blog-layout","children":[["$","div",null,{"id":"main","children":["$","$L5",null,{"parallelRouterKey":"children","segmentPath":["children","(blogs)","children"],"loading":"$undefined","loadingStyles":"$undefined","loadingScripts":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}]}],["$","$L7",null,{}],["$","$L8",null,{}]]}],null]]},[null,["$","html",null,{"lang":"zh-CN","suppressHydrationWarning":true,"children":[["$","head",null,{"children":[["$","link",null,{"rel":"icon","href":"/favicon.ico","type":"image/x-icon"}],["$","script",null,{"async":true,"src":"https://www.googletagmanager.com/gtag/js?id=G-45BYSZ6WPY"}],["$","script",null,{"async":true,"src":"https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\nwindow.dataLayer = window.dataLayer || [];\nfunction gtag() {\n    dataLayer.push(arguments);\n}\ngtag('js', new Date());\ngtag('config', 'G-45BYSZ6WPY');\n"}}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"const a=z=>h.getItem(z),b=(y,z)=>h.setItem(y,z),c=(y,z)=>document.documentElement.setAttribute(y,z),d='theme',e='dark',f='light',g='class',h=localStorage;a(d)!==e&&a(d)!==f&&b(d,f);a(d)===e?c(g,e):c(g,f);"}}]]}],["$","body",null,{"children":["$","$L9",null,{"children":[["$","$La",null,{}],["$","$L5",null,{"parallelRouterKey":"children","segmentPath":["children"],"loading":"$undefined","loadingStyles":"$undefined","loadingScripts":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","div",null,{"id":"notfound","children":[["$","img",null,{"alt":"img","src":"/images/cry.gif"}],["$","div",null,{"id":"notfound-404","children":"404"}],["$","div",null,{"id":"notfound-text","children":"Page Not Found"}]]}],"notFoundStyles":[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/eccd2e7a1149e571.css","precedence":"next","crossOrigin":""}]],"styles":[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/391c9d793ecdcfc9.css","precedence":"next","crossOrigin":""}]]}]]}]}]]}],null]],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/bbd99870a93ab979.css","precedence":"next","crossOrigin":""}]],"$Lb"]]]]
b:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"论文速记 - 铃木的网络日记"}],["$","link","3",{"rel":"canonical","href":"https://1kuzus.github.io/longtime/papers/"}]]
1:null
