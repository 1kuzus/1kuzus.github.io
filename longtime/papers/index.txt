1:HL["/_next/static/css/8ac0b0e7f508d966.css","style",{"crossOrigin":""}]
0:["a2ccQY1j6x_FTv2AsGrpW",[[["",{"children":["(blogs)",{"children":["longtime",{"children":["papers",{"children":["__PAGE__",{}]}]}]}]},"$undefined","$undefined",true],"$L2",[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/8ac0b0e7f508d966.css","precedence":"next","crossOrigin":""}]],"$L3"]]]]
4:HL["/_next/static/css/c84b60895b980f14.css","style",{"crossOrigin":""}]
5:HL["/_next/static/css/f70c26425a044d6b.css","style",{"crossOrigin":""}]
6:I[9226,["185","static/chunks/app/layout-e12e822557cc964c.js"],"GlobalProvider"]
7:I[9330,["185","static/chunks/app/layout-e12e822557cc964c.js"],""]
8:I[6954,[],""]
9:I[7264,[],""]
a:I[5285,["176","static/chunks/176-6509553bec92a789.js","135","static/chunks/app/(blogs)/layout-75f88dfdcbd0ba18.js"],""]
c:I[4484,["954","static/chunks/d3ac728e-1e5d8b71e3d43fec.js","202","static/chunks/202-7b9f876bcb1bf7b4.js","487","static/chunks/app/(blogs)/longtime/papers/page-4221026e5e3c9d64.js"],"Title"]
d:I[4484,["954","static/chunks/d3ac728e-1e5d8b71e3d43fec.js","202","static/chunks/202-7b9f876bcb1bf7b4.js","487","static/chunks/app/(blogs)/longtime/papers/page-4221026e5e3c9d64.js"],"H1"]
e:I[4484,["954","static/chunks/d3ac728e-1e5d8b71e3d43fec.js","202","static/chunks/202-7b9f876bcb1bf7b4.js","487","static/chunks/app/(blogs)/longtime/papers/page-4221026e5e3c9d64.js"],"H2"]
f:I[8275,["954","static/chunks/d3ac728e-1e5d8b71e3d43fec.js","202","static/chunks/202-7b9f876bcb1bf7b4.js","487","static/chunks/app/(blogs)/longtime/papers/page-4221026e5e3c9d64.js"],"Oli"]
11:I[4484,["954","static/chunks/d3ac728e-1e5d8b71e3d43fec.js","202","static/chunks/202-7b9f876bcb1bf7b4.js","487","static/chunks/app/(blogs)/longtime/papers/page-4221026e5e3c9d64.js"],"H3"]
12:I[8275,["954","static/chunks/d3ac728e-1e5d8b71e3d43fec.js","202","static/chunks/202-7b9f876bcb1bf7b4.js","487","static/chunks/app/(blogs)/longtime/papers/page-4221026e5e3c9d64.js"],"Uli"]
13:I[8411,["954","static/chunks/d3ac728e-1e5d8b71e3d43fec.js","202","static/chunks/202-7b9f876bcb1bf7b4.js","487","static/chunks/app/(blogs)/longtime/papers/page-4221026e5e3c9d64.js"],""]
10:T645,从<code class="x-inline-highlight">(<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">ϕ</span></span></span></span>)</code>到<code class="x-inline-highlight">(<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">G</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span></span>)</code>2:[null,["$","html",null,{"lang":"en","children":["$","body",null,{"children":["$","$L6",null,{"children":[["$","$L7",null,{}],["$","$L8",null,{"parallelRouterKey":"children","segmentPath":["children"],"loading":"$undefined","loadingStyles":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","template":["$","$L9",null,{}],"templateStyles":"$undefined","notFound":[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":"404"}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],"notFoundStyles":[],"childProp":{"current":[null,["$","$La",null,{"children":["$","$L8",null,{"parallelRouterKey":"children","segmentPath":["children","(blogs)","children"],"loading":"$undefined","loadingStyles":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","template":["$","$L9",null,{}],"templateStyles":"$undefined","notFound":[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":"404"}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],"notFoundStyles":[],"childProp":{"current":["$","$L8",null,{"parallelRouterKey":"children","segmentPath":["children","(blogs)","children","longtime","children"],"loading":"$undefined","loadingStyles":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","template":["$","$L9",null,{}],"templateStyles":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","childProp":{"current":["$","$L8",null,{"parallelRouterKey":"children","segmentPath":["children","(blogs)","children","longtime","children","papers","children"],"loading":"$undefined","loadingStyles":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","template":["$","$L9",null,{}],"templateStyles":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","childProp":{"current":["$Lb",[["$","$Lc",null,{"children":"$undefined"}],["$","$Ld",null,{"children":"研究"}],["$","$Le",null,{"href":"https://arxiv.org/pdf/2003.08934.pdf","children":"【NeRF】NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis (2020)"}],["$","div",null,{"className":"x-highlightblock highlight-background-gray","children":[false,[["$","$Lf","0",{"children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">文章的主题 / 文章要解决什么问题？</span>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"使用稀疏输入2D图片集实现场景的3D视图合成"}}]]}],["$","$Lf","1",{"children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">文章的核心方法 / 具体是如何做的？</span>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"$10"}}]]}],["$","$Lf","2",{"children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">做了什么实验，效果怎么样？</span>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"测试数据集是<code class=\"x-inline-highlight\">Diffuse Synthetic 360</code>、<code class=\"x-inline-highlight\">Realistic Synthetic 360</code>和<code class=\"x-inline-highlight\">Real Forward-Facing</code>"}}]]}],["$","$Lf","3",{"children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">研究的创新点</span>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"将输入坐标位置编码，帮助MLP表示高频函数<br/>分层采样"}}]]}],["$","$Lf","4",{"children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">有什么限制或可以改进的地方？</span>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"有效地优化和渲染神经辐射场<br/>可解释性"}}]]}]]]}],["$","div",null,{"className":"x-highlightblock highlight-background-blue","children":[["$","$L11",null,{"children":"更多笔记"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"神经辐射场用于从2D的图片重建3D的场景。"}}],["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"文中出现的三个指标：PSNR、SSIM、LPIPS"}}],["$","$L12",null,{"children":"*峰值信噪比*`(Peak Signal to Noise Ratio, PSNR)`：--- 用于衡量图像恢复的质量，数值越高表示图像质量越好。接近`50 dB`代表误差非常小，大于`30 dB`--- 人眼难察觉差异。"}],["$","$L12",null,{"children":"*结构相似性*`(Structural Similarity Index Measure, SSIM)`：--- 用于衡量图像的结构相似性，得分通常在`0`~`1`之间，数值越高表示图像结构越相似。相较于PSNR在图像质量的衡量上更能符合人眼对图像质量的判断。"}],["$","$L12",null,{"children":"*基于学习的感知图像质量评价*`(Learned Perceptual Image Patch Similarity, LPIPS)`：--- 测量从预训练网络中提取的两个图像的特征之间的相似性，得分通常在`0`~`1`之间，数值越低表示感知质量越高。"}]]}],["$","$Le",null,{"href":"https://arxiv.org/pdf/2308.04079.pdf","children":"【3DGS】3D Gaussian Splatting for Real-Time Radiance Field Rendering (2023)"}],["$","div",null,{"className":"x-highlightblock highlight-background-gray","children":[false,[["$","$Lf","0",{"children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">文章的主题 / 文章要解决什么问题？</span>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"实现实时辐射场渲染，同时保持高质量的视觉效果，并且保持较短的训练时间"}}]]}],"$undefined","$undefined","$undefined","$undefined"]]}],["$","div",null,{"className":"x-highlightblock highlight-background-blue","children":[["$","$L11",null,{"children":"更多笔记"}],["$","$L11",null,{"children":"文章的相关工作部分"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"传统的场景重建与渲染：基于光场的，密集采样、非结构化捕获；<span class=\"x-inline-strong\">运动恢复结构</span><code class=\"x-inline-highlight\">(Structure from Motion, SFM)</code>用一组照片估计稀疏点云合成新视图；<span class=\"x-inline-strong\">多视点立体视觉</span><code class=\"x-inline-highlight\">(Multi-View Stereo, MVS)</code>；<br/> 神经渲染和辐射场：用CNN估计混合权重，用于纹理空间；Soft3D提出<code class=\"x-inline-highlight\">Volumetric representations</code>；NeRF提出重要性采样和位置编码来提高质量，但使用了大型多层感知器，对速度有负面影响；"}}],["$","$L11",null,{"children":"稀疏重建和稠密重建"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"稀疏重建主要用于定位，得到每张图片的相机参数，提取特征点，例如SFM；稠密重建是假设相机参数已知的情况下，从不同视角的图像中找到匹配的对应点，对整个图像或图像中绝大部分像素进行重建。"}}]]}],["$","$Le",null,{"href":"https://arxiv.org/pdf/2201.05989.pdf","children":"【Instant NGP】Instant Neural Graphics Primitives with a Multiresolution Hash Encoding (2022)"}],["$","div",null,{"className":"x-highlightblock highlight-background-gray","children":[false,[["$","$Lf","0",{"children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">文章的主题 / 文章要解决什么问题？</span>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"实现实时辐射场渲染，同时保持高质量的视觉效果，并且保持较短的训练时间"}}]]}],"$undefined","$undefined","$undefined","$undefined"]]}],["$","div",null,{"className":"x-highlightblock highlight-background-blue","children":[["$","$L11",null,{"children":"更多笔记"}],["$","$L11",null,{"children":"Instant NGP与NeRF的异同"}],["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"转载自<a href=\"https://zhuanlan.zhihu.com/p/631284285\" target=\"_blank\" rel=\"noreferrer\" class=\"x-inline-link\">知乎：从NeRF到Instant-NGP</a>"}}],["$","$L12",null,{"children":"同样基于体渲染"}],["$","$L12",null,{"children":"不同于NeRF的MLP，Instant NGP使用稀疏的参数化的`voxel grid`作为场景表达"}]]}],["$","$Le",null,{"href":"https://arxiv.org/pdf/2211.11646.pdf","children":"【NeRF RPN】NeRF-RPN: A general framework for object detection in NeRFs (2022)"}],["$","div",null,{"className":"x-highlightblock highlight-background-gray","children":[false,[["$","$Lf","0",{"children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">文章的主题 / 文章要解决什么问题？</span>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"在NeRF中直接进行3D物体检测"}}]]}],["$","$Lf","1",{"children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">文章的核心方法 / 具体是如何做的？</span>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"第一部分：特征提取器<br/>从NeRF采样的辐射度和密度网格作为输入，生成特征金字塔作为输出。<br/> 第二部分：RPN头<br/>对特征金字塔进行操作并生成对象建议。"}}]]}],"$undefined",["$","$Lf","3",{"children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">研究的创新点</span>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"第一次将RPN引入NeRF以进行3D物体检测和相关任务<br/>利用Hypersim和3D-FRONT数据集构建了第一个用于3D目标检测的NeRF数据集"}}]]}],"$undefined"]]}],["$","$Le",null,{"href":"https://arxiv.org/pdf/2304.04395v3.pdf","children":"【Instance NeRF】Instance Neural Radiance Field (2023)"}],["$","div",null,{"className":"x-highlightblock highlight-background-gray","children":[false,[["$","$Lf","0",{"children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">文章的主题 / 文章要解决什么问题？</span>"}}],[["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"输入一个以多视图RGB图像预训练的NeRF，学习给定场景的3D实例分割"}}],["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"文章的主要贡献："}}],["$","$L12",null,{"children":"第一个在NeRF中进行3D实例分割的尝试之一，而没有使用真实分割标签作为输入"}],["$","$L12",null,{"children":"提出`Neural Instance Field`的结构和训练方法，可以产生*多视图一致*的2D分割和连续的3D分割"}],["$","$L12",null,{"children":"对合成室内NeRF数据集进行实验和消融研究"}]]]}],["$","$Lf","1",{"children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">文章的核心方法 / 具体是如何做的？</span>"}}],[["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"Instance NeRF有两个组件：预训练的NeRF模型、和文中提出的<code class=\"x-inline-highlight\">Instance Field</code>。"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">Instance Field</code>的训练过程如下："}}],["$","$L13",null,{"src":{"default":{"src":"/_next/static/media/instance_field.500af71e.jpg","height":500,"width":1458,"blurDataURL":"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAMACAMBIgACEQEDEQH/xAAnAAEBAAAAAAAAAAAAAAAAAAAABwEBAAAAAAAAAAAAAAAAAAAAAf/aAAwDAQACEAMQAAAAqwD/xAAbEAEAAQUBAAAAAAAAAAAAAAABAwACERNBIf/aAAgBAQABPwBij2CW4V55xr//xAAWEQEBAQAAAAAAAAAAAAAAAAABIQD/2gAIAQIBAT8AVl3/xAAVEQEBAAAAAAAAAAAAAAAAAAAAAf/aAAgBAwEBPwCv/9k=","blurWidth":8,"blurHeight":3}},"width":"96%","invertInDarkTheme":true}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"NeRF-RCNN用预训练的NeRF提取的辐射场和密度场，为每个检测到的对象输出3D掩码；Mask2Former生成从NeRF渲染的图像的二维全景分割图（跨视图的实例标签并不一定一致）。然后按照相机位置，投影3D掩码去匹配不同视图中的相同实例，去产生多视图一致的2D分割图。CascadePSP用于细化2D mask。"}}]]]}],"$undefined","$undefined","$undefined"]]}],["$","div",null,{"className":"x-highlightblock highlight-background-blue","children":[["$","$L11",null,{"children":"更多笔记"}],["$","$L11",null,{"children":"包围体：AABB和OBB"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">AABB</span>：轴对齐包围盒<code class=\"x-inline-highlight\">(Axis-Aligned Bounding Box)</code><br/><span class=\"x-inline-strong\">OBB</span>：有向包围盒<code class=\"x-inline-highlight\">(Oriented Bounding Box)</code>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"下图展示了更多种类的包围体："}}],["$","$L13",null,{"src":{"default":{"src":"/_next/static/media/bounding_volumes.e98c20aa.png","height":323,"width":850,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAADBAMAAABc5lN7AAAAJ1BMVEX////+/v78/Pz19fX09PX09PTz8/Py8vLx8fHw8fHw8PDv7+/u7u532tZ0AAAAF0lEQVR42mNgqj4qwOA2fYU5Q3mpeTsAIZYEuS5vM7AAAAAASUVORK5CYII=","blurWidth":8,"blurHeight":3}},"width":"100%","invertInDarkTheme":true}]]}],["$","$Le",null,{"href":"https://openaccess.thecvf.com/content_CVPR_2020/papers/Cheng_CascadePSP_Toward_Class-Agnostic_and_Very_High-Resolution_Segmentation_via_Global_and_CVPR_2020_paper.pdf","children":"【CascadePSP】CascadePSP: Toward Class-Agnostic and Very High-Resolution Segmentation via Global and Local Refinement (2020)"}],["$","div",null,{"className":"x-highlightblock highlight-background-gray","children":[false,[["$","$Lf","0",{"children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">文章的主题 / 文章要解决什么问题？</span>"}}],[["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"提出一种不使用高分辨率训练数据，解决高分辨率分割问题的方法<br/>右图是改进后的结果："}}],["$","$L13",null,{"src":{"default":{"src":"/_next/static/media/cascadepsp.0d9b618b.jpg","height":542,"width":747,"blurDataURL":"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAYACAMBIgACEQEDEQH/xAAoAAEBAAAAAAAAAAAAAAAAAAAABgEBAQAAAAAAAAAAAAAAAAAAAgP/2gAMAwEAAhADEAAAAKQRf//EABoQAQACAwEAAAAAAAAAAAAAAAEDBAACEkH/2gAIAQEAAT8A1s3C1KbWpeScA68XP//EABcRAAMBAAAAAAAAAAAAAAAAAAABAhH/2gAIAQIBAT8Amnh//8QAGBEAAgMAAAAAAAAAAAAAAAAAAQIAEnH/2gAIAQMBAT8AcC5xZ//Z","blurWidth":8,"blurHeight":6}},"width":"400px","invertInDarkTheme":true}]]]}],"$undefined","$undefined","$undefined","$undefined"]]}],["$","$Le",null,{"href":"https://arxiv.org/pdf/2312.00860.pdf","children":"【SAGA】Segment Any 3D Gaussians (2023)"}],["$","div",null,{"className":"x-highlightblock highlight-background-gray","children":[false,[["$","$Lf","0",{"children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">文章的主题 / 文章要解决什么问题？</span>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"交互式3D分割"}}]]}],"$undefined","$undefined","$undefined","$undefined"]]}],["$","$Ld",null,{"children":"学习"}],["$","$Le",null,{"href":"https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf","children":"【R-CNN】Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation (2014)"}],["$","div",null,{"className":"x-highlightblock highlight-background-gray","children":[false,[["$","$Lf","0",{"children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">文章的主题 / 文章要解决什么问题？</span>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"提出<code class=\"x-inline-highlight\">Regions with CNN features, R-CNN</code>提高目标检测性能"}}]]}],["$","$Lf","1",{"children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">文章的核心方法 / 具体是如何做的？</span>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"区域提议<code class=\"x-inline-highlight\">(Region Proposals)</code>：使用<code class=\"x-inline-highlight\">selective search</code>生成候选框"}}]]}],["$","$Lf","2",{"children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">做了什么实验，效果怎么样？</span>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"在<code class=\"x-inline-highlight\">PASCAL VOC 2012</code>取得<code class=\"x-inline-highlight\">mAP 53.3%</code>，在<code class=\"x-inline-highlight\">ILSVRC 2013</code>竞赛数据集取得<code class=\"x-inline-highlight\">mAP 31.4%</code>"}}]]}],"$undefined","$undefined"]]}],["$","div",null,{"className":"x-highlightblock highlight-background-blue","children":[["$","$L11",null,{"children":"更多笔记"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"转载自<a href=\"https://zh-v2.d2l.ai/chapter_computer-vision/rcnn.html\" target=\"_blank\" rel=\"noreferrer\" class=\"x-inline-link\">动手学深度学习 - 区域卷积神经网络系列</a>"}}],["$","$L11",null,{"children":"R-CNN"}],["$","$L12",null,{"children":"R-CNN使用启发式搜索算法`selective search`（之前人们通常也是这样做的）来选择锚框"}],["$","$L12",null,{"children":"用预训练的模型，对每一个锚框抽取特征"}],["$","$L12",null,{"children":"训练一个SVM来对类别分类"}],["$","$L12",null,{"children":"训练一个线性回归模型来预测边缘框"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"R-CNN的速度很慢，因为可能从一张图像中选出上千个提议区域，这需要上千次的卷积神经网络的前向传播来执行目标检测。这种庞大的计算量使得R-CNN在现实世界中难以被广泛应用。"}}],["$","$L13",null,{"src":{"default":{"src":"/_next/static/media/rcnn.09a38e8a.jpg","height":467,"width":1563,"blurDataURL":"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAIACAMBIgACEQEDEQH/xAAoAAEBAAAAAAAAAAAAAAAAAAAABwEBAQAAAAAAAAAAAAAAAAAAAgP/2gAMAwEAAhADEAAAAKYKD//EABkQAQACAwAAAAAAAAAAAAAAAAIAAQMhI//aAAgBAQABPwBpHLpXXKf/xAAXEQADAQAAAAAAAAAAAAAAAAAAAkFx/9oACAECAQE/AFun/8QAFxEAAwEAAAAAAAAAAAAAAAAAAAFBcf/aAAgBAwEBPwBzD//Z","blurWidth":8,"blurHeight":2}},"width":"600px"}],["$","$L11",null,{"children":"Fast R-CNN"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"R-CNN的主要性能瓶颈在于，对每个提议区域，卷积神经网络的前向传播是独立的，而没有共享计算。由于这些区域通常有重叠，独立的特征抽取会导致重复的计算。Fast R-CNN的主要改进之一，是仅在整张图象上执行卷积神经网络的前向传播，并且引入<span class=\"x-inline-strong\">兴趣区域池化</span><code class=\"x-inline-highlight\">(ROI Pooling)</code>，将卷积神经网络的输出和提议区域作为输入，输出连结后的各个提议区域抽取的特征。"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"兴趣区域池化层可以给出固定大小的输出：把给定的锚框均匀分割成<span class=\"katex\"><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\">n</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">m</span></span></span></span>块，输出每块里的最大值，这样无论锚框多大，总是输出<span class=\"katex\"><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">nm</span></span></span></span>个值。"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"Fast R-CNN先对图片用CNN抽取特征，然后将<code class=\"x-inline-highlight\">selective search</code>给出的原图上的提议区域映射到CNN特征图上，再经过<code class=\"x-inline-highlight\">ROI Pooling</code>就可以得到维度对齐的特征。"}}],["$","$L13",null,{"src":{"default":{"src":"/_next/static/media/fastrcnn.46850bc6.jpg","height":838,"width":917,"blurDataURL":"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAcACAMBIgACEQEDEQH/xAAnAAEBAAAAAAAAAAAAAAAAAAAABwEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEAMQAAAArAP/xAAbEAACAgMBAAAAAAAAAAAAAAABAgMEABEhcf/aAAgBAQABPwCEobVpUCgxtrzfeZ//xAAXEQEAAwAAAAAAAAAAAAAAAAABABEx/9oACAECAQE/ALXZ/8QAFhEBAQEAAAAAAAAAAAAAAAAAAQAR/9oACAEDAQE/AMC//9k=","blurWidth":8,"blurHeight":7}},"width":"400px"}],["$","$L11",null,{"children":"Faster R-CNN"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"与Fast R-CNN相比，Faster R-CNN将生成提议区域的方法从<code class=\"x-inline-highlight\">selective search</code>改为了<span class=\"x-inline-strong\">区域提议网络</span><code class=\"x-inline-highlight\">(Region Proposal Network, RPN)</code>，模型的其余部分保持不变。区域提议网络作为Faster R-CNN模型的一部分，是和整个模型一起训练得到的。换句话说，Faster R-CNN的目标函数不仅包括目标检测中的类别和边界框预测，还包括区域提议网络中锚框的二元类别和边界框预测。作为端到端训练的结果，区域提议网络能够学习到如何生成高质量的提议区域，从而在减少了从数据中学习的提议区域的数量的情况下，仍保持目标检测的精度。"}}],["$","$L13",null,{"src":{"default":{"src":"/_next/static/media/fasterrcnn.1f4436bb.jpg","height":871,"width":1579,"blurDataURL":"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAQACAMBIgACEQEDEQH/xAAnAAEBAAAAAAAAAAAAAAAAAAAABwEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEAMQAAAArgP/xAAaEAEAAwADAAAAAAAAAAAAAAABAhEhAAME/9oACAEBAAE/APPEnBbSu2eGGNc//8QAFBEBAAAAAAAAAAAAAAAAAAAAAP/aAAgBAgEBPwB//8QAFBEBAAAAAAAAAAAAAAAAAAAAAP/aAAgBAwEBPwB//9k=","blurWidth":8,"blurHeight":4}},"width":"600px"}]]}],["$","$Le",null,{"href":"https://arxiv.org/pdf/1703.06870.pdf","children":"【Mask R-CNN】Mask R-CNN (2017)"}],["$","div",null,{"className":"x-highlightblock highlight-background-gray","children":[false,[["$","$Lf","0",{"children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">文章的主题 / 文章要解决什么问题？</span>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"目标检测&实例分割"}}]]}],["$","$Lf","1",{"children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">文章的核心方法 / 具体是如何做的？</span>"}}],[["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"基于Faster R-CNN，添加一个对每个ROI预测分割mask的分支。这个分支可以与分类和边界框预测分支并行。"}}],["$","$L13",null,{"src":{"default":{"src":"/_next/static/media/maskrcnn1.6cdb9105.jpg","height":342,"width":676,"blurDataURL":"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAQACAMBIgACEQEDEQH/xAAoAAEBAAAAAAAAAAAAAAAAAAAABwEBAQAAAAAAAAAAAAAAAAAAAAL/2gAMAwEAAhADEAAAAKsIf//EABwQAAICAgMAAAAAAAAAAAAAAAEDERIAIQITMf/aAAgBAQABPwDgkUWvsZv022ZM5//EABcRAAMBAAAAAAAAAAAAAAAAAAABQXH/2gAIAQIBAT8Acw//xAAVEQEBAAAAAAAAAAAAAAAAAAAAQf/aAAgBAwEBPwCP/9k=","blurWidth":8,"blurHeight":4}},"width":"600px"}]]]}],["$","$Lf","2",{"children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">做了什么实验，效果怎么样？</span>"}}],[["$","$L13",null,{"src":{"default":{"src":"/_next/static/media/maskrcnn2.fe26f764.jpg","height":184,"width":763,"blurDataURL":"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAIACAMBIgACEQEDEQH/xAAnAAEBAAAAAAAAAAAAAAAAAAAABwEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEAMQAAAArQP/xAAVEAEBAAAAAAAAAAAAAAAAAAAAAv/aAAgBAQABPwCH/8QAFBEBAAAAAAAAAAAAAAAAAAAAAP/aAAgBAgEBPwB//8QAFBEBAAAAAAAAAAAAAAAAAAAAAP/aAAgBAwEBPwB//9k=","blurWidth":8,"blurHeight":2}},"width":"600px","invertInDarkTheme":true}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"在COCO数据集上表现优异，超过了<code class=\"x-inline-highlight\">2015</code>和<code class=\"x-inline-highlight\">2016</code>年COCO分割任务的冠军。"}}]]]}],["$","$Lf","3",{"children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">研究的创新点</span>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":" <code class=\"x-inline-highlight\">ROI Pooling</code>中发生了两次浮点数取整，第一次是将锚框均匀分割成<span class=\"katex\"><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\">n</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">m</span></span></span></span>块时，第二次是把提议区域映射回CNN特征图时。<br/> 分割任务的精细程度更高，因此文章提出了<code class=\"x-inline-highlight\">ROI Align</code>，使用双线性插值来保留特征图上的空间信息。 "}}]]}],"$undefined"]]}],["$","div",null,{"className":"x-highlightblock highlight-background-blue","children":[["$","$L11",null,{"children":"更多笔记"}],["$","$L11",null,{"children":"语义分割与实例分割"}],["$","$L13",null,{"src":{"default":{"src":"/_next/static/media/ss_and_is.f6df7378.jpg","height":370,"width":1243,"blurDataURL":"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAIACAMBIgACEQEDEQH/xAAnAAEBAAAAAAAAAAAAAAAAAAAABgEBAAAAAAAAAAAAAAAAAAAAAf/aAAwDAQACEAMQAAAAqQv/xAAbEAABBAMAAAAAAAAAAAAAAAACAAEDIhEhov/aAAgBAQABPwCConjVpH7X/8QAFxEAAwEAAAAAAAAAAAAAAAAAAAECMv/aAAgBAgEBPwCco//EABgRAAIDAAAAAAAAAAAAAAAAAAABAjJx/9oACAEDAQE/AJ2en//Z","blurWidth":8,"blurHeight":2}},"width":"600px"}],["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">语义分割</span><code class=\"x-inline-highlight\">(semantic segmentation)</code>：为每一个像素分配一个类别，但不区分同一类别之间的对象。"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">实例分割</span><code class=\"x-inline-highlight\">(instance segmentation)</code>：会区分属于同一类别的不同实例。"}}]]}],["$","$Le",null,{"href":"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf","children":"【FCN】Fully Convolutional Networks for Semantic Segmentation (2015)"}],["$","div",null,{"className":"x-highlightblock highlight-background-gray","children":[false,[["$","$Lf","0",{"children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">文章的主题 / 文章要解决什么问题？</span>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"使用全卷积网络进行语义分割"}}]]}],["$","$Lf","1",{"children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">文章的核心方法 / 具体是如何做的？</span>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":" 是将现有的分类网络（AlexNet、VGG Net、GoogLeNet）改造为全卷积网络，以便在语义分割任务上进行端到端（输入图像，输出分割掩码）的训练。<br/> 改造的方式是将最后的全连接层替换成卷积层。 "}}]]}],"$undefined","$undefined","$undefined"]]}],["$","div",null,{"className":"x-highlightblock highlight-background-blue","children":[["$","$L11",null,{"children":"更多笔记"}],["$","$L11",null,{"children":"转置卷积"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"卷积通常不会增大输入的高宽，而是保持不变或降低。由于语义分割任务需要像素级别的输出，转置卷积被用来增大输入的高宽。"}}],["$","$L13",null,{"src":{"default":{"src":"/_next/static/media/transpose_convolution.fb99cc55.jpg","height":393,"width":890,"blurDataURL":"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAQACAMBIgACEQEDEQH/xAAnAAEBAAAAAAAAAAAAAAAAAAAABwEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEAMQAAAArQP/xAAbEAABBAMAAAAAAAAAAAAAAAACAAEDESEiUf/aAAgBAQABPwBpCwV7dX//xAAUEQEAAAAAAAAAAAAAAAAAAAAA/9oACAECAQE/AH//xAAUEQEAAAAAAAAAAAAAAAAAAAAA/9oACAEDAQE/AH//2Q==","blurWidth":8,"blurHeight":4}},"width":"600px","invertInDarkTheme":true}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"图片转载自<a href=\"https://zh-v2.d2l.ai/chapter_computer-vision/transposed-conv.html\" target=\"_blank\" rel=\"noreferrer\" class=\"x-inline-link\">动手学深度学习 - 转置卷积</a>"}}],["$","$L11",null,{"children":"FCN中的转置卷积"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"例如对于ImageNet的图片输入，大小通常为<code class=\"x-inline-highlight\">224</code>&#42;<code class=\"x-inline-highlight\">224</code>&#42;<code class=\"x-inline-highlight\">3</code>（RGB通道）；经过卷积后缩小宽高缩小<code class=\"x-inline-highlight\">32</code>倍，通道增加到<code class=\"x-inline-highlight\">512</code>，变成<code class=\"x-inline-highlight\">7</code>&#42;<code class=\"x-inline-highlight\">7</code>&#42;<code class=\"x-inline-highlight\">512</code>的特征图。此时FCN会先通过一个<code class=\"x-inline-highlight\">1x1conv</code>进行通道降维，然后通过转置卷积将特征图的高度和宽度增加<code class=\"x-inline-highlight\">32</code>倍，<span class=\"x-inline-strong\">输出通道数等于类别数</span>，相当于储存了对每一类的预测结果。"}}]]}]],null],"segment":"__PAGE__"},"styles":[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/f70c26425a044d6b.css","precedence":"next","crossOrigin":""}]]}],"segment":"papers"},"styles":[]}],"segment":"longtime"},"styles":[]}],"params":{}}],null],"segment":"(blogs)"},"styles":[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/c84b60895b980f14.css","precedence":"next","crossOrigin":""}]]}]]}]}]}],null]
3:[["$","meta","0",{"charSet":"utf-8"}],["$","title","1",{"children":"Create Next App"}],["$","meta","2",{"name":"description","content":"Generated by create next app"}],["$","meta","3",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","link","4",{"rel":"icon","href":"/favicon.ico","type":"image/x-icon","sizes":"16x16"}]]
b:null
