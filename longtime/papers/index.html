<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/css/8ac0b0e7f508d966.css" crossorigin="" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/d1ba65e78d6f432c.css" crossorigin="" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/f70c26425a044d6b.css" crossorigin="" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-70957058d2eabc37.js" crossorigin=""/><script src="/_next/static/chunks/fd9d1056-317889f780d115a2.js" async="" crossorigin=""></script><script src="/_next/static/chunks/472-d5046572d4bcdf16.js" async="" crossorigin=""></script><script src="/_next/static/chunks/main-app-cfdfd0e1c1afc4e3.js" async="" crossorigin=""></script><script src="/_next/static/chunks/app/layout-959d1352bec0edcb.js" async=""></script><script src="/_next/static/chunks/413-dc980fd24c15cede.js" async=""></script><script src="/_next/static/chunks/app/not-found-aa1272efa852a321.js" async=""></script><script src="/_next/static/chunks/d3ac728e-1e5d8b71e3d43fec.js" async=""></script><script src="/_next/static/chunks/202-268c3b9a2ea32d77.js" async=""></script><script src="/_next/static/chunks/396-150eb8004cde3d82.js" async=""></script><script src="/_next/static/chunks/app/(blogs)/layout-39f18c96d96b8978.js" async=""></script><script src="/_next/static/chunks/app/(blogs)/longtime/papers/page-81ce2c3df2c4378d.js" async=""></script><title>Create Next App</title><meta name="description" content="Generated by create next app"/><script src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js" crossorigin="" noModule=""></script></head><body><div id="header" class=""><div id="header-left-wrapper"><div id="header-logo-bg"><svg viewBox="0 0 1560 1560" width="36px" height="36px" xmlns="http://www.w3.org/2000/svg"><g fill-rule="evenodd"><path d="M644 97h272.529L1189 780H916.471z" fill="#00A8C4"></path><path d="M98 97h272.84L780 1120.73 1189.162 97H1462L916.438 1462H643.562z" fill="#30303C"></path><path d="M98 1462L643.3 97H916L370.7 1462z" fill="#00F8FF"></path></g></svg></div><div id="header-archive"><h3 id="header-archive-text">归档</h3><div class="header-archive-rightarrow"><svg viewBox="0 0 1024 1024" width="16px" height="16px" xmlns="http://www.w3.org/2000/svg"><path d="M761.056 532.128c.512-.992 1.344-1.824 1.792-2.848 8.8-18.304 5.92-40.704-9.664-55.424L399.936 139.744c-19.264-18.208-49.632-17.344-67.872 1.888-18.208 19.264-17.376 49.632 1.888 67.872l316.96 299.84L335.2 813.632c-19.072 18.4-19.648 48.768-1.248 67.872 9.408 9.792 21.984 14.688 34.56 14.688 12 0 24-4.48 33.312-13.44l350.048-337.376c.672-.672.928-1.6 1.6-2.304.512-.48 1.056-.832 1.568-1.344 2.72-2.848 4.16-6.336 6.016-9.6z" fill="#50505c"></path></svg></div></div></div><div id="header-right-wrapper"><div id="header-theme-bg"><svg viewBox="0 0 1024 1024" width="20px" height="20px" xmlns="http://www.w3.org/2000/svg"><path fill="#FCFCFC" d="M512 61.44c13.312 0 25.252 7.639 29.942 19.17l27.566 67.972c11.92 29.348-4.178 62.075-35.943 73.094A65.946 65.946 0 01512 225.28c-33.935 0-61.44-25.416-61.44-56.77 0-6.8 1.331-13.558 3.912-19.928l27.586-67.973C486.728 69.08 498.647 61.44 512 61.44zm0 901.12c-13.332 0-25.272-7.639-29.942-19.17l-27.586-67.972a52.961 52.961 0 01-3.912-19.927c0-31.335 27.505-56.771 61.44-56.771a66.28 66.28 0 0121.565 3.604c31.765 11.019 47.862 43.746 35.943 73.114l-27.566 67.953c-4.69 11.53-16.63 19.169-29.942 19.169zM962.56 512c0 13.312-7.639 25.252-19.17 29.942l-67.972 27.566c-29.348 11.92-62.075-4.178-73.094-35.943A65.946 65.946 0 01798.72 512c0-33.935 25.416-61.44 56.77-61.44 6.8 0 13.558 1.331 19.928 3.912l67.973 27.586c11.53 4.67 19.169 16.589 19.169 29.942zm-901.12 0c0-13.332 7.639-25.272 19.17-29.942l67.972-27.586a52.9 52.9 0 0119.927-3.912c31.335 0 56.771 27.505 56.771 61.44a66.28 66.28 0 01-3.604 21.565c-11.019 31.765-43.746 47.862-73.114 35.943l-67.953-27.566C69.08 537.252 61.44 525.312 61.44 512zm131.953-318.587c9.42-9.42 23.265-12.452 34.734-7.618l67.563 28.549c29.184 12.35 40.94 46.858 26.256 77.107a65.946 65.946 0 01-12.698 17.817c-23.982 23.983-61.399 25.457-83.558 3.277a52.961 52.961 0 01-11.346-16.855l-28.55-67.543c-4.853-11.469-1.822-25.313 7.599-34.734zm637.194 637.194c-9.42 9.421-23.265 12.452-34.734 7.598l-67.543-28.549a52.961 52.961 0 01-16.876-11.325c-22.16-22.18-20.685-59.597 3.298-83.579a65.946 65.946 0 0117.817-12.698c30.25-14.684 64.758-2.928 77.107 26.256l28.55 67.563c4.833 11.47 1.802 25.293-7.62 34.734zm0-637.214c9.42 9.42 12.452 23.265 7.618 34.734l-28.549 67.563c-12.35 29.184-46.858 40.94-77.107 26.256a65.946 65.946 0 01-17.817-12.698c-23.983-23.982-25.457-61.399-3.277-83.558 4.792-4.834 10.506-8.663 16.855-11.346l67.543-28.55c11.469-4.853 25.313-1.822 34.734 7.599zM193.393 830.587c-9.421-9.42-12.452-23.265-7.598-34.734l28.549-67.543a52.618 52.618 0 0111.325-16.876c22.18-22.16 59.597-20.685 83.579 3.298a65.842 65.842 0 0112.698 17.817c14.684 30.25 2.928 64.758-26.256 77.107l-67.563 28.55c-11.47 4.833-25.293 1.802-34.734-7.62zM512 737.28c-124.416 0-225.28-100.864-225.28-225.28S387.584 286.72 512 286.72 737.28 387.584 737.28 512 636.416 737.28 512 737.28z"></path></svg></div><a href="https://github.com/1kuzus/1kuzus.github.io" target="_blank" rel="noreferrer"><div id="header-github-bg"><svg viewBox="0 0 1024 1024" width="36px" height="36px" xmlns="http://www.w3.org/2000/svg"><path d="M411.306667 831.146667c3.413333-5.12 6.826667-10.24 6.826666-11.946667v-69.973333c-105.813333 22.186667-128-44.373333-128-44.373334-17.066667-44.373333-42.666667-56.32-42.666666-56.32-34.133333-23.893333 3.413333-23.893333 3.413333-23.893333 37.546667 3.413333 58.026667 39.253333 58.026667 39.253333 34.133333 58.026667 88.746667 40.96 110.933333 32.426667 3.413333-23.893333 13.653333-40.96 23.893333-51.2-85.333333-10.24-174.08-42.666667-174.08-187.733333 0-40.96 15.36-75.093333 39.253334-102.4-3.413333-10.24-17.066667-47.786667 3.413333-100.693334 0 0 32.426667-10.24 104.106667 39.253334 30.72-8.533333 63.146667-11.946667 95.573333-11.946667 32.426667 0 64.853333 5.12 95.573333 11.946667 73.386667-49.493333 104.106667-39.253333 104.106667-39.253334 20.48 52.906667 8.533333 90.453333 3.413333 100.693334 23.893333 27.306667 39.253333 59.733333 39.253334 102.4 0 145.066667-88.746667 177.493333-174.08 187.733333 13.653333 11.946667 25.6 34.133333 25.6 69.973333v104.106667c0 3.413333 1.706667 6.826667 6.826666 11.946667 5.12 6.826667 3.413333 18.773333-3.413333 23.893333-3.413333 1.706667-6.826667 3.413333-10.24 3.413333h-174.08c-10.24 0-17.066667-6.826667-17.066667-17.066666 0-5.12 1.706667-8.533333 3.413334-10.24z" fill="#FCFCFC"></path></svg></div></a></div></div><div id="blog-layout"><div id="main"><h1 class="x-title">/longtime/papers/</h1><h1 class="x-title"></h1><h2 class="x-h1">研究</h2><h3 class="x-h2"><a href="https://arxiv.org/pdf/2003.08934.pdf" target="_blank" rel="noreferrer">【NeRF】NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis (2020)</a></h3><div class="x-highlightblock highlight-background-gray"><div class="x-oli"><div class="x-oli-number">1.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">使用稀疏输入2D图片集实现场景的3D视图合成</p></div></div><div class="x-oli"><div class="x-oli-number">2.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的核心方法 / 具体是如何做的？</span></p><p class="x-p">从<code class="x-inline-highlight">(<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">ϕ</span></span></span></span>)</code>到<code class="x-inline-highlight">(<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">G</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span></span>)</code></p></div></div><div class="x-oli"><div class="x-oli-number">3.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">做了什么实验，效果怎么样？</span></p><p class="x-p">测试数据集是<code class="x-inline-highlight">Diffuse Synthetic 360</code>、<code class="x-inline-highlight">Realistic Synthetic 360</code>和<code class="x-inline-highlight">Real Forward-Facing</code></p></div></div><div class="x-oli"><div class="x-oli-number">4.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">研究的创新点</span></p><p class="x-p">将输入坐标位置编码，帮助MLP表示高频函数<br/>分层采样</p></div></div><div class="x-oli"><div class="x-oli-number">5.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">有什么限制或可以改进的地方？</span></p><p class="x-p">有效地优化和渲染神经辐射场<br/>可解释性</p></div></div></div><div class="x-highlightblock highlight-background-blue"><h4 class="x-h3">更多笔记</h4><p class="x-p">神经辐射场用于从2D的图片重建3D的场景。</p><p class="x-p no-margin-bottom">文中出现的三个指标：PSNR、SSIM、LPIPS</p><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><span class="x-inline-strong">峰值信噪比</span><code class="x-inline-highlight">(Peak Signal to Noise Ratio, PSNR)</code>：用于衡量图像恢复的质量，数值越高表示图像质量越好。接近<code class="x-inline-highlight">50 dB</code>代表误差非常小，大于<code class="x-inline-highlight">30 dB</code>人眼难察觉差异。</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><span class="x-inline-strong">结构相似性</span><code class="x-inline-highlight">(Structural Similarity Index Measure, SSIM)</code>：用于衡量图像的结构相似性，得分通常在<code class="x-inline-highlight">0</code>~<code class="x-inline-highlight">1</code>之间，数值越高表示图像结构越相似。相较于PSNR在图像质量的衡量上更能符合人眼对图像质量的判断。</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><span class="x-inline-strong">基于学习的感知图像质量评价</span><code class="x-inline-highlight">(Learned Perceptual Image Patch Similarity, LPIPS)</code>：测量从预训练网络中提取的两个图像的特征之间的相似性，得分通常在<code class="x-inline-highlight">0</code>~<code class="x-inline-highlight">1</code>之间，数值越低表示感知质量越高。</p></div></div></div><h3 class="x-h2"><a href="https://arxiv.org/pdf/2308.04079.pdf" target="_blank" rel="noreferrer">【3DGS】3D Gaussian Splatting for Real-Time Radiance Field Rendering (2023)</a></h3><div class="x-highlightblock highlight-background-gray"><div class="x-oli"><div class="x-oli-number">6.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">实现实时辐射场渲染，同时保持高质量的视觉效果，并且保持较短的训练时间</p></div></div></div><div class="x-highlightblock highlight-background-blue"><h4 class="x-h3">更多笔记</h4><h4 class="x-h3">文章的相关工作部分</h4><p class="x-p">传统的场景重建与渲染：基于光场的，密集采样、非结构化捕获；<span class="x-inline-strong">运动恢复结构</span><code class="x-inline-highlight">(Structure from Motion, SFM)</code>用一组照片估计稀疏点云合成新视图；<span class="x-inline-strong">多视点立体视觉</span><code class="x-inline-highlight">(Multi-View Stereo, MVS)</code>；<br/> 神经渲染和辐射场：用CNN估计混合权重，用于纹理空间；Soft3D提出<code class="x-inline-highlight">Volumetric representations</code>；NeRF提出重要性采样和位置编码来提高质量，但使用了大型多层感知器，对速度有负面影响；</p><h4 class="x-h3">稀疏重建和稠密重建</h4><p class="x-p">稀疏重建主要用于定位，得到每张图片的相机参数，提取特征点，例如SFM；稠密重建是假设相机参数已知的情况下，从不同视角的图像中找到匹配的对应点，对整个图像或图像中绝大部分像素进行重建。</p></div><h3 class="x-h2"><a href="https://arxiv.org/pdf/2201.05989.pdf" target="_blank" rel="noreferrer">【Instant NGP】Instant Neural Graphics Primitives with a Multiresolution Hash Encoding (2022)</a></h3><div class="x-highlightblock highlight-background-gray"><div class="x-oli"><div class="x-oli-number">7.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">实现实时辐射场渲染，同时保持高质量的视觉效果，并且保持较短的训练时间</p></div></div></div><div class="x-highlightblock highlight-background-blue"><h4 class="x-h3">更多笔记</h4><h4 class="x-h3">Instant NGP与NeRF的异同</h4><p class="x-p no-margin-bottom">转载自<a href="https://zhuanlan.zhihu.com/p/631284285" target="_blank" rel="noreferrer" class="x-inline-link">知乎：从NeRF到Instant-NGP</a></p><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p">同样基于体渲染</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p">不同于NeRF的MLP，Instant NGP使用稀疏的参数化的<code class="x-inline-highlight">voxel grid</code>作为场景表达</p></div></div></div><h3 class="x-h2"><a href="https://arxiv.org/pdf/2211.11646.pdf" target="_blank" rel="noreferrer">【NeRF RPN】NeRF-RPN: A general framework for object detection in NeRFs (2022)</a></h3><div class="x-highlightblock highlight-background-gray"><div class="x-oli"><div class="x-oli-number">8.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">在NeRF中直接进行3D物体检测</p></div></div><div class="x-oli"><div class="x-oli-number">9.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的核心方法 / 具体是如何做的？</span></p><p class="x-p">第一部分：特征提取器<br/>从NeRF采样的辐射度和密度网格作为输入，生成特征金字塔作为输出。<br/> 第二部分：RPN头<br/>对特征金字塔进行操作并生成对象建议。</p></div></div><div class="x-oli"><div class="x-oli-number">10.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">研究的创新点</span></p><p class="x-p">第一次将RPN引入NeRF以进行3D物体检测和相关任务<br/>利用Hypersim和3D-FRONT数据集构建了第一个用于3D目标检测的NeRF数据集</p></div></div></div><h3 class="x-h2"><a href="https://arxiv.org/pdf/2304.04395v3.pdf" target="_blank" rel="noreferrer">【Instance NeRF】Instance Neural Radiance Field (2023)</a></h3><div class="x-highlightblock highlight-background-gray"><div class="x-oli"><div class="x-oli-number">11.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">输入一个以多视图RGB图像预训练的NeRF，学习给定场景的3D实例分割</p><p class="x-p no-margin-bottom">文章的主要贡献：</p><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p">第一个在NeRF中进行3D实例分割的尝试之一，而没有使用真实分割标签作为输入</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p">提出<code class="x-inline-highlight">Neural Instance Field</code>的结构和训练方法，可以产生<span class="x-inline-strong">多视图一致</span>的2D分割和连续的3D分割</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p">对合成室内NeRF数据集进行实验和消融研究</p></div></div></div></div><div class="x-oli"><div class="x-oli-number">12.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的核心方法 / 具体是如何做的？</span></p><p class="x-p">Instance NeRF有两个组件：预训练的NeRF模型、和文中提出的<code class="x-inline-highlight">Instance Field</code>。</p><p class="x-p"><code class="x-inline-highlight">Instance Field</code>的训练过程如下：</p><div class="x-image-wrapper x-image-invert"><img alt="img" loading="lazy" width="1458" height="500" decoding="async" data-nimg="1" style="color:transparent;width:96%;height:auto" src="/_next/static/media/instance_field.500af71e.jpg"/></div><p class="x-p">NeRF-RCNN用预训练的NeRF提取的辐射场和密度场，为每个检测到的对象输出3D掩码；Mask2Former生成从NeRF渲染的图像的二维全景分割图（跨视图的实例标签并不一定一致）。然后按照相机位置，投影3D掩码去匹配不同视图中的相同实例，去产生多视图一致的2D分割图。CascadePSP用于细化2D mask。</p></div></div></div><div class="x-highlightblock highlight-background-blue"><h4 class="x-h3">更多笔记</h4><h4 class="x-h3">包围体：AABB和OBB</h4><p class="x-p"><span class="x-inline-strong">AABB</span>：轴对齐包围盒<code class="x-inline-highlight">(Axis-Aligned Bounding Box)</code><br/><span class="x-inline-strong">OBB</span>：有向包围盒<code class="x-inline-highlight">(Oriented Bounding Box)</code></p><p class="x-p">下图展示了更多种类的包围体：</p><div class="x-image-wrapper x-image-invert"><img alt="img" loading="lazy" width="850" height="323" decoding="async" data-nimg="1" style="color:transparent;width:100%;height:auto" src="/_next/static/media/bounding_volumes.e98c20aa.png"/></div></div><h3 class="x-h2"><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Cheng_CascadePSP_Toward_Class-Agnostic_and_Very_High-Resolution_Segmentation_via_Global_and_CVPR_2020_paper.pdf" target="_blank" rel="noreferrer">【CascadePSP】CascadePSP: Toward Class-Agnostic and Very High-Resolution Segmentation via Global and Local Refinement (2020)</a></h3><div class="x-highlightblock highlight-background-gray"><div class="x-oli"><div class="x-oli-number">13.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">提出一种不使用高分辨率训练数据，解决高分辨率分割问题的方法<br/>右图是改进后的结果：</p><div class="x-image-wrapper x-image-invert"><img alt="img" loading="lazy" width="747" height="542" decoding="async" data-nimg="1" style="color:transparent;width:400px;height:auto" src="/_next/static/media/cascadepsp.0d9b618b.jpg"/></div></div></div></div><h3 class="x-h2"><a href="https://arxiv.org/pdf/2312.00860.pdf" target="_blank" rel="noreferrer">【SAGA】Segment Any 3D Gaussians (2023)</a></h3><div class="x-highlightblock highlight-background-gray"><div class="x-oli"><div class="x-oli-number">14.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">交互式3D分割</p></div></div></div><h2 class="x-h1">学习</h2><h3 class="x-h2"><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf" target="_blank" rel="noreferrer">【R-CNN】Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation (2014)</a></h3><div class="x-highlightblock highlight-background-gray"><div class="x-oli"><div class="x-oli-number">15.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">提出<code class="x-inline-highlight">Regions with CNN features, R-CNN</code>提高目标检测性能</p></div></div><div class="x-oli"><div class="x-oli-number">16.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的核心方法 / 具体是如何做的？</span></p><p class="x-p">区域提议<code class="x-inline-highlight">(Region Proposals)</code>：使用<code class="x-inline-highlight">selective search</code>生成候选框</p></div></div><div class="x-oli"><div class="x-oli-number">17.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">做了什么实验，效果怎么样？</span></p><p class="x-p">在<code class="x-inline-highlight">PASCAL VOC 2012</code>取得<code class="x-inline-highlight">mAP 53.3%</code>，在<code class="x-inline-highlight">ILSVRC 2013</code>竞赛数据集取得<code class="x-inline-highlight">mAP 31.4%</code></p></div></div></div><div class="x-highlightblock highlight-background-blue"><h4 class="x-h3">更多笔记</h4><p class="x-p">转载自<a href="https://zh-v2.d2l.ai/chapter_computer-vision/rcnn.html" target="_blank" rel="noreferrer" class="x-inline-link">动手学深度学习 - 区域卷积神经网络系列</a></p><h4 class="x-h3">R-CNN</h4><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p">R-CNN使用启发式搜索算法<code class="x-inline-highlight">selective search</code>（之前人们通常也是这样做的）来选择锚框</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p">用预训练的模型，对每一个锚框抽取特征</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p">训练一个SVM来对类别分类</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p">训练一个线性回归模型来预测边缘框</p></div></div><p class="x-p">R-CNN的速度很慢，因为可能从一张图像中选出上千个提议区域，这需要上千次的卷积神经网络的前向传播来执行目标检测。这种庞大的计算量使得R-CNN在现实世界中难以被广泛应用。</p><div class="x-image-wrapper"><img alt="img" loading="lazy" width="1563" height="467" decoding="async" data-nimg="1" style="color:transparent;width:600px;height:auto" src="/_next/static/media/rcnn.09a38e8a.jpg"/></div><h4 class="x-h3">Fast R-CNN</h4><p class="x-p">R-CNN的主要性能瓶颈在于，对每个提议区域，卷积神经网络的前向传播是独立的，而没有共享计算。由于这些区域通常有重叠，独立的特征抽取会导致重复的计算。Fast R-CNN的主要改进之一，是仅在整张图象上执行卷积神经网络的前向传播，并且引入<span class="x-inline-strong">兴趣区域池化</span><code class="x-inline-highlight">(ROI Pooling)</code>，将卷积神经网络的输出和提议区域作为输入，输出连结后的各个提议区域抽取的特征。</p><p class="x-p">兴趣区域池化层可以给出固定大小的输出：把给定的锚框均匀分割成<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">m</span></span></span></span>块，输出每块里的最大值，这样无论锚框多大，总是输出<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">nm</span></span></span></span>个值。</p><p class="x-p">Fast R-CNN先对图片用CNN抽取特征，然后将<code class="x-inline-highlight">selective search</code>给出的原图上的提议区域映射到CNN特征图上，再经过<code class="x-inline-highlight">ROI Pooling</code>就可以得到维度对齐的特征。</p><div class="x-image-wrapper"><img alt="img" loading="lazy" width="917" height="838" decoding="async" data-nimg="1" style="color:transparent;width:400px;height:auto" src="/_next/static/media/fastrcnn.46850bc6.jpg"/></div><h4 class="x-h3">Faster R-CNN</h4><p class="x-p">与Fast R-CNN相比，Faster R-CNN将生成提议区域的方法从<code class="x-inline-highlight">selective search</code>改为了<span class="x-inline-strong">区域提议网络</span><code class="x-inline-highlight">(Region Proposal Network, RPN)</code>，模型的其余部分保持不变。区域提议网络作为Faster R-CNN模型的一部分，是和整个模型一起训练得到的。换句话说，Faster R-CNN的目标函数不仅包括目标检测中的类别和边界框预测，还包括区域提议网络中锚框的二元类别和边界框预测。作为端到端训练的结果，区域提议网络能够学习到如何生成高质量的提议区域，从而在减少了从数据中学习的提议区域的数量的情况下，仍保持目标检测的精度。</p><div class="x-image-wrapper"><img alt="img" loading="lazy" width="1579" height="871" decoding="async" data-nimg="1" style="color:transparent;width:600px;height:auto" src="/_next/static/media/fasterrcnn.1f4436bb.jpg"/></div></div><h3 class="x-h2"><a href="https://arxiv.org/pdf/1703.06870.pdf" target="_blank" rel="noreferrer">【Mask R-CNN】Mask R-CNN (2017)</a></h3><div class="x-highlightblock highlight-background-gray"><div class="x-oli"><div class="x-oli-number">18.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">目标检测&实例分割</p></div></div><div class="x-oli"><div class="x-oli-number">19.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的核心方法 / 具体是如何做的？</span></p><p class="x-p">基于Faster R-CNN，添加一个对每个ROI预测分割mask的分支。这个分支可以与分类和边界框预测分支并行。</p><div class="x-image-wrapper"><img alt="img" loading="lazy" width="676" height="342" decoding="async" data-nimg="1" style="color:transparent;width:600px;height:auto" src="/_next/static/media/maskrcnn1.6cdb9105.jpg"/></div></div></div><div class="x-oli"><div class="x-oli-number">20.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">做了什么实验，效果怎么样？</span></p><div class="x-image-wrapper x-image-invert"><img alt="img" loading="lazy" width="763" height="184" decoding="async" data-nimg="1" style="color:transparent;width:600px;height:auto" src="/_next/static/media/maskrcnn2.fe26f764.jpg"/></div><p class="x-p">在COCO数据集上表现优异，超过了<code class="x-inline-highlight">2015</code>和<code class="x-inline-highlight">2016</code>年COCO分割任务的冠军。</p></div></div><div class="x-oli"><div class="x-oli-number">21.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">研究的创新点</span></p><p class="x-p"> <code class="x-inline-highlight">ROI Pooling</code>中发生了两次浮点数取整，第一次是将锚框均匀分割成<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">m</span></span></span></span>块时，第二次是把提议区域映射回CNN特征图时。<br/> 分割任务的精细程度更高，因此文章提出了<code class="x-inline-highlight">ROI Align</code>，使用双线性插值来保留特征图上的空间信息。 </p></div></div></div><div class="x-highlightblock highlight-background-blue"><h4 class="x-h3">更多笔记</h4><h4 class="x-h3">语义分割与实例分割</h4><div class="x-image-wrapper"><img alt="img" loading="lazy" width="1243" height="370" decoding="async" data-nimg="1" style="color:transparent;width:600px;height:auto" src="/_next/static/media/ss_and_is.f6df7378.jpg"/></div><p class="x-p no-margin-bottom"><span class="x-inline-strong">语义分割</span><code class="x-inline-highlight">(semantic segmentation)</code>：为每一个像素分配一个类别，但不区分同一类别之间的对象。</p><p class="x-p"><span class="x-inline-strong">实例分割</span><code class="x-inline-highlight">(instance segmentation)</code>：会区分属于同一类别的不同实例。</p></div><h3 class="x-h2"><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf" target="_blank" rel="noreferrer">【FCN】Fully Convolutional Networks for Semantic Segmentation (2015)</a></h3><div class="x-highlightblock highlight-background-gray"><div class="x-oli"><div class="x-oli-number">22.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">使用全卷积网络进行语义分割</p></div></div><div class="x-oli"><div class="x-oli-number">23.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的核心方法 / 具体是如何做的？</span></p><p class="x-p"> 是将现有的分类网络（AlexNet、VGG Net、GoogLeNet）改造为全卷积网络，以便在语义分割任务上进行端到端（输入图像，输出分割掩码）的训练。<br/> 改造的方式是将最后的全连接层替换成卷积层。 </p></div></div></div><div class="x-highlightblock highlight-background-blue"><h4 class="x-h3">更多笔记</h4><h4 class="x-h3">转置卷积</h4><p class="x-p">卷积通常不会增大输入的高宽，而是保持不变或降低。由于语义分割任务需要像素级别的输出，转置卷积被用来增大输入的高宽。</p><div class="x-image-wrapper x-image-invert"><img alt="img" loading="lazy" width="890" height="393" decoding="async" data-nimg="1" style="color:transparent;width:600px;height:auto" src="/_next/static/media/transpose_convolution.fb99cc55.jpg"/></div><p class="x-p">图片转载自<a href="https://zh-v2.d2l.ai/chapter_computer-vision/transposed-conv.html" target="_blank" rel="noreferrer" class="x-inline-link">动手学深度学习 - 转置卷积</a></p><h4 class="x-h3">FCN中的转置卷积</h4><p class="x-p">例如对于ImageNet的图片输入，大小通常为<code class="x-inline-highlight">224</code>&#42;<code class="x-inline-highlight">224</code>&#42;<code class="x-inline-highlight">3</code>（RGB通道）；经过卷积后缩小宽高缩小<code class="x-inline-highlight">32</code>倍，通道增加到<code class="x-inline-highlight">512</code>，变成<code class="x-inline-highlight">7</code>&#42;<code class="x-inline-highlight">7</code>&#42;<code class="x-inline-highlight">512</code>的特征图。此时FCN会先通过一个<code class="x-inline-highlight">1x1conv</code>进行通道降维，然后通过转置卷积将特征图的高度和宽度增加<code class="x-inline-highlight">32</code>倍，<span class="x-inline-strong">输出通道数等于类别数</span>，相当于储存了对每一类的预测结果。</p></div></div><div id="sidebar"><div class="sidebar-list show-list"><div class="sidebar-list-head"><h3 class="sidebar-list-category">网络杂识 (4)</h3><div class="sidebar-list-category-rightarrow"><svg viewBox="0 0 1024 1024" width="16px" height="16px" xmlns="http://www.w3.org/2000/svg"><path d="M761.056 532.128c.512-.992 1.344-1.824 1.792-2.848 8.8-18.304 5.92-40.704-9.664-55.424L399.936 139.744c-19.264-18.208-49.632-17.344-67.872 1.888-18.208 19.264-17.376 49.632 1.888 67.872l316.96 299.84L335.2 813.632c-19.072 18.4-19.648 48.768-1.248 67.872 9.408 9.792 21.984 14.688 34.56 14.688 12 0 24-4.48 33.312-13.44l350.048-337.376c.672-.672.928-1.6 1.6-2.304.512-.48 1.056-.832 1.568-1.344 2.72-2.848 4.16-6.336 6.016-9.6z" fill="#50505c"></path></svg></div></div><div class="sidebar-list-ul-wrapper"><ul class="sidebar-list-ul"><li class="sidebar-list-li"><a class="sidebar-list-link" href="/23d/database-3nf/"><span class="sidebar-list-title">数据库设计三大范式</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/23d/github-linguist-vendored/"><span class="sidebar-list-title">不统计Github仓库某个目录下的语言</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/24a/deepl-shortcut-setting/"><span class="sidebar-list-title">解决：DeepL该快捷键已被使用</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/24a/git-merge-allow-unrelated-histories/"><span class="sidebar-list-title">记录：使用--allow-unrelated-histories</span></a></li></ul></div></div><div class="sidebar-list show-list"><div class="sidebar-list-head"><h3 class="sidebar-list-category">深度学习 (5)</h3><div class="sidebar-list-category-rightarrow"><svg viewBox="0 0 1024 1024" width="16px" height="16px" xmlns="http://www.w3.org/2000/svg"><path d="M761.056 532.128c.512-.992 1.344-1.824 1.792-2.848 8.8-18.304 5.92-40.704-9.664-55.424L399.936 139.744c-19.264-18.208-49.632-17.344-67.872 1.888-18.208 19.264-17.376 49.632 1.888 67.872l316.96 299.84L335.2 813.632c-19.072 18.4-19.648 48.768-1.248 67.872 9.408 9.792 21.984 14.688 34.56 14.688 12 0 24-4.48 33.312-13.44l350.048-337.376c.672-.672.928-1.6 1.6-2.304.512-.48 1.056-.832 1.568-1.344 2.72-2.848 4.16-6.336 6.016-9.6z" fill="#50505c"></path></svg></div></div><div class="sidebar-list-ul-wrapper"><ul class="sidebar-list-ul"><li class="sidebar-list-li"><a class="sidebar-list-link" href="/longtime/papers/"><span class="sidebar-list-title">论文速记</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/23d/r2plus1d/"><span class="sidebar-list-title">行为识别R(2+1)D网络</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/23d/object-detection-map/"><span class="sidebar-list-title">目标检测评价指标mAP</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/23d/learn-rnn-lstm/"><span class="sidebar-list-title">学习RNN和LSTM</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/24a/reproduce-nerf-rpn/"><span class="sidebar-list-title">记录：复现NeRF-RPN代码</span></a></li></ul></div></div><div class="sidebar-list show-list"><div class="sidebar-list-head"><h3 class="sidebar-list-category">Python学习 (2)</h3><div class="sidebar-list-category-rightarrow"><svg viewBox="0 0 1024 1024" width="16px" height="16px" xmlns="http://www.w3.org/2000/svg"><path d="M761.056 532.128c.512-.992 1.344-1.824 1.792-2.848 8.8-18.304 5.92-40.704-9.664-55.424L399.936 139.744c-19.264-18.208-49.632-17.344-67.872 1.888-18.208 19.264-17.376 49.632 1.888 67.872l316.96 299.84L335.2 813.632c-19.072 18.4-19.648 48.768-1.248 67.872 9.408 9.792 21.984 14.688 34.56 14.688 12 0 24-4.48 33.312-13.44l350.048-337.376c.672-.672.928-1.6 1.6-2.304.512-.48 1.056-.832 1.568-1.344 2.72-2.848 4.16-6.336 6.016-9.6z" fill="#50505c"></path></svg></div></div><div class="sidebar-list-ul-wrapper"><ul class="sidebar-list-ul"><li class="sidebar-list-li"><a class="sidebar-list-link" href="/24a/torch-numpy-topk/"><span class="sidebar-list-title">在pytorch和numpy中取top-k值和索引</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/24a/object-oriented-programming-python/"><span class="sidebar-list-title">Python面向对象编程</span></a></li></ul></div></div><div class="sidebar-list show-list"><div class="sidebar-list-head"><h3 class="sidebar-list-category">前端与JavaScript (2)</h3><div class="sidebar-list-category-rightarrow"><svg viewBox="0 0 1024 1024" width="16px" height="16px" xmlns="http://www.w3.org/2000/svg"><path d="M761.056 532.128c.512-.992 1.344-1.824 1.792-2.848 8.8-18.304 5.92-40.704-9.664-55.424L399.936 139.744c-19.264-18.208-49.632-17.344-67.872 1.888-18.208 19.264-17.376 49.632 1.888 67.872l316.96 299.84L335.2 813.632c-19.072 18.4-19.648 48.768-1.248 67.872 9.408 9.792 21.984 14.688 34.56 14.688 12 0 24-4.48 33.312-13.44l350.048-337.376c.672-.672.928-1.6 1.6-2.304.512-.48 1.056-.832 1.568-1.344 2.72-2.848 4.16-6.336 6.016-9.6z" fill="#50505c"></path></svg></div></div><div class="sidebar-list-ul-wrapper"><ul class="sidebar-list-ul"><li class="sidebar-list-li"><a class="sidebar-list-link" href="/23c/js-array/"><span class="sidebar-list-title">JavaScript数组常用方法</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/23d/css-auto-height-transition/"><span class="sidebar-list-title">CSS实现auto高度的过渡动画</span></a></li></ul></div></div><div class="sidebar-list show-list"><div class="sidebar-list-head"><h3 class="sidebar-list-category">课程 (10)</h3><div class="sidebar-list-category-rightarrow"><svg viewBox="0 0 1024 1024" width="16px" height="16px" xmlns="http://www.w3.org/2000/svg"><path d="M761.056 532.128c.512-.992 1.344-1.824 1.792-2.848 8.8-18.304 5.92-40.704-9.664-55.424L399.936 139.744c-19.264-18.208-49.632-17.344-67.872 1.888-18.208 19.264-17.376 49.632 1.888 67.872l316.96 299.84L335.2 813.632c-19.072 18.4-19.648 48.768-1.248 67.872 9.408 9.792 21.984 14.688 34.56 14.688 12 0 24-4.48 33.312-13.44l350.048-337.376c.672-.672.928-1.6 1.6-2.304.512-.48 1.056-.832 1.568-1.344 2.72-2.848 4.16-6.336 6.016-9.6z" fill="#50505c"></path></svg></div></div><div class="sidebar-list-ul-wrapper"><ul class="sidebar-list-ul"><li class="sidebar-list-li"><a class="sidebar-list-link" href="/23c/pattern-recognition-1/"><span class="sidebar-list-title">【模式识别】统计决策方法</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/23c/pattern-recognition-2/"><span class="sidebar-list-title">【模式识别】参数估计</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/23c/pattern-recognition-3/"><span class="sidebar-list-title">【模式识别】非参数估计</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/23d/pattern-recognition-4/"><span class="sidebar-list-title">【模式识别】线性学习器与线性分类器</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/23d/protocols/"><span class="sidebar-list-title">【计算机网络】协议总结</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/24a/machine-learning-exercises/"><span class="sidebar-list-title">【机器学习】习题</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/24a/games101-01-transformation/"><span class="sidebar-list-title">【GAMES101】Transformation</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/24a/games101-02-rasterization/"><span class="sidebar-list-title">【GAMES101】Rasterization</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/24a/games101-03-shading/"><span class="sidebar-list-title">【GAMES101】Shading</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/24a/games101-04-geometry/"><span class="sidebar-list-title">【GAMES101】Geometry</span></a></li></ul></div></div><div class="sidebar-list show-list"><div class="sidebar-list-head"><h3 class="sidebar-list-category">其他 (2)</h3><div class="sidebar-list-category-rightarrow"><svg viewBox="0 0 1024 1024" width="16px" height="16px" xmlns="http://www.w3.org/2000/svg"><path d="M761.056 532.128c.512-.992 1.344-1.824 1.792-2.848 8.8-18.304 5.92-40.704-9.664-55.424L399.936 139.744c-19.264-18.208-49.632-17.344-67.872 1.888-18.208 19.264-17.376 49.632 1.888 67.872l316.96 299.84L335.2 813.632c-19.072 18.4-19.648 48.768-1.248 67.872 9.408 9.792 21.984 14.688 34.56 14.688 12 0 24-4.48 33.312-13.44l350.048-337.376c.672-.672.928-1.6 1.6-2.304.512-.48 1.056-.832 1.568-1.344 2.72-2.848 4.16-6.336 6.016-9.6z" fill="#50505c"></path></svg></div></div><div class="sidebar-list-ul-wrapper"><ul class="sidebar-list-ul"><li class="sidebar-list-li"><a class="sidebar-list-link" href="/longtime/demo/"><span class="sidebar-list-title">示例</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/longtime/updates/"><span class="sidebar-list-title">更新日志</span></a></li></ul></div></div></div><div id="sidebar-mask"></div></div><script src="/_next/static/chunks/webpack-70957058d2eabc37.js" crossorigin="" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/css/8ac0b0e7f508d966.css\",\"style\",{\"crossOrigin\":\"\"}]\n0:\"$L2\"\n"])</script><script>self.__next_f.push([1,"3:HL[\"/_next/static/css/d1ba65e78d6f432c.css\",\"style\",{\"crossOrigin\":\"\"}]\n4:HL[\"/_next/static/css/f70c26425a044d6b.css\",\"style\",{\"crossOrigin\":\"\"}]\n"])</script><script>self.__next_f.push([1,"5:I[3728,[],\"\"]\n7:I[9928,[],\"\"]\n8:I[9226,[\"185\",\"static/chunks/app/layout-959d1352bec0edcb.js\"],\"GlobalProvider\"]\n9:I[9330,[\"185\",\"static/chunks/app/layout-959d1352bec0edcb.js\"],\"\"]\na:I[6954,[],\"\"]\nb:I[7264,[],\"\"]\nc:I[413,[\"413\",\"static/chunks/413-dc980fd24c15cede.js\",\"160\",\"static/chunks/app/not-found-aa1272efa852a321.js\"],\"Image\"]\nd:I[5285,[\"954\",\"static/chunks/d3ac728e-1e5d8b71e3d43fec.js\",\"413\",\"static/chunks/413-dc980fd24c15cede.js\",\"202\",\"static/chunks/202-268c3b9a2ea32d77.js\",\"396\",\"static/chunks/396"])</script><script>self.__next_f.push([1,"-150eb8004cde3d82.js\",\"135\",\"static/chunks/app/(blogs)/layout-39f18c96d96b8978.js\"],\"\"]\nf:I[4484,[\"954\",\"static/chunks/d3ac728e-1e5d8b71e3d43fec.js\",\"413\",\"static/chunks/413-dc980fd24c15cede.js\",\"202\",\"static/chunks/202-268c3b9a2ea32d77.js\",\"487\",\"static/chunks/app/(blogs)/longtime/papers/page-81ce2c3df2c4378d.js\"],\"Title\"]\n10:I[4484,[\"954\",\"static/chunks/d3ac728e-1e5d8b71e3d43fec.js\",\"413\",\"static/chunks/413-dc980fd24c15cede.js\",\"202\",\"static/chunks/202-268c3b9a2ea32d77.js\",\"487\",\"static/chunks/app/(blogs)"])</script><script>self.__next_f.push([1,"/longtime/papers/page-81ce2c3df2c4378d.js\"],\"H1\"]\n11:I[4484,[\"954\",\"static/chunks/d3ac728e-1e5d8b71e3d43fec.js\",\"413\",\"static/chunks/413-dc980fd24c15cede.js\",\"202\",\"static/chunks/202-268c3b9a2ea32d77.js\",\"487\",\"static/chunks/app/(blogs)/longtime/papers/page-81ce2c3df2c4378d.js\"],\"H2\"]\n12:I[8275,[\"954\",\"static/chunks/d3ac728e-1e5d8b71e3d43fec.js\",\"413\",\"static/chunks/413-dc980fd24c15cede.js\",\"202\",\"static/chunks/202-268c3b9a2ea32d77.js\",\"487\",\"static/chunks/app/(blogs)/longtime/papers/page-81ce2c3df2c4378d.j"])</script><script>self.__next_f.push([1,"s\"],\"Oli\"]\n14:I[4484,[\"954\",\"static/chunks/d3ac728e-1e5d8b71e3d43fec.js\",\"413\",\"static/chunks/413-dc980fd24c15cede.js\",\"202\",\"static/chunks/202-268c3b9a2ea32d77.js\",\"487\",\"static/chunks/app/(blogs)/longtime/papers/page-81ce2c3df2c4378d.js\"],\"H3\"]\n15:I[8275,[\"954\",\"static/chunks/d3ac728e-1e5d8b71e3d43fec.js\",\"413\",\"static/chunks/413-dc980fd24c15cede.js\",\"202\",\"static/chunks/202-268c3b9a2ea32d77.js\",\"487\",\"static/chunks/app/(blogs)/longtime/papers/page-81ce2c3df2c4378d.js\"],\"Uli\"]\n16:I[8411,[\"954\",\"static/chu"])</script><script>self.__next_f.push([1,"nks/d3ac728e-1e5d8b71e3d43fec.js\",\"413\",\"static/chunks/413-dc980fd24c15cede.js\",\"202\",\"static/chunks/202-268c3b9a2ea32d77.js\",\"487\",\"static/chunks/app/(blogs)/longtime/papers/page-81ce2c3df2c4378d.js\"],\"\"]\n13:T645,"])</script><script>self.__next_f.push([1,"从\u003ccode class=\"x-inline-highlight\"\u003e(\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ex\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003ey\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.04398em;\"\u003ez\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\"\u003eθ\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eϕ\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e)\u003c/code\u003e到\u003ccode class=\"x-inline-highlight\"\u003e(\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8778em;vertical-align:-0.1944em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.00773em;\"\u003eR\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eG\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.05017em;\"\u003eB\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003eσ\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e)\u003c/code\u003e"])</script><script>self.__next_f.push([1,"2:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/8ac0b0e7f508d966.css\",\"precedence\":\"next\",\"crossOrigin\":\"\"}]],[\"$\",\"$L5\",null,{\"buildId\":\"F8tpvN5-yNKUAmwFuXero\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/longtime/papers/\",\"initialTree\":[\"\",{\"children\":[\"(blogs)\",{\"children\":[\"longtime\",{\"children\":[\"papers\",{\"children\":[\"__PAGE__\",{}]}]}]}]},\"$undefined\",\"$undefined\",true],\"initialHead\":[false,\"$L6\"],\"globalErrorComponent\":\"$7\",\"children\":[null,[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"children\":[\"$\",\"$L8\",null,{\"children\":[[\"$\",\"$L9\",null,{}],[\"$\",\"$La\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$Lb\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":[\"$\",\"div\",null,{\"id\":\"notfound\",\"children\":[[\"$\",\"$Lc\",null,{\"alt\":\"img\",\"width\":true,\"height\":true,\"src\":\"/next.svg\",\"style\":{\"width\":500,\"height\":\"auto\"}}],[\"$\",\"div\",null,{\"id\":\"notfound-404\",\"children\":\"404\"}],[\"$\",\"div\",null,{\"id\":\"notfound-text\",\"children\":\"Page Not Found\"}]]}],\"notFoundStyles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/eccd2e7a1149e571.css\",\"precedence\":\"next\",\"crossOrigin\":\"\"}]],\"childProp\":{\"current\":[null,[\"$\",\"$Ld\",null,{\"children\":[\"$\",\"$La\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"(blogs)\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$Lb\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"childProp\":{\"current\":[\"$\",\"$La\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"(blogs)\",\"children\",\"longtime\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$Lb\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"childProp\":{\"current\":[\"$\",\"$La\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"(blogs)\",\"children\",\"longtime\",\"children\",\"papers\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$Lb\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"childProp\":{\"current\":[\"$Le\",[[\"$\",\"$Lf\",null,{\"children\":\"$undefined\"}],[\"$\",\"$L10\",null,{\"children\":\"研究\"}],[\"$\",\"$L11\",null,{\"href\":\"https://arxiv.org/pdf/2003.08934.pdf\",\"children\":\"【NeRF】NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis (2020)\"}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-gray\",\"children\":[false,[[\"$\",\"$L12\",\"0\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的主题 / 文章要解决什么问题？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"使用稀疏输入2D图片集实现场景的3D视图合成\"}}]]}],[\"$\",\"$L12\",\"1\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的核心方法 / 具体是如何做的？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"$13\"}}]]}],[\"$\",\"$L12\",\"2\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e做了什么实验，效果怎么样？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"测试数据集是\u003ccode class=\\\"x-inline-highlight\\\"\u003eDiffuse Synthetic 360\u003c/code\u003e、\u003ccode class=\\\"x-inline-highlight\\\"\u003eRealistic Synthetic 360\u003c/code\u003e和\u003ccode class=\\\"x-inline-highlight\\\"\u003eReal Forward-Facing\u003c/code\u003e\"}}]]}],[\"$\",\"$L12\",\"3\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e研究的创新点\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"将输入坐标位置编码，帮助MLP表示高频函数\u003cbr/\u003e分层采样\"}}]]}],[\"$\",\"$L12\",\"4\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e有什么限制或可以改进的地方？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"有效地优化和渲染神经辐射场\u003cbr/\u003e可解释性\"}}]]}]]]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-blue\",\"children\":[[\"$\",\"$L14\",null,{\"children\":\"更多笔记\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"神经辐射场用于从2D的图片重建3D的场景。\"}}],[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"文中出现的三个指标：PSNR、SSIM、LPIPS\"}}],[\"$\",\"$L15\",null,{\"children\":\"*峰值信噪比*`(Peak Signal to Noise Ratio, PSNR)`：--- 用于衡量图像恢复的质量，数值越高表示图像质量越好。接近`50 dB`代表误差非常小，大于`30 dB`--- 人眼难察觉差异。\"}],[\"$\",\"$L15\",null,{\"children\":\"*结构相似性*`(Structural Similarity Index Measure, SSIM)`：--- 用于衡量图像的结构相似性，得分通常在`0`~`1`之间，数值越高表示图像结构越相似。相较于PSNR在图像质量的衡量上更能符合人眼对图像质量的判断。\"}],[\"$\",\"$L15\",null,{\"children\":\"*基于学习的感知图像质量评价*`(Learned Perceptual Image Patch Similarity, LPIPS)`：--- 测量从预训练网络中提取的两个图像的特征之间的相似性，得分通常在`0`~`1`之间，数值越低表示感知质量越高。\"}]]}],[\"$\",\"$L11\",null,{\"href\":\"https://arxiv.org/pdf/2308.04079.pdf\",\"children\":\"【3DGS】3D Gaussian Splatting for Real-Time Radiance Field Rendering (2023)\"}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-gray\",\"children\":[false,[[\"$\",\"$L12\",\"0\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的主题 / 文章要解决什么问题？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"实现实时辐射场渲染，同时保持高质量的视觉效果，并且保持较短的训练时间\"}}]]}],\"$undefined\",\"$undefined\",\"$undefined\",\"$undefined\"]]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-blue\",\"children\":[[\"$\",\"$L14\",null,{\"children\":\"更多笔记\"}],[\"$\",\"$L14\",null,{\"children\":\"文章的相关工作部分\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"传统的场景重建与渲染：基于光场的，密集采样、非结构化捕获；\u003cspan class=\\\"x-inline-strong\\\"\u003e运动恢复结构\u003c/span\u003e\u003ccode class=\\\"x-inline-highlight\\\"\u003e(Structure from Motion, SFM)\u003c/code\u003e用一组照片估计稀疏点云合成新视图；\u003cspan class=\\\"x-inline-strong\\\"\u003e多视点立体视觉\u003c/span\u003e\u003ccode class=\\\"x-inline-highlight\\\"\u003e(Multi-View Stereo, MVS)\u003c/code\u003e；\u003cbr/\u003e 神经渲染和辐射场：用CNN估计混合权重，用于纹理空间；Soft3D提出\u003ccode class=\\\"x-inline-highlight\\\"\u003eVolumetric representations\u003c/code\u003e；NeRF提出重要性采样和位置编码来提高质量，但使用了大型多层感知器，对速度有负面影响；\"}}],[\"$\",\"$L14\",null,{\"children\":\"稀疏重建和稠密重建\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"稀疏重建主要用于定位，得到每张图片的相机参数，提取特征点，例如SFM；稠密重建是假设相机参数已知的情况下，从不同视角的图像中找到匹配的对应点，对整个图像或图像中绝大部分像素进行重建。\"}}]]}],[\"$\",\"$L11\",null,{\"href\":\"https://arxiv.org/pdf/2201.05989.pdf\",\"children\":\"【Instant NGP】Instant Neural Graphics Primitives with a Multiresolution Hash Encoding (2022)\"}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-gray\",\"children\":[false,[[\"$\",\"$L12\",\"0\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的主题 / 文章要解决什么问题？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"实现实时辐射场渲染，同时保持高质量的视觉效果，并且保持较短的训练时间\"}}]]}],\"$undefined\",\"$undefined\",\"$undefined\",\"$undefined\"]]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-blue\",\"children\":[[\"$\",\"$L14\",null,{\"children\":\"更多笔记\"}],[\"$\",\"$L14\",null,{\"children\":\"Instant NGP与NeRF的异同\"}],[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"转载自\u003ca href=\\\"https://zhuanlan.zhihu.com/p/631284285\\\" target=\\\"_blank\\\" rel=\\\"noreferrer\\\" class=\\\"x-inline-link\\\"\u003e知乎：从NeRF到Instant-NGP\u003c/a\u003e\"}}],[\"$\",\"$L15\",null,{\"children\":\"同样基于体渲染\"}],[\"$\",\"$L15\",null,{\"children\":\"不同于NeRF的MLP，Instant NGP使用稀疏的参数化的`voxel grid`作为场景表达\"}]]}],[\"$\",\"$L11\",null,{\"href\":\"https://arxiv.org/pdf/2211.11646.pdf\",\"children\":\"【NeRF RPN】NeRF-RPN: A general framework for object detection in NeRFs (2022)\"}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-gray\",\"children\":[false,[[\"$\",\"$L12\",\"0\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的主题 / 文章要解决什么问题？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"在NeRF中直接进行3D物体检测\"}}]]}],[\"$\",\"$L12\",\"1\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的核心方法 / 具体是如何做的？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"第一部分：特征提取器\u003cbr/\u003e从NeRF采样的辐射度和密度网格作为输入，生成特征金字塔作为输出。\u003cbr/\u003e 第二部分：RPN头\u003cbr/\u003e对特征金字塔进行操作并生成对象建议。\"}}]]}],\"$undefined\",[\"$\",\"$L12\",\"3\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e研究的创新点\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"第一次将RPN引入NeRF以进行3D物体检测和相关任务\u003cbr/\u003e利用Hypersim和3D-FRONT数据集构建了第一个用于3D目标检测的NeRF数据集\"}}]]}],\"$undefined\"]]}],[\"$\",\"$L11\",null,{\"href\":\"https://arxiv.org/pdf/2304.04395v3.pdf\",\"children\":\"【Instance NeRF】Instance Neural Radiance Field (2023)\"}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-gray\",\"children\":[false,[[\"$\",\"$L12\",\"0\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的主题 / 文章要解决什么问题？\u003c/span\u003e\"}}],[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"输入一个以多视图RGB图像预训练的NeRF，学习给定场景的3D实例分割\"}}],[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"文章的主要贡献：\"}}],[\"$\",\"$L15\",null,{\"children\":\"第一个在NeRF中进行3D实例分割的尝试之一，而没有使用真实分割标签作为输入\"}],[\"$\",\"$L15\",null,{\"children\":\"提出`Neural Instance Field`的结构和训练方法，可以产生*多视图一致*的2D分割和连续的3D分割\"}],[\"$\",\"$L15\",null,{\"children\":\"对合成室内NeRF数据集进行实验和消融研究\"}]]]}],[\"$\",\"$L12\",\"1\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的核心方法 / 具体是如何做的？\u003c/span\u003e\"}}],[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"Instance NeRF有两个组件：预训练的NeRF模型、和文中提出的\u003ccode class=\\\"x-inline-highlight\\\"\u003eInstance Field\u003c/code\u003e。\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003ccode class=\\\"x-inline-highlight\\\"\u003eInstance Field\u003c/code\u003e的训练过程如下：\"}}],[\"$\",\"$L16\",null,{\"src\":{\"default\":{\"src\":\"/_next/static/media/instance_field.500af71e.jpg\",\"height\":500,\"width\":1458,\"blurDataURL\":\"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAMACAMBIgACEQEDEQH/xAAnAAEBAAAAAAAAAAAAAAAAAAAABwEBAAAAAAAAAAAAAAAAAAAAAf/aAAwDAQACEAMQAAAAqwD/xAAbEAEAAQUBAAAAAAAAAAAAAAABAwACERNBIf/aAAgBAQABPwBij2CW4V55xr//xAAWEQEBAQAAAAAAAAAAAAAAAAABIQD/2gAIAQIBAT8AVl3/xAAVEQEBAAAAAAAAAAAAAAAAAAAAAf/aAAgBAwEBPwCv/9k=\",\"blurWidth\":8,\"blurHeight\":3}},\"width\":\"96%\",\"invertInDarkTheme\":true}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"NeRF-RCNN用预训练的NeRF提取的辐射场和密度场，为每个检测到的对象输出3D掩码；Mask2Former生成从NeRF渲染的图像的二维全景分割图（跨视图的实例标签并不一定一致）。然后按照相机位置，投影3D掩码去匹配不同视图中的相同实例，去产生多视图一致的2D分割图。CascadePSP用于细化2D mask。\"}}]]]}],\"$undefined\",\"$undefined\",\"$undefined\"]]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-blue\",\"children\":[[\"$\",\"$L14\",null,{\"children\":\"更多笔记\"}],[\"$\",\"$L14\",null,{\"children\":\"包围体：AABB和OBB\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003eAABB\u003c/span\u003e：轴对齐包围盒\u003ccode class=\\\"x-inline-highlight\\\"\u003e(Axis-Aligned Bounding Box)\u003c/code\u003e\u003cbr/\u003e\u003cspan class=\\\"x-inline-strong\\\"\u003eOBB\u003c/span\u003e：有向包围盒\u003ccode class=\\\"x-inline-highlight\\\"\u003e(Oriented Bounding Box)\u003c/code\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"下图展示了更多种类的包围体：\"}}],[\"$\",\"$L16\",null,{\"src\":{\"default\":{\"src\":\"/_next/static/media/bounding_volumes.e98c20aa.png\",\"height\":323,\"width\":850,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAADBAMAAABc5lN7AAAAJ1BMVEX////+/v78/Pz19fX09PX09PTz8/Py8vLx8fHw8fHw8PDv7+/u7u532tZ0AAAAF0lEQVR42mNgqj4qwOA2fYU5Q3mpeTsAIZYEuS5vM7AAAAAASUVORK5CYII=\",\"blurWidth\":8,\"blurHeight\":3}},\"width\":\"100%\",\"invertInDarkTheme\":true}]]}],[\"$\",\"$L11\",null,{\"href\":\"https://openaccess.thecvf.com/content_CVPR_2020/papers/Cheng_CascadePSP_Toward_Class-Agnostic_and_Very_High-Resolution_Segmentation_via_Global_and_CVPR_2020_paper.pdf\",\"children\":\"【CascadePSP】CascadePSP: Toward Class-Agnostic and Very High-Resolution Segmentation via Global and Local Refinement (2020)\"}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-gray\",\"children\":[false,[[\"$\",\"$L12\",\"0\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的主题 / 文章要解决什么问题？\u003c/span\u003e\"}}],[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"提出一种不使用高分辨率训练数据，解决高分辨率分割问题的方法\u003cbr/\u003e右图是改进后的结果：\"}}],[\"$\",\"$L16\",null,{\"src\":{\"default\":{\"src\":\"/_next/static/media/cascadepsp.0d9b618b.jpg\",\"height\":542,\"width\":747,\"blurDataURL\":\"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAYACAMBIgACEQEDEQH/xAAoAAEBAAAAAAAAAAAAAAAAAAAABgEBAQAAAAAAAAAAAAAAAAAAAgP/2gAMAwEAAhADEAAAAKQRf//EABoQAQACAwEAAAAAAAAAAAAAAAEDBAACEkH/2gAIAQEAAT8A1s3C1KbWpeScA68XP//EABcRAAMBAAAAAAAAAAAAAAAAAAABAhH/2gAIAQIBAT8Amnh//8QAGBEAAgMAAAAAAAAAAAAAAAAAAQIAEnH/2gAIAQMBAT8AcC5xZ//Z\",\"blurWidth\":8,\"blurHeight\":6}},\"width\":\"400px\",\"invertInDarkTheme\":true}]]]}],\"$undefined\",\"$undefined\",\"$undefined\",\"$undefined\"]]}],[\"$\",\"$L11\",null,{\"href\":\"https://arxiv.org/pdf/2312.00860.pdf\",\"children\":\"【SAGA】Segment Any 3D Gaussians (2023)\"}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-gray\",\"children\":[false,[[\"$\",\"$L12\",\"0\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的主题 / 文章要解决什么问题？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"交互式3D分割\"}}]]}],\"$undefined\",\"$undefined\",\"$undefined\",\"$undefined\"]]}],[\"$\",\"$L10\",null,{\"children\":\"学习\"}],[\"$\",\"$L11\",null,{\"href\":\"https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf\",\"children\":\"【R-CNN】Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation (2014)\"}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-gray\",\"children\":[false,[[\"$\",\"$L12\",\"0\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的主题 / 文章要解决什么问题？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"提出\u003ccode class=\\\"x-inline-highlight\\\"\u003eRegions with CNN features, R-CNN\u003c/code\u003e提高目标检测性能\"}}]]}],[\"$\",\"$L12\",\"1\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的核心方法 / 具体是如何做的？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"区域提议\u003ccode class=\\\"x-inline-highlight\\\"\u003e(Region Proposals)\u003c/code\u003e：使用\u003ccode class=\\\"x-inline-highlight\\\"\u003eselective search\u003c/code\u003e生成候选框\"}}]]}],[\"$\",\"$L12\",\"2\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e做了什么实验，效果怎么样？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"在\u003ccode class=\\\"x-inline-highlight\\\"\u003ePASCAL VOC 2012\u003c/code\u003e取得\u003ccode class=\\\"x-inline-highlight\\\"\u003emAP 53.3%\u003c/code\u003e，在\u003ccode class=\\\"x-inline-highlight\\\"\u003eILSVRC 2013\u003c/code\u003e竞赛数据集取得\u003ccode class=\\\"x-inline-highlight\\\"\u003emAP 31.4%\u003c/code\u003e\"}}]]}],\"$undefined\",\"$undefined\"]]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-blue\",\"children\":[[\"$\",\"$L14\",null,{\"children\":\"更多笔记\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"转载自\u003ca href=\\\"https://zh-v2.d2l.ai/chapter_computer-vision/rcnn.html\\\" target=\\\"_blank\\\" rel=\\\"noreferrer\\\" class=\\\"x-inline-link\\\"\u003e动手学深度学习 - 区域卷积神经网络系列\u003c/a\u003e\"}}],[\"$\",\"$L14\",null,{\"children\":\"R-CNN\"}],[\"$\",\"$L15\",null,{\"children\":\"R-CNN使用启发式搜索算法`selective search`（之前人们通常也是这样做的）来选择锚框\"}],[\"$\",\"$L15\",null,{\"children\":\"用预训练的模型，对每一个锚框抽取特征\"}],[\"$\",\"$L15\",null,{\"children\":\"训练一个SVM来对类别分类\"}],[\"$\",\"$L15\",null,{\"children\":\"训练一个线性回归模型来预测边缘框\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"R-CNN的速度很慢，因为可能从一张图像中选出上千个提议区域，这需要上千次的卷积神经网络的前向传播来执行目标检测。这种庞大的计算量使得R-CNN在现实世界中难以被广泛应用。\"}}],[\"$\",\"$L16\",null,{\"src\":{\"default\":{\"src\":\"/_next/static/media/rcnn.09a38e8a.jpg\",\"height\":467,\"width\":1563,\"blurDataURL\":\"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAIACAMBIgACEQEDEQH/xAAoAAEBAAAAAAAAAAAAAAAAAAAABwEBAQAAAAAAAAAAAAAAAAAAAgP/2gAMAwEAAhADEAAAAKYKD//EABkQAQACAwAAAAAAAAAAAAAAAAIAAQMhI//aAAgBAQABPwBpHLpXXKf/xAAXEQADAQAAAAAAAAAAAAAAAAAAAkFx/9oACAECAQE/AFun/8QAFxEAAwEAAAAAAAAAAAAAAAAAAAFBcf/aAAgBAwEBPwBzD//Z\",\"blurWidth\":8,\"blurHeight\":2}},\"width\":\"600px\"}],[\"$\",\"$L14\",null,{\"children\":\"Fast R-CNN\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"R-CNN的主要性能瓶颈在于，对每个提议区域，卷积神经网络的前向传播是独立的，而没有共享计算。由于这些区域通常有重叠，独立的特征抽取会导致重复的计算。Fast R-CNN的主要改进之一，是仅在整张图象上执行卷积神经网络的前向传播，并且引入\u003cspan class=\\\"x-inline-strong\\\"\u003e兴趣区域池化\u003c/span\u003e\u003ccode class=\\\"x-inline-highlight\\\"\u003e(ROI Pooling)\u003c/code\u003e，将卷积神经网络的输出和提议区域作为输入，输出连结后的各个提议区域抽取的特征。\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"兴趣区域池化层可以给出固定大小的输出：把给定的锚框均匀分割成\u003cspan class=\\\"katex\\\"\u003e\u003cspan class=\\\"katex-html\\\" aria-hidden=\\\"true\\\"\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:0.6667em;vertical-align:-0.0833em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003en\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2222em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mbin\\\"\u003e×\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2222em;\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:0.4306em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003em\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e块，输出每块里的最大值，这样无论锚框多大，总是输出\u003cspan class=\\\"katex\\\"\u003e\u003cspan class=\\\"katex-html\\\" aria-hidden=\\\"true\\\"\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:0.4306em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003enm\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e个值。\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"Fast R-CNN先对图片用CNN抽取特征，然后将\u003ccode class=\\\"x-inline-highlight\\\"\u003eselective search\u003c/code\u003e给出的原图上的提议区域映射到CNN特征图上，再经过\u003ccode class=\\\"x-inline-highlight\\\"\u003eROI Pooling\u003c/code\u003e就可以得到维度对齐的特征。\"}}],[\"$\",\"$L16\",null,{\"src\":{\"default\":{\"src\":\"/_next/static/media/fastrcnn.46850bc6.jpg\",\"height\":838,\"width\":917,\"blurDataURL\":\"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAcACAMBIgACEQEDEQH/xAAnAAEBAAAAAAAAAAAAAAAAAAAABwEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEAMQAAAArAP/xAAbEAACAgMBAAAAAAAAAAAAAAABAgMEABEhcf/aAAgBAQABPwCEobVpUCgxtrzfeZ//xAAXEQEAAwAAAAAAAAAAAAAAAAABABEx/9oACAECAQE/ALXZ/8QAFhEBAQEAAAAAAAAAAAAAAAAAAQAR/9oACAEDAQE/AMC//9k=\",\"blurWidth\":8,\"blurHeight\":7}},\"width\":\"400px\"}],[\"$\",\"$L14\",null,{\"children\":\"Faster R-CNN\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"与Fast R-CNN相比，Faster R-CNN将生成提议区域的方法从\u003ccode class=\\\"x-inline-highlight\\\"\u003eselective search\u003c/code\u003e改为了\u003cspan class=\\\"x-inline-strong\\\"\u003e区域提议网络\u003c/span\u003e\u003ccode class=\\\"x-inline-highlight\\\"\u003e(Region Proposal Network, RPN)\u003c/code\u003e，模型的其余部分保持不变。区域提议网络作为Faster R-CNN模型的一部分，是和整个模型一起训练得到的。换句话说，Faster R-CNN的目标函数不仅包括目标检测中的类别和边界框预测，还包括区域提议网络中锚框的二元类别和边界框预测。作为端到端训练的结果，区域提议网络能够学习到如何生成高质量的提议区域，从而在减少了从数据中学习的提议区域的数量的情况下，仍保持目标检测的精度。\"}}],[\"$\",\"$L16\",null,{\"src\":{\"default\":{\"src\":\"/_next/static/media/fasterrcnn.1f4436bb.jpg\",\"height\":871,\"width\":1579,\"blurDataURL\":\"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAQACAMBIgACEQEDEQH/xAAnAAEBAAAAAAAAAAAAAAAAAAAABwEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEAMQAAAArgP/xAAaEAEAAwADAAAAAAAAAAAAAAABAhEhAAME/9oACAEBAAE/APPEnBbSu2eGGNc//8QAFBEBAAAAAAAAAAAAAAAAAAAAAP/aAAgBAgEBPwB//8QAFBEBAAAAAAAAAAAAAAAAAAAAAP/aAAgBAwEBPwB//9k=\",\"blurWidth\":8,\"blurHeight\":4}},\"width\":\"600px\"}]]}],[\"$\",\"$L11\",null,{\"href\":\"https://arxiv.org/pdf/1703.06870.pdf\",\"children\":\"【Mask R-CNN】Mask R-CNN (2017)\"}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-gray\",\"children\":[false,[[\"$\",\"$L12\",\"0\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的主题 / 文章要解决什么问题？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"目标检测\u0026实例分割\"}}]]}],[\"$\",\"$L12\",\"1\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的核心方法 / 具体是如何做的？\u003c/span\u003e\"}}],[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"基于Faster R-CNN，添加一个对每个ROI预测分割mask的分支。这个分支可以与分类和边界框预测分支并行。\"}}],[\"$\",\"$L16\",null,{\"src\":{\"default\":{\"src\":\"/_next/static/media/maskrcnn1.6cdb9105.jpg\",\"height\":342,\"width\":676,\"blurDataURL\":\"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAQACAMBIgACEQEDEQH/xAAoAAEBAAAAAAAAAAAAAAAAAAAABwEBAQAAAAAAAAAAAAAAAAAAAAL/2gAMAwEAAhADEAAAAKsIf//EABwQAAICAgMAAAAAAAAAAAAAAAEDERIAIQITMf/aAAgBAQABPwDgkUWvsZv022ZM5//EABcRAAMBAAAAAAAAAAAAAAAAAAABQXH/2gAIAQIBAT8Acw//xAAVEQEBAAAAAAAAAAAAAAAAAAAAQf/aAAgBAwEBPwCP/9k=\",\"blurWidth\":8,\"blurHeight\":4}},\"width\":\"600px\"}]]]}],[\"$\",\"$L12\",\"2\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e做了什么实验，效果怎么样？\u003c/span\u003e\"}}],[[\"$\",\"$L16\",null,{\"src\":{\"default\":{\"src\":\"/_next/static/media/maskrcnn2.fe26f764.jpg\",\"height\":184,\"width\":763,\"blurDataURL\":\"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAIACAMBIgACEQEDEQH/xAAnAAEBAAAAAAAAAAAAAAAAAAAABwEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEAMQAAAArQP/xAAVEAEBAAAAAAAAAAAAAAAAAAAAAv/aAAgBAQABPwCH/8QAFBEBAAAAAAAAAAAAAAAAAAAAAP/aAAgBAgEBPwB//8QAFBEBAAAAAAAAAAAAAAAAAAAAAP/aAAgBAwEBPwB//9k=\",\"blurWidth\":8,\"blurHeight\":2}},\"width\":\"600px\",\"invertInDarkTheme\":true}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"在COCO数据集上表现优异，超过了\u003ccode class=\\\"x-inline-highlight\\\"\u003e2015\u003c/code\u003e和\u003ccode class=\\\"x-inline-highlight\\\"\u003e2016\u003c/code\u003e年COCO分割任务的冠军。\"}}]]]}],[\"$\",\"$L12\",\"3\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e研究的创新点\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\" \u003ccode class=\\\"x-inline-highlight\\\"\u003eROI Pooling\u003c/code\u003e中发生了两次浮点数取整，第一次是将锚框均匀分割成\u003cspan class=\\\"katex\\\"\u003e\u003cspan class=\\\"katex-html\\\" aria-hidden=\\\"true\\\"\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:0.6667em;vertical-align:-0.0833em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003en\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2222em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mbin\\\"\u003e×\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2222em;\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:0.4306em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003em\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e块时，第二次是把提议区域映射回CNN特征图时。\u003cbr/\u003e 分割任务的精细程度更高，因此文章提出了\u003ccode class=\\\"x-inline-highlight\\\"\u003eROI Align\u003c/code\u003e，使用双线性插值来保留特征图上的空间信息。 \"}}]]}],\"$undefined\"]]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-blue\",\"children\":[[\"$\",\"$L14\",null,{\"children\":\"更多笔记\"}],[\"$\",\"$L14\",null,{\"children\":\"语义分割与实例分割\"}],[\"$\",\"$L16\",null,{\"src\":{\"default\":{\"src\":\"/_next/static/media/ss_and_is.f6df7378.jpg\",\"height\":370,\"width\":1243,\"blurDataURL\":\"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAIACAMBIgACEQEDEQH/xAAnAAEBAAAAAAAAAAAAAAAAAAAABgEBAAAAAAAAAAAAAAAAAAAAAf/aAAwDAQACEAMQAAAAqQv/xAAbEAABBAMAAAAAAAAAAAAAAAACAAEDIhEhov/aAAgBAQABPwCConjVpH7X/8QAFxEAAwEAAAAAAAAAAAAAAAAAAAECMv/aAAgBAgEBPwCco//EABgRAAIDAAAAAAAAAAAAAAAAAAABAjJx/9oACAEDAQE/AJ2en//Z\",\"blurWidth\":8,\"blurHeight\":2}},\"width\":\"600px\"}],[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e语义分割\u003c/span\u003e\u003ccode class=\\\"x-inline-highlight\\\"\u003e(semantic segmentation)\u003c/code\u003e：为每一个像素分配一个类别，但不区分同一类别之间的对象。\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e实例分割\u003c/span\u003e\u003ccode class=\\\"x-inline-highlight\\\"\u003e(instance segmentation)\u003c/code\u003e：会区分属于同一类别的不同实例。\"}}]]}],[\"$\",\"$L11\",null,{\"href\":\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf\",\"children\":\"【FCN】Fully Convolutional Networks for Semantic Segmentation (2015)\"}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-gray\",\"children\":[false,[[\"$\",\"$L12\",\"0\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的主题 / 文章要解决什么问题？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"使用全卷积网络进行语义分割\"}}]]}],[\"$\",\"$L12\",\"1\",{\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的核心方法 / 具体是如何做的？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\" 是将现有的分类网络（AlexNet、VGG Net、GoogLeNet）改造为全卷积网络，以便在语义分割任务上进行端到端（输入图像，输出分割掩码）的训练。\u003cbr/\u003e 改造的方式是将最后的全连接层替换成卷积层。 \"}}]]}],\"$undefined\",\"$undefined\",\"$undefined\"]]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-blue\",\"children\":[[\"$\",\"$L14\",null,{\"children\":\"更多笔记\"}],[\"$\",\"$L14\",null,{\"children\":\"转置卷积\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"卷积通常不会增大输入的高宽，而是保持不变或降低。由于语义分割任务需要像素级别的输出，转置卷积被用来增大输入的高宽。\"}}],[\"$\",\"$L16\",null,{\"src\":{\"default\":{\"src\":\"/_next/static/media/transpose_convolution.fb99cc55.jpg\",\"height\":393,\"width\":890,\"blurDataURL\":\"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAQACAMBIgACEQEDEQH/xAAnAAEBAAAAAAAAAAAAAAAAAAAABwEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEAMQAAAArQP/xAAbEAABBAMAAAAAAAAAAAAAAAACAAEDESEiUf/aAAgBAQABPwBpCwV7dX//xAAUEQEAAAAAAAAAAAAAAAAAAAAA/9oACAECAQE/AH//xAAUEQEAAAAAAAAAAAAAAAAAAAAA/9oACAEDAQE/AH//2Q==\",\"blurWidth\":8,\"blurHeight\":4}},\"width\":\"600px\",\"invertInDarkTheme\":true}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"图片转载自\u003ca href=\\\"https://zh-v2.d2l.ai/chapter_computer-vision/transposed-conv.html\\\" target=\\\"_blank\\\" rel=\\\"noreferrer\\\" class=\\\"x-inline-link\\\"\u003e动手学深度学习 - 转置卷积\u003c/a\u003e\"}}],[\"$\",\"$L14\",null,{\"children\":\"FCN中的转置卷积\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"例如对于ImageNet的图片输入，大小通常为\u003ccode class=\\\"x-inline-highlight\\\"\u003e224\u003c/code\u003e\u0026#42;\u003ccode class=\\\"x-inline-highlight\\\"\u003e224\u003c/code\u003e\u0026#42;\u003ccode class=\\\"x-inline-highlight\\\"\u003e3\u003c/code\u003e（RGB通道）；经过卷积后缩小宽高缩小\u003ccode class=\\\"x-inline-highlight\\\"\u003e32\u003c/code\u003e倍，通道增加到\u003ccode class=\\\"x-inline-highlight\\\"\u003e512\u003c/code\u003e，变成\u003ccode class=\\\"x-inline-highlight\\\"\u003e7\u003c/code\u003e\u0026#42;\u003ccode class=\\\"x-inline-highlight\\\"\u003e7\u003c/code\u003e\u0026#42;\u003ccode class=\\\"x-inline-highlight\\\"\u003e512\u003c/code\u003e的特征图。此时FCN会先通过一个\u003ccode class=\\\"x-inline-highlight\\\"\u003e1x1conv\u003c/code\u003e进行通道降维，然后通过转置卷积将特征图的高度和宽度增加\u003ccode class=\\\"x-inline-highlight\\\"\u003e32\u003c/code\u003e倍，\u003cspan class=\\\"x-inline-strong\\\"\u003e输出通道数等于类别数\u003c/span\u003e，相当于储存了对每一类的预测结果。\"}}]]}]],null],\"segment\":\"__PAGE__\"},\"styles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/f70c26425a044d6b.css\",\"precedence\":\"next\",\"crossOrigin\":\"\"}]]}],\"segment\":\"papers\"},\"styles\":[]}],\"segment\":\"longtime\"},\"styles\":[]}],\"params\":{}}],null],\"segment\":\"(blogs)\"},\"styles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/d1ba65e78d6f432c.css\",\"precedence\":\"next\",\"crossOrigin\":\"\"}]]}]]}]}]}],null]}]]\n"])</script><script>self.__next_f.push([1,"6:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"1\",{\"children\":\"Create Next App\"}],[\"$\",\"meta\",\"2\",{\"name\":\"description\",\"content\":\"Generated by create next app\"}],[\"$\",\"meta\",\"3\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\ne:null\n"])</script></body></html>