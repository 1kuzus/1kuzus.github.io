<!DOCTYPE html><html lang="zh-CN"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/css/e43a733539111c31.css" crossorigin="" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/803fe687e7e31b6c.css" crossorigin="" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/70373e6f6e0beca4.css" crossorigin="" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-d56e577e6f69f464.js" crossorigin=""/><script src="/_next/static/chunks/fd9d1056-ef431402238f9d90.js" async="" crossorigin=""></script><script src="/_next/static/chunks/69-ae9cebba1b3d8b52.js" async="" crossorigin=""></script><script src="/_next/static/chunks/main-app-00b9bbd3f4a844ac.js" async="" crossorigin=""></script><script src="/_next/static/chunks/202-fbcb7cc54bf4f9f2.js" async=""></script><script src="/_next/static/chunks/452-25327cca6690da96.js" async=""></script><script src="/_next/static/chunks/app/(blogs)/longtime/papers/page-327424e0e36ec255.js" async=""></script><script src="/_next/static/chunks/792-a494ddef3d40c017.js" async=""></script><script src="/_next/static/chunks/app/(blogs)/layout-565e6400d12a9f60.js" async=""></script><script src="/_next/static/chunks/app/layout-fee515b4ccc2d098.js" async=""></script><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-45BYSZ6WPY"></script><script async="" src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><title>论文速记 - 铃木的网络日记</title><meta name="description" content="..."/><link rel="canonical" href="https://1kuzus.github.io/longtime/papers/"/><script>
window.dataLayer = window.dataLayer || [];
function gtag() {
    dataLayer.push(arguments);
}
gtag('js', new Date());
gtag('config', 'G-45BYSZ6WPY');
if (!localStorage.getItem('theme')) localStorage.setItem('theme', 'light');
document.documentElement.setAttribute('class', localStorage.getItem('theme'));
</script><script>const a=z=>h.getItem(z),b=(y,z)=>h.setItem(y,z),c=(y,z)=>document.documentElement.setAttribute(y,z),d='theme',e='dark',f='light',g='class',h=localStorage;a(d)!==e&&a(d)!==f&&b(d,f);a(d)===e?c(g,e):c(g,f);</script><script src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js" crossorigin="" noModule=""></script></head><body><div id="header" class=""><div id="header-left-wrapper"><a href="/"><div id="header-logo-bg"><svg viewBox="0 0 1560 1560" width="36px" height="36px" xmlns="http://www.w3.org/2000/svg"><g fill-rule="evenodd"><path d="M644 97h272.529L1189 780H916.471z" fill="#00A8C4"></path><path d="M98 97h272.84L780 1120.73 1189.162 97H1462L916.438 1462H643.562z" fill="#30303C"></path><path d="M98 1462L643.3 97H916L370.7 1462z" fill="#00F8FF"></path></g></svg></div></a><div id="header-archive"><h3 id="header-archive-text">归档</h3><div class="header-archive-rightarrow"><svg viewBox="0 0 1024 1024" width="16px" height="16px" xmlns="http://www.w3.org/2000/svg"><path d="M761.056 532.128c.512-.992 1.344-1.824 1.792-2.848 8.8-18.304 5.92-40.704-9.664-55.424L399.936 139.744c-19.264-18.208-49.632-17.344-67.872 1.888-18.208 19.264-17.376 49.632 1.888 67.872l316.96 299.84L335.2 813.632c-19.072 18.4-19.648 48.768-1.248 67.872 9.408 9.792 21.984 14.688 34.56 14.688 12 0 24-4.48 33.312-13.44l350.048-337.376c.672-.672.928-1.6 1.6-2.304.512-.48 1.056-.832 1.568-1.344 2.72-2.848 4.16-6.336 6.016-9.6z" fill="#50505c"></path></svg></div></div></div><div id="header-right-wrapper"><div id="header-theme-bg"><div id="header-dark-theme-icon"><svg viewBox="0 0 1024 1024" width="20px" height="20px" xmlns="http://www.w3.org/2000/svg"><path fill="#FCFCFC" d="M524.8 938.667h-4.267a439.893 439.893 0 01-313.173-134.4 446.293 446.293 0 01-11.093-597.334A432.213 432.213 0 01366.933 90.027a42.667 42.667 0 0145.227 9.386 42.667 42.667 0 0110.24 42.667 358.4 358.4 0 0082.773 375.893 361.387 361.387 0 00376.747 82.774 42.667 42.667 0 0154.187 55.04 433.493 433.493 0 01-99.84 154.88 438.613 438.613 0 01-311.467 128z"></path></svg></div><div id="header-light-theme-icon"><svg viewBox="0 0 1024 1024" width="20px" height="20px" xmlns="http://www.w3.org/2000/svg"><path fill="#FCFCFC" d="M512 61.44c13.312 0 25.252 7.639 29.942 19.17l27.566 67.972c11.92 29.348-4.178 62.075-35.943 73.094A65.946 65.946 0 01512 225.28c-33.935 0-61.44-25.416-61.44-56.77 0-6.8 1.331-13.558 3.912-19.928l27.586-67.973C486.728 69.08 498.647 61.44 512 61.44zm0 901.12c-13.332 0-25.272-7.639-29.942-19.17l-27.586-67.972a52.961 52.961 0 01-3.912-19.927c0-31.335 27.505-56.771 61.44-56.771a66.28 66.28 0 0121.565 3.604c31.765 11.019 47.862 43.746 35.943 73.114l-27.566 67.953c-4.69 11.53-16.63 19.169-29.942 19.169zM962.56 512c0 13.312-7.639 25.252-19.17 29.942l-67.972 27.566c-29.348 11.92-62.075-4.178-73.094-35.943A65.946 65.946 0 01798.72 512c0-33.935 25.416-61.44 56.77-61.44 6.8 0 13.558 1.331 19.928 3.912l67.973 27.586c11.53 4.67 19.169 16.589 19.169 29.942zm-901.12 0c0-13.332 7.639-25.272 19.17-29.942l67.972-27.586a52.9 52.9 0 0119.927-3.912c31.335 0 56.771 27.505 56.771 61.44a66.28 66.28 0 01-3.604 21.565c-11.019 31.765-43.746 47.862-73.114 35.943l-67.953-27.566C69.08 537.252 61.44 525.312 61.44 512zm131.953-318.587c9.42-9.42 23.265-12.452 34.734-7.618l67.563 28.549c29.184 12.35 40.94 46.858 26.256 77.107a65.946 65.946 0 01-12.698 17.817c-23.982 23.983-61.399 25.457-83.558 3.277a52.961 52.961 0 01-11.346-16.855l-28.55-67.543c-4.853-11.469-1.822-25.313 7.599-34.734zm637.194 637.194c-9.42 9.421-23.265 12.452-34.734 7.598l-67.543-28.549a52.961 52.961 0 01-16.876-11.325c-22.16-22.18-20.685-59.597 3.298-83.579a65.946 65.946 0 0117.817-12.698c30.25-14.684 64.758-2.928 77.107 26.256l28.55 67.563c4.833 11.47 1.802 25.293-7.62 34.734zm0-637.214c9.42 9.42 12.452 23.265 7.618 34.734l-28.549 67.563c-12.35 29.184-46.858 40.94-77.107 26.256a65.946 65.946 0 01-17.817-12.698c-23.983-23.982-25.457-61.399-3.277-83.558 4.792-4.834 10.506-8.663 16.855-11.346l67.543-28.55c11.469-4.853 25.313-1.822 34.734 7.599zM193.393 830.587c-9.421-9.42-12.452-23.265-7.598-34.734l28.549-67.543a52.618 52.618 0 0111.325-16.876c22.18-22.16 59.597-20.685 83.579 3.298a65.842 65.842 0 0112.698 17.817c14.684 30.25 2.928 64.758-26.256 77.107l-67.563 28.55c-11.47 4.833-25.293 1.802-34.734-7.62zM512 737.28c-124.416 0-225.28-100.864-225.28-225.28S387.584 286.72 512 286.72 737.28 387.584 737.28 512 636.416 737.28 512 737.28z"></path></svg></div></div><a href="https://github.com/1kuzus/1kuzus.github.io" target="_blank" rel="noreferrer"><div id="header-github-bg"><svg viewBox="0 0 1024 1024" width="36px" height="36px" xmlns="http://www.w3.org/2000/svg"><path d="M411.306667 831.146667c3.413333-5.12 6.826667-10.24 6.826666-11.946667v-69.973333c-105.813333 22.186667-128-44.373333-128-44.373334-17.066667-44.373333-42.666667-56.32-42.666666-56.32-34.133333-23.893333 3.413333-23.893333 3.413333-23.893333 37.546667 3.413333 58.026667 39.253333 58.026667 39.253333 34.133333 58.026667 88.746667 40.96 110.933333 32.426667 3.413333-23.893333 13.653333-40.96 23.893333-51.2-85.333333-10.24-174.08-42.666667-174.08-187.733333 0-40.96 15.36-75.093333 39.253334-102.4-3.413333-10.24-17.066667-47.786667 3.413333-100.693334 0 0 32.426667-10.24 104.106667 39.253334 30.72-8.533333 63.146667-11.946667 95.573333-11.946667 32.426667 0 64.853333 5.12 95.573333 11.946667 73.386667-49.493333 104.106667-39.253333 104.106667-39.253334 20.48 52.906667 8.533333 90.453333 3.413333 100.693334 23.893333 27.306667 39.253333 59.733333 39.253334 102.4 0 145.066667-88.746667 177.493333-174.08 187.733333 13.653333 11.946667 25.6 34.133333 25.6 69.973333v104.106667c0 3.413333 1.706667 6.826667 6.826666 11.946667 5.12 6.826667 3.413333 18.773333-3.413333 23.893333-3.413333 1.706667-6.826667 3.413333-10.24 3.413333h-174.08c-10.24 0-17.066667-6.826667-17.066667-17.066666 0-5.12 1.706667-8.533333 3.413334-10.24z" fill="#FCFCFC"></path></svg></div></a></div></div><div id="blog-layout"><div id="main"><h1 class="x-title">论文速记</h1><h2 class="x-h1"><span>研究</span></h2><h3 class="x-h2"><span><a href="https://arxiv.org/pdf/2003.08934.pdf" target="_blank" rel="noreferrer">【NeRF】NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis (2020)</a></span></h3><div class="x-highlightblock highlight-background-gray"><div class="x-oli"><div class="x-oli-number">1.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">使用稀疏输入2D图片集实现场景的3D视图合成</p></div></div><div class="x-oli"><div class="x-oli-number">2.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的核心方法 / 具体是如何做的？</span></p><p class="x-p">从<code class="x-inline-highlight">(<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">ϕ</span></span></span></span>)</code>到<code class="x-inline-highlight">(<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">G</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span></span>)</code></p></div></div><div class="x-oli"><div class="x-oli-number">3.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">做了什么实验，效果怎么样？</span></p><p class="x-p">测试数据集是<code class="x-inline-highlight">Diffuse Synthetic 360</code>、<code class="x-inline-highlight">Realistic Synthetic 360</code>和<code class="x-inline-highlight">Real Forward-Facing</code></p></div></div><div class="x-oli"><div class="x-oli-number">4.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">研究的创新点</span></p><p class="x-p">将输入坐标位置编码，帮助MLP表示高频函数<br/>分层采样</p></div></div><div class="x-oli"><div class="x-oli-number">5.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">有什么限制或可以改进的地方？</span></p><p class="x-p">有效地优化和渲染神经辐射场<br/>可解释性</p></div></div></div><div class="x-highlightblock highlight-background-blue"><h4 class="x-h3">更多笔记</h4><p class="x-p">神经辐射场用于从2D的图片重建3D的场景。</p><p class="x-p no-margin-bottom">文中出现的三个指标：PSNR、SSIM、LPIPS</p><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><span class="x-inline-strong">峰值信噪比</span><code class="x-inline-highlight">(Peak Signal to Noise Ratio, PSNR)</code>：用于衡量图像恢复的质量，数值越高表示图像质量越好。接近<code class="x-inline-highlight">50 dB</code>代表误差非常小，大于<code class="x-inline-highlight">30 dB</code>人眼难察觉差异。</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><span class="x-inline-strong">结构相似性</span><code class="x-inline-highlight">(Structural Similarity Index Measure, SSIM)</code>：用于衡量图像的结构相似性，得分通常在<code class="x-inline-highlight">0</code>~<code class="x-inline-highlight">1</code>之间，数值越高表示图像结构越相似。相较于PSNR在图像质量的衡量上更能符合人眼对图像质量的判断。</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><span class="x-inline-strong">基于学习的感知图像质量评价</span><code class="x-inline-highlight">(Learned Perceptual Image Patch Similarity, LPIPS)</code>：测量从预训练网络中提取的两个图像的特征之间的相似性，得分通常在<code class="x-inline-highlight">0</code>~<code class="x-inline-highlight">1</code>之间，数值越低表示感知质量越高。</p></div></div></div><h3 class="x-h2"><span><a href="https://arxiv.org/pdf/2308.04079.pdf" target="_blank" rel="noreferrer">【3DGS】3D Gaussian Splatting for Real-Time Radiance Field Rendering (2023)</a></span></h3><div class="x-highlightblock highlight-background-gray"><div class="x-oli"><div class="x-oli-number">1.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">实现实时辐射场渲染，同时保持高质量的视觉效果，并且保持较短的训练时间</p></div></div></div><div class="x-highlightblock highlight-background-blue"><h4 class="x-h3">更多笔记</h4><h4 class="x-h3">文章的相关工作部分</h4><p class="x-p">传统的场景重建与渲染：基于光场的，密集采样、非结构化捕获；<span class="x-inline-strong">运动恢复结构</span><code class="x-inline-highlight">(Structure from Motion, SFM)</code>用一组照片估计稀疏点云合成新视图；<span class="x-inline-strong">多视点立体视觉</span><code class="x-inline-highlight">(Multi-View Stereo, MVS)</code>；<br/> 神经渲染和辐射场：用CNN估计混合权重，用于纹理空间；Soft3D提出<code class="x-inline-highlight">Volumetric representations</code>；NeRF提出重要性采样和位置编码来提高质量，但使用了大型多层感知器，对速度有负面影响；</p><h4 class="x-h3">稀疏重建和稠密重建</h4><p class="x-p">稀疏重建主要用于定位，得到每张图片的相机参数，提取特征点，例如SFM；稠密重建是假设相机参数已知的情况下，从不同视角的图像中找到匹配的对应点，对整个图像或图像中绝大部分像素进行重建。</p></div><h3 class="x-h2"><span><a href="https://arxiv.org/pdf/2201.05989.pdf" target="_blank" rel="noreferrer">【Instant NGP】Instant Neural Graphics Primitives with a Multiresolution Hash Encoding (2022)</a></span></h3><div class="x-highlightblock highlight-background-gray"><div class="x-oli"><div class="x-oli-number">1.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">实现实时辐射场渲染，同时保持高质量的视觉效果，并且保持较短的训练时间</p></div></div></div><div class="x-highlightblock highlight-background-blue"><h4 class="x-h3">更多笔记</h4><h4 class="x-h3">Instant NGP与NeRF的异同</h4><p class="x-p no-margin-bottom">转载自<a href="https://zhuanlan.zhihu.com/p/631284285" target="_blank" rel="noreferrer" class="x-inline-link">知乎：从NeRF到Instant-NGP</a></p><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p">同样基于体渲染</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p">不同于NeRF的MLP，Instant NGP使用稀疏的参数化的<code class="x-inline-highlight">voxel grid</code>作为场景表达</p></div></div></div><h3 class="x-h2"><span><a href="https://arxiv.org/pdf/2211.11646.pdf" target="_blank" rel="noreferrer">【NeRF RPN】NeRF-RPN: A general framework for object detection in NeRFs (2022)</a></span></h3><div class="x-highlightblock highlight-background-gray"><div class="x-oli"><div class="x-oli-number">1.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">在NeRF中直接进行3D物体检测</p></div></div><div class="x-oli"><div class="x-oli-number">2.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的核心方法 / 具体是如何做的？</span></p><p class="x-p">第一部分：特征提取器<br/>从NeRF采样的辐射度和密度网格作为输入，生成特征金字塔作为输出。<br/> 第二部分：RPN头<br/>对特征金字塔进行操作并生成对象建议。</p></div></div><div class="x-oli"><div class="x-oli-number">3.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">研究的创新点</span></p><p class="x-p">第一次将RPN引入NeRF以进行3D物体检测和相关任务<br/>利用Hypersim和3D-FRONT数据集构建了第一个用于3D目标检测的NeRF数据集</p></div></div></div><h3 class="x-h2"><span><a href="https://arxiv.org/pdf/2304.04395v3.pdf" target="_blank" rel="noreferrer">【Instance NeRF】Instance Neural Radiance Field (2023)</a></span></h3><div class="x-highlightblock highlight-background-gray"><div class="x-oli"><div class="x-oli-number">1.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">输入一个以多视图RGB图像预训练的NeRF，学习给定场景的3D实例分割</p><p class="x-p no-margin-bottom">文章的主要贡献：</p><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p">第一个在NeRF中进行3D实例分割的尝试之一，而没有使用真实分割标签作为输入</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p">提出<code class="x-inline-highlight">Neural Instance Field</code>的结构和训练方法，可以产生<span class="x-inline-strong">多视图一致</span>的2D分割和连续的3D分割</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p">对合成室内NeRF数据集进行实验和消融研究</p></div></div></div></div><div class="x-oli"><div class="x-oli-number">2.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的核心方法 / 具体是如何做的？</span></p><p class="x-p">Instance NeRF有两个组件：预训练的NeRF模型、和文中提出的<code class="x-inline-highlight">Instance Field</code>。</p><p class="x-p"><code class="x-inline-highlight">Instance Field</code>的训练过程如下：</p><div class="x-image-wrapper x-image-invert"><img alt="img" loading="lazy" width="1458" height="500" decoding="async" data-nimg="1" style="color:transparent;width:96%;height:auto" src="/_next/static/media/instance_field.500af71e.jpg"/></div><p class="x-p">NeRF-RCNN用预训练的NeRF提取的辐射场和密度场，为每个检测到的对象输出3D掩码；Mask2Former生成从NeRF渲染的图像的二维全景分割图（跨视图的实例标签并不一定一致）。然后按照相机位置，投影3D掩码去匹配不同视图中的相同实例，去产生多视图一致的2D分割图。CascadePSP用于细化2D mask。</p></div></div></div><div class="x-highlightblock highlight-background-blue"><h4 class="x-h3">更多笔记</h4><h4 class="x-h3">包围体：AABB和OBB</h4><p class="x-p"><span class="x-inline-strong">AABB</span>：轴对齐包围盒<code class="x-inline-highlight">(Axis-Aligned Bounding Box)</code><br/><span class="x-inline-strong">OBB</span>：有向包围盒<code class="x-inline-highlight">(Oriented Bounding Box)</code></p><p class="x-p">下图展示了更多种类的包围体：</p><div class="x-image-wrapper x-image-invert"><img alt="img" loading="lazy" width="850" height="323" decoding="async" data-nimg="1" style="color:transparent;width:100%;height:auto" src="/_next/static/media/bounding_volumes.e98c20aa.png"/></div></div><h3 class="x-h2"><span><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Cheng_CascadePSP_Toward_Class-Agnostic_and_Very_High-Resolution_Segmentation_via_Global_and_CVPR_2020_paper.pdf" target="_blank" rel="noreferrer">【CascadePSP】CascadePSP: Toward Class-Agnostic and Very High-Resolution Segmentation via Global and Local Refinement (2020)</a></span></h3><div class="x-highlightblock highlight-background-gray"><div class="x-oli"><div class="x-oli-number">1.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">提出一种不使用高分辨率训练数据，解决高分辨率分割问题的方法<br/>右图是改进后的结果：</p><div class="x-image-wrapper x-image-invert"><img alt="img" loading="lazy" width="747" height="542" decoding="async" data-nimg="1" style="color:transparent;width:400px;height:auto" src="/_next/static/media/cascadepsp.0d9b618b.jpg"/></div></div></div></div><h3 class="x-h2"><span><a href="https://arxiv.org/pdf/2312.00860.pdf" target="_blank" rel="noreferrer">【SAGA】Segment Any 3D Gaussians (2023)</a></span></h3><div class="x-highlightblock highlight-background-gray"><div class="x-oli"><div class="x-oli-number">1.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">交互式3D分割</p></div></div></div><h2 class="x-h1"><span>学习</span></h2><h3 class="x-h2"><span><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf" target="_blank" rel="noreferrer">【R-CNN】Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation (2014)</a></span></h3><div class="x-highlightblock highlight-background-gray"><div class="x-oli"><div class="x-oli-number">1.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">提出<code class="x-inline-highlight">Regions with CNN features, R-CNN</code>提高目标检测性能</p></div></div><div class="x-oli"><div class="x-oli-number">2.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的核心方法 / 具体是如何做的？</span></p><p class="x-p">区域提议<code class="x-inline-highlight">(Region Proposals)</code>：使用<code class="x-inline-highlight">selective search</code>生成候选框</p></div></div><div class="x-oli"><div class="x-oli-number">3.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">做了什么实验，效果怎么样？</span></p><p class="x-p">在<code class="x-inline-highlight">PASCAL VOC 2012</code>取得<code class="x-inline-highlight">mAP 53.3%</code>，在<code class="x-inline-highlight">ILSVRC 2013</code>竞赛数据集取得<code class="x-inline-highlight">mAP 31.4%</code></p></div></div></div><div class="x-highlightblock highlight-background-blue"><h4 class="x-h3">更多笔记</h4><p class="x-p">转载自<a href="https://zh-v2.d2l.ai/chapter_computer-vision/rcnn.html" target="_blank" rel="noreferrer" class="x-inline-link">动手学深度学习 - 区域卷积神经网络系列</a></p><h4 class="x-h3">R-CNN</h4><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p">R-CNN使用启发式搜索算法<code class="x-inline-highlight">selective search</code>（之前人们通常也是这样做的）来选择锚框</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p">用预训练的模型，对每一个锚框抽取特征</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p">训练一个SVM来对类别分类</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p">训练一个线性回归模型来预测边缘框</p></div></div><p class="x-p">R-CNN的速度很慢，因为可能从一张图像中选出上千个提议区域，这需要上千次的卷积神经网络的前向传播来执行目标检测。这种庞大的计算量使得R-CNN在现实世界中难以被广泛应用。</p><div class="x-image-wrapper"><img alt="img" loading="lazy" width="1563" height="467" decoding="async" data-nimg="1" style="color:transparent;width:600px;height:auto" src="/_next/static/media/rcnn.09a38e8a.jpg"/></div><h4 class="x-h3">Fast R-CNN</h4><p class="x-p">R-CNN的主要性能瓶颈在于，对每个提议区域，卷积神经网络的前向传播是独立的，而没有共享计算。由于这些区域通常有重叠，独立的特征抽取会导致重复的计算。Fast R-CNN的主要改进之一，是仅在整张图象上执行卷积神经网络的前向传播，并且引入<span class="x-inline-strong">兴趣区域池化</span><code class="x-inline-highlight">(ROI Pooling)</code>，将卷积神经网络的输出和提议区域作为输入，输出连结后的各个提议区域抽取的特征。</p><p class="x-p">兴趣区域池化层可以给出固定大小的输出：把给定的锚框均匀分割成<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">m</span></span></span></span>块，输出每块里的最大值，这样无论锚框多大，总是输出<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">nm</span></span></span></span>个值。</p><p class="x-p">Fast R-CNN先对图片用CNN抽取特征，然后将<code class="x-inline-highlight">selective search</code>给出的原图上的提议区域映射到CNN特征图上，再经过<code class="x-inline-highlight">ROI Pooling</code>就可以得到维度对齐的特征。</p><div class="x-image-wrapper"><img alt="img" loading="lazy" width="917" height="838" decoding="async" data-nimg="1" style="color:transparent;width:400px;height:auto" src="/_next/static/media/fastrcnn.46850bc6.jpg"/></div><h4 class="x-h3">Faster R-CNN</h4><p class="x-p">与Fast R-CNN相比，Faster R-CNN将生成提议区域的方法从<code class="x-inline-highlight">selective search</code>改为了<span class="x-inline-strong">区域提议网络</span><code class="x-inline-highlight">(Region Proposal Network, RPN)</code>，模型的其余部分保持不变。区域提议网络作为Faster R-CNN模型的一部分，是和整个模型一起训练得到的。换句话说，Faster R-CNN的目标函数不仅包括目标检测中的类别和边界框预测，还包括区域提议网络中锚框的二元类别和边界框预测。作为端到端训练的结果，区域提议网络能够学习到如何生成高质量的提议区域，从而在减少了从数据中学习的提议区域的数量的情况下，仍保持目标检测的精度。</p><div class="x-image-wrapper"><img alt="img" loading="lazy" width="1579" height="871" decoding="async" data-nimg="1" style="color:transparent;width:600px;height:auto" src="/_next/static/media/fasterrcnn.1f4436bb.jpg"/></div></div><h3 class="x-h2"><span><a href="https://arxiv.org/pdf/1703.06870.pdf" target="_blank" rel="noreferrer">【Mask R-CNN】Mask R-CNN (2017)</a></span></h3><div class="x-highlightblock highlight-background-gray"><div class="x-oli"><div class="x-oli-number">1.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">目标检测&实例分割</p></div></div><div class="x-oli"><div class="x-oli-number">2.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的核心方法 / 具体是如何做的？</span></p><p class="x-p">基于Faster R-CNN，添加一个对每个ROI预测分割mask的分支。这个分支可以与分类和边界框预测分支并行。</p><div class="x-image-wrapper"><img alt="img" loading="lazy" width="676" height="342" decoding="async" data-nimg="1" style="color:transparent;width:600px;height:auto" src="/_next/static/media/maskrcnn1.6cdb9105.jpg"/></div></div></div><div class="x-oli"><div class="x-oli-number">3.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">做了什么实验，效果怎么样？</span></p><div class="x-image-wrapper x-image-invert"><img alt="img" loading="lazy" width="763" height="184" decoding="async" data-nimg="1" style="color:transparent;width:600px;height:auto" src="/_next/static/media/maskrcnn2.fe26f764.jpg"/></div><p class="x-p">在COCO数据集上表现优异，超过了<code class="x-inline-highlight">2015</code>和<code class="x-inline-highlight">2016</code>年COCO分割任务的冠军。</p></div></div><div class="x-oli"><div class="x-oli-number">4.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">研究的创新点</span></p><p class="x-p"> <code class="x-inline-highlight">ROI Pooling</code>中发生了两次浮点数取整，第一次是将锚框均匀分割成<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">m</span></span></span></span>块时，第二次是把提议区域映射回CNN特征图时。<br/> 分割任务的精细程度更高，因此文章提出了<code class="x-inline-highlight">ROI Align</code>，使用双线性插值来保留特征图上的空间信息。 </p></div></div></div><div class="x-highlightblock highlight-background-blue"><h4 class="x-h3">更多笔记</h4><h4 class="x-h3">语义分割与实例分割</h4><div class="x-image-wrapper"><img alt="img" loading="lazy" width="1243" height="370" decoding="async" data-nimg="1" style="color:transparent;width:600px;height:auto" src="/_next/static/media/ss_and_is.f6df7378.jpg"/></div><p class="x-p no-margin-bottom"><span class="x-inline-strong">语义分割</span><code class="x-inline-highlight">(semantic segmentation)</code>：为每一个像素分配一个类别，但不区分同一类别之间的对象。</p><p class="x-p"><span class="x-inline-strong">实例分割</span><code class="x-inline-highlight">(instance segmentation)</code>：会区分属于同一类别的不同实例。</p></div><h3 class="x-h2"><span><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf" target="_blank" rel="noreferrer">【FCN】Fully Convolutional Networks for Semantic Segmentation (2015)</a></span></h3><div class="x-highlightblock highlight-background-gray"><div class="x-oli"><div class="x-oli-number">1.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">使用全卷积网络进行语义分割</p></div></div><div class="x-oli"><div class="x-oli-number">2.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的核心方法 / 具体是如何做的？</span></p><p class="x-p"> 是将现有的分类网络（AlexNet、VGG Net、GoogLeNet）改造为全卷积网络，以便在语义分割任务上进行端到端（输入图像，输出分割掩码）的训练。<br/> 改造的方式是将最后的全连接层替换成卷积层。 </p></div></div></div><div class="x-highlightblock highlight-background-blue"><h4 class="x-h3">更多笔记</h4><h4 class="x-h3">转置卷积</h4><p class="x-p">卷积通常不会增大输入的高宽，而是保持不变或降低。由于语义分割任务需要像素级别的输出，转置卷积被用来增大输入的高宽。</p><div class="x-image-wrapper x-image-invert"><img alt="img" loading="lazy" width="890" height="393" decoding="async" data-nimg="1" style="color:transparent;width:600px;height:auto" src="/_next/static/media/transpose_convolution.fb99cc55.jpg"/></div><p class="x-p">图片转载自<a href="https://zh-v2.d2l.ai/chapter_computer-vision/transposed-conv.html" target="_blank" rel="noreferrer" class="x-inline-link">动手学深度学习 - 转置卷积</a></p><h4 class="x-h3">FCN中的转置卷积</h4><p class="x-p">例如对于ImageNet的图片输入，大小通常为<code class="x-inline-highlight">224</code>&#42;<code class="x-inline-highlight">224</code>&#42;<code class="x-inline-highlight">3</code>（RGB通道）；经过卷积后缩小宽高缩小<code class="x-inline-highlight">32</code>倍，通道增加到<code class="x-inline-highlight">512</code>，变成<code class="x-inline-highlight">7</code>&#42;<code class="x-inline-highlight">7</code>&#42;<code class="x-inline-highlight">512</code>的特征图。此时FCN会先通过一个<code class="x-inline-highlight">1x1conv</code>进行通道降维，然后通过转置卷积将特征图的高度和宽度增加<code class="x-inline-highlight">32</code>倍，<span class="x-inline-strong">输出通道数等于类别数</span>，相当于储存了对每一类的预测结果。</p></div></div><div id="sidebar"><div class="sidebar-list show-list"><div class="sidebar-list-head"><h3 class="sidebar-list-category">网络杂识 (4)</h3><div class="sidebar-list-category-rightarrow"><svg viewBox="0 0 1024 1024" width="16px" height="16px" xmlns="http://www.w3.org/2000/svg"><path d="M761.056 532.128c.512-.992 1.344-1.824 1.792-2.848 8.8-18.304 5.92-40.704-9.664-55.424L399.936 139.744c-19.264-18.208-49.632-17.344-67.872 1.888-18.208 19.264-17.376 49.632 1.888 67.872l316.96 299.84L335.2 813.632c-19.072 18.4-19.648 48.768-1.248 67.872 9.408 9.792 21.984 14.688 34.56 14.688 12 0 24-4.48 33.312-13.44l350.048-337.376c.672-.672.928-1.6 1.6-2.304.512-.48 1.056-.832 1.568-1.344 2.72-2.848 4.16-6.336 6.016-9.6z" fill="#50505c"></path></svg></div></div><div class="sidebar-list-ul-wrapper"><ul class="sidebar-list-ul"><li class="sidebar-list-li"><a class="sidebar-list-link" href="/23d/database-3nf/"><span class="sidebar-list-title">数据库设计三大范式</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/23d/github-linguist-vendored/"><span class="sidebar-list-title">不统计Github仓库某个目录下的语言</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/24a/deepl-shortcut-setting/"><span class="sidebar-list-title">解决：DeepL该快捷键已被使用</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/24a/git-merge-allow-unrelated-histories/"><span class="sidebar-list-title">记录：使用--allow-unrelated-histories</span></a></li></ul></div></div><div class="sidebar-list show-list"><div class="sidebar-list-head"><h3 class="sidebar-list-category">算法 (8)</h3><div class="sidebar-list-category-rightarrow"><svg viewBox="0 0 1024 1024" width="16px" height="16px" xmlns="http://www.w3.org/2000/svg"><path d="M761.056 532.128c.512-.992 1.344-1.824 1.792-2.848 8.8-18.304 5.92-40.704-9.664-55.424L399.936 139.744c-19.264-18.208-49.632-17.344-67.872 1.888-18.208 19.264-17.376 49.632 1.888 67.872l316.96 299.84L335.2 813.632c-19.072 18.4-19.648 48.768-1.248 67.872 9.408 9.792 21.984 14.688 34.56 14.688 12 0 24-4.48 33.312-13.44l350.048-337.376c.672-.672.928-1.6 1.6-2.304.512-.48 1.056-.832 1.568-1.344 2.72-2.848 4.16-6.336 6.016-9.6z" fill="#50505c"></path></svg></div></div><div class="sidebar-list-ul-wrapper"><ul class="sidebar-list-ul"><li class="sidebar-list-li"><a class="sidebar-list-link" href="/24a/cpp-stl/"><span class="sidebar-list-title">C++中STL的基本使用</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/24a/csp-2016-04/"><span class="sidebar-list-title">CSP 201604 T1-T4题解</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/24a/csp-2018-03/"><span class="sidebar-list-title">CSP 201803 T1-T4题解</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/24a/csp-2020-06/"><span class="sidebar-list-title">CSP 202006 T1-T4题解</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/24a/csp-2020-09/"><span class="sidebar-list-title">CSP 202009 T1-T4题解</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/24a/csp-2020-12/"><span class="sidebar-list-title">CSP 202012 T1-T5题解</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/24a/csp-2023-05/"><span class="sidebar-list-title">CSP 202305 T1-T4题解</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/24a/csp-2023-09/"><span class="sidebar-list-title">CSP 202309 T1-T4题解</span></a></li></ul></div></div><div class="sidebar-list show-list"><div class="sidebar-list-head"><h3 class="sidebar-list-category">深度学习 (5)</h3><div class="sidebar-list-category-rightarrow"><svg viewBox="0 0 1024 1024" width="16px" height="16px" xmlns="http://www.w3.org/2000/svg"><path d="M761.056 532.128c.512-.992 1.344-1.824 1.792-2.848 8.8-18.304 5.92-40.704-9.664-55.424L399.936 139.744c-19.264-18.208-49.632-17.344-67.872 1.888-18.208 19.264-17.376 49.632 1.888 67.872l316.96 299.84L335.2 813.632c-19.072 18.4-19.648 48.768-1.248 67.872 9.408 9.792 21.984 14.688 34.56 14.688 12 0 24-4.48 33.312-13.44l350.048-337.376c.672-.672.928-1.6 1.6-2.304.512-.48 1.056-.832 1.568-1.344 2.72-2.848 4.16-6.336 6.016-9.6z" fill="#50505c"></path></svg></div></div><div class="sidebar-list-ul-wrapper"><ul class="sidebar-list-ul"><li class="sidebar-list-li active"><a class="sidebar-list-link" href="/longtime/papers/"><span class="sidebar-list-title">论文速记</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/23d/r2plus1d/"><span class="sidebar-list-title">行为识别R(2+1)D网络</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/23d/object-detection-map/"><span class="sidebar-list-title">目标检测评价指标mAP</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/23d/learn-rnn-lstm/"><span class="sidebar-list-title">学习RNN和LSTM</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/24a/reproduce-nerf-rpn/"><span class="sidebar-list-title">记录：复现NeRF-RPN代码</span></a></li></ul></div></div><div class="sidebar-list show-list"><div class="sidebar-list-head"><h3 class="sidebar-list-category">Python学习 (2)</h3><div class="sidebar-list-category-rightarrow"><svg viewBox="0 0 1024 1024" width="16px" height="16px" xmlns="http://www.w3.org/2000/svg"><path d="M761.056 532.128c.512-.992 1.344-1.824 1.792-2.848 8.8-18.304 5.92-40.704-9.664-55.424L399.936 139.744c-19.264-18.208-49.632-17.344-67.872 1.888-18.208 19.264-17.376 49.632 1.888 67.872l316.96 299.84L335.2 813.632c-19.072 18.4-19.648 48.768-1.248 67.872 9.408 9.792 21.984 14.688 34.56 14.688 12 0 24-4.48 33.312-13.44l350.048-337.376c.672-.672.928-1.6 1.6-2.304.512-.48 1.056-.832 1.568-1.344 2.72-2.848 4.16-6.336 6.016-9.6z" fill="#50505c"></path></svg></div></div><div class="sidebar-list-ul-wrapper"><ul class="sidebar-list-ul"><li class="sidebar-list-li"><a class="sidebar-list-link" href="/24a/torch-numpy-topk/"><span class="sidebar-list-title">在pytorch和numpy中取top-k值和索引</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/24a/object-oriented-programming-python/"><span class="sidebar-list-title">Python面向对象编程</span></a></li></ul></div></div><div class="sidebar-list show-list"><div class="sidebar-list-head"><h3 class="sidebar-list-category">前端与JavaScript (2)</h3><div class="sidebar-list-category-rightarrow"><svg viewBox="0 0 1024 1024" width="16px" height="16px" xmlns="http://www.w3.org/2000/svg"><path d="M761.056 532.128c.512-.992 1.344-1.824 1.792-2.848 8.8-18.304 5.92-40.704-9.664-55.424L399.936 139.744c-19.264-18.208-49.632-17.344-67.872 1.888-18.208 19.264-17.376 49.632 1.888 67.872l316.96 299.84L335.2 813.632c-19.072 18.4-19.648 48.768-1.248 67.872 9.408 9.792 21.984 14.688 34.56 14.688 12 0 24-4.48 33.312-13.44l350.048-337.376c.672-.672.928-1.6 1.6-2.304.512-.48 1.056-.832 1.568-1.344 2.72-2.848 4.16-6.336 6.016-9.6z" fill="#50505c"></path></svg></div></div><div class="sidebar-list-ul-wrapper"><ul class="sidebar-list-ul"><li class="sidebar-list-li"><a class="sidebar-list-link" href="/23c/js-array/"><span class="sidebar-list-title">JavaScript数组常用方法</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/23d/css-auto-height-transition/"><span class="sidebar-list-title">CSS实现auto高度的过渡动画</span></a></li></ul></div></div><div class="sidebar-list show-list"><div class="sidebar-list-head"><h3 class="sidebar-list-category">课程 (11)</h3><div class="sidebar-list-category-rightarrow"><svg viewBox="0 0 1024 1024" width="16px" height="16px" xmlns="http://www.w3.org/2000/svg"><path d="M761.056 532.128c.512-.992 1.344-1.824 1.792-2.848 8.8-18.304 5.92-40.704-9.664-55.424L399.936 139.744c-19.264-18.208-49.632-17.344-67.872 1.888-18.208 19.264-17.376 49.632 1.888 67.872l316.96 299.84L335.2 813.632c-19.072 18.4-19.648 48.768-1.248 67.872 9.408 9.792 21.984 14.688 34.56 14.688 12 0 24-4.48 33.312-13.44l350.048-337.376c.672-.672.928-1.6 1.6-2.304.512-.48 1.056-.832 1.568-1.344 2.72-2.848 4.16-6.336 6.016-9.6z" fill="#50505c"></path></svg></div></div><div class="sidebar-list-ul-wrapper"><ul class="sidebar-list-ul"><li class="sidebar-list-li"><a class="sidebar-list-link" href="/23c/pattern-recognition-1/"><span class="sidebar-list-title">【模式识别】统计决策方法</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/23c/pattern-recognition-2/"><span class="sidebar-list-title">【模式识别】参数估计</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/23c/pattern-recognition-3/"><span class="sidebar-list-title">【模式识别】非参数估计</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/23d/pattern-recognition-4/"><span class="sidebar-list-title">【模式识别】线性学习器与线性分类器</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/23d/protocols/"><span class="sidebar-list-title">【计算机网络】协议总结</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/24a/machine-learning-exercises/"><span class="sidebar-list-title">【机器学习】习题</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/24a/games101-01-transformation/"><span class="sidebar-list-title">【GAMES101】Transformation</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/24a/games101-02-rasterization/"><span class="sidebar-list-title">【GAMES101】Rasterization</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/24a/games101-03-shading/"><span class="sidebar-list-title">【GAMES101】Shading</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/24a/games101-04-geometry/"><span class="sidebar-list-title">【GAMES101】Geometry</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/24a/games101-05-ray-tracing/"><span class="sidebar-list-title">【GAMES101】Ray Tracing</span></a></li></ul></div></div><div class="sidebar-list show-list"><div class="sidebar-list-head"><h3 class="sidebar-list-category">其他 (2)</h3><div class="sidebar-list-category-rightarrow"><svg viewBox="0 0 1024 1024" width="16px" height="16px" xmlns="http://www.w3.org/2000/svg"><path d="M761.056 532.128c.512-.992 1.344-1.824 1.792-2.848 8.8-18.304 5.92-40.704-9.664-55.424L399.936 139.744c-19.264-18.208-49.632-17.344-67.872 1.888-18.208 19.264-17.376 49.632 1.888 67.872l316.96 299.84L335.2 813.632c-19.072 18.4-19.648 48.768-1.248 67.872 9.408 9.792 21.984 14.688 34.56 14.688 12 0 24-4.48 33.312-13.44l350.048-337.376c.672-.672.928-1.6 1.6-2.304.512-.48 1.056-.832 1.568-1.344 2.72-2.848 4.16-6.336 6.016-9.6z" fill="#50505c"></path></svg></div></div><div class="sidebar-list-ul-wrapper"><ul class="sidebar-list-ul"><li class="sidebar-list-li"><a class="sidebar-list-link" href="/longtime/demo/"><span class="sidebar-list-title">示例</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/longtime/updates/"><span class="sidebar-list-title">更新日志</span></a></li></ul></div></div></div><div id="sidebar-mask"></div></div><script src="/_next/static/chunks/webpack-d56e577e6f69f464.js" crossorigin="" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/css/e43a733539111c31.css\",\"style\",{\"crossOrigin\":\"\"}]\n0:\"$L2\"\n"])</script><script>self.__next_f.push([1,"3:HL[\"/_next/static/css/803fe687e7e31b6c.css\",\"style\",{\"crossOrigin\":\"\"}]\n4:HL[\"/_next/static/css/70373e6f6e0beca4.css\",\"style\",{\"crossOrigin\":\"\"}]\n"])</script><script>self.__next_f.push([1,"5:I[7690,[],\"\"]\n7:I[4500,[\"202\",\"static/chunks/202-fbcb7cc54bf4f9f2.js\",\"452\",\"static/chunks/452-25327cca6690da96.js\",\"487\",\"static/chunks/app/(blogs)/longtime/papers/page-327424e0e36ec255.js\"],\"\"]\n9:I[4365,[\"202\",\"static/chunks/202-fbcb7cc54bf4f9f2.js\",\"452\",\"static/chunks/452-25327cca6690da96.js\",\"487\",\"static/chunks/app/(blogs)/longtime/papers/page-327424e0e36ec255.js\"],\"\"]\na:I[5613,[],\"\"]\nb:I[1778,[],\"\"]\nc:I[9806,[\"792\",\"static/chunks/792-a494ddef3d40c017.js\",\"135\",\"static/chunks/app/(blogs)/layout-565e6400d12a9f60.js\"],\"\"]\nd:I[3393,[\"792\",\"static/chunks/792-a494ddef3d40c017.js\",\"135\",\"static/chunks/app/(blogs)/layout-565e6400d12a9f60.js\"],\"\"]\ne:I[5694,[\"792\",\"static/chunks/792-a494ddef3d40c017.js\",\"185\",\"static/chunks/app/layout-fee515b4ccc2d098.js\"],\"GlobalProvider\"]\nf:I[397,[\"792\",\"static/chunks/792-a494ddef3d40c017.js\",\"185\",\"static/chunks/app/layout-fee515b4ccc2d098.js\"],\"\"]\n11:I[8955,[],\"\"]\n8:T645,从\u003ccode class=\"x-inline-highlight\"\u003e(\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ex\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003ey\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.04398em;\"\u003ez\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\"\u003eθ\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eϕ\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e)\u003c/code\u003e到\u003ccode class=\"x-inline-highlight\"\u003e(\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8778em;vertical-align:-0.1944em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.00773em;\"\u003eR\u003c"])</script><script>self.__next_f.push([1,"/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eG\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.05017em;\"\u003eB\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003eσ\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e)\u003c/code\u003e12:[]\n"])</script><script>self.__next_f.push([1,"2:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/e43a733539111c31.css\",\"precedence\":\"next\",\"crossOrigin\":\"\"}]],[\"$\",\"$L5\",null,{\"buildId\":\"HIlA3j4Igwf_YBWZLyozd\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/longtime/papers/\",\"initialTree\":[\"\",{\"children\":[\"(blogs)\",{\"children\":[\"longtime\",{\"children\":[\"papers\",{\"children\":[\"__PAGE__\",{}]}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"(blogs)\",{\"children\":[\"longtime\",{\"children\":[\"papers\",{\"children\":[\"__PAGE__\",{},[\"$L6\",[[\"$\",\"h1\",null,{\"className\":\"x-title\",\"children\":\"论文速记\"}],[\"$\",\"h2\",null,{\"className\":\"x-h1\",\"children\":[\"$\",\"$L7\",null,{\"excludeFromContents\":\"$undefined\",\"children\":\"研究\"}]}],[\"$\",\"h3\",null,{\"className\":\"x-h2\",\"children\":[\"$\",\"$L7\",null,{\"excludeFromContents\":\"$undefined\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/pdf/2003.08934.pdf\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":\"【NeRF】NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis (2020)\"}]}]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-gray\",\"children\":[false,[[\"$\",\"div\",null,{\"className\":\"x-oli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-oli-number\",\"children\":\"1.\"}],[\"$\",\"div\",null,{\"className\":\"x-oli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的主题 / 文章要解决什么问题？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"使用稀疏输入2D图片集实现场景的3D视图合成\"}}]]}]]}],[\"$\",\"div\",null,{\"className\":\"x-oli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-oli-number\",\"children\":\"2.\"}],[\"$\",\"div\",null,{\"className\":\"x-oli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的核心方法 / 具体是如何做的？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"$8\"}}]]}]]}],[\"$\",\"div\",null,{\"className\":\"x-oli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-oli-number\",\"children\":\"3.\"}],[\"$\",\"div\",null,{\"className\":\"x-oli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e做了什么实验，效果怎么样？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"测试数据集是\u003ccode class=\\\"x-inline-highlight\\\"\u003eDiffuse Synthetic 360\u003c/code\u003e、\u003ccode class=\\\"x-inline-highlight\\\"\u003eRealistic Synthetic 360\u003c/code\u003e和\u003ccode class=\\\"x-inline-highlight\\\"\u003eReal Forward-Facing\u003c/code\u003e\"}}]]}]]}],[\"$\",\"div\",null,{\"className\":\"x-oli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-oli-number\",\"children\":\"4.\"}],[\"$\",\"div\",null,{\"className\":\"x-oli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e研究的创新点\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"将输入坐标位置编码，帮助MLP表示高频函数\u003cbr/\u003e分层采样\"}}]]}]]}],[\"$\",\"div\",null,{\"className\":\"x-oli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-oli-number\",\"children\":\"5.\"}],[\"$\",\"div\",null,{\"className\":\"x-oli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e有什么限制或可以改进的地方？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"有效地优化和渲染神经辐射场\u003cbr/\u003e可解释性\"}}]]}]]}]]]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-blue\",\"children\":[[\"$\",\"h4\",null,{\"className\":\"x-h3\",\"children\":\"更多笔记\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"神经辐射场用于从2D的图片重建3D的场景。\"}}],[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"文中出现的三个指标：PSNR、SSIM、LPIPS\"}}],[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e峰值信噪比\u003c/span\u003e\u003ccode class=\\\"x-inline-highlight\\\"\u003e(Peak Signal to Noise Ratio, PSNR)\u003c/code\u003e：用于衡量图像恢复的质量，数值越高表示图像质量越好。接近\u003ccode class=\\\"x-inline-highlight\\\"\u003e50 dB\u003c/code\u003e代表误差非常小，大于\u003ccode class=\\\"x-inline-highlight\\\"\u003e30 dB\u003c/code\u003e人眼难察觉差异。\"}}]}]]}],[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e结构相似性\u003c/span\u003e\u003ccode class=\\\"x-inline-highlight\\\"\u003e(Structural Similarity Index Measure, SSIM)\u003c/code\u003e：用于衡量图像的结构相似性，得分通常在\u003ccode class=\\\"x-inline-highlight\\\"\u003e0\u003c/code\u003e~\u003ccode class=\\\"x-inline-highlight\\\"\u003e1\u003c/code\u003e之间，数值越高表示图像结构越相似。相较于PSNR在图像质量的衡量上更能符合人眼对图像质量的判断。\"}}]}]]}],[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e基于学习的感知图像质量评价\u003c/span\u003e\u003ccode class=\\\"x-inline-highlight\\\"\u003e(Learned Perceptual Image Patch Similarity, LPIPS)\u003c/code\u003e：测量从预训练网络中提取的两个图像的特征之间的相似性，得分通常在\u003ccode class=\\\"x-inline-highlight\\\"\u003e0\u003c/code\u003e~\u003ccode class=\\\"x-inline-highlight\\\"\u003e1\u003c/code\u003e之间，数值越低表示感知质量越高。\"}}]}]]}]]}],[\"$\",\"h3\",null,{\"className\":\"x-h2\",\"children\":[\"$\",\"$L7\",null,{\"excludeFromContents\":\"$undefined\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/pdf/2308.04079.pdf\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":\"【3DGS】3D Gaussian Splatting for Real-Time Radiance Field Rendering (2023)\"}]}]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-gray\",\"children\":[false,[[\"$\",\"div\",null,{\"className\":\"x-oli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-oli-number\",\"children\":\"1.\"}],[\"$\",\"div\",null,{\"className\":\"x-oli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的主题 / 文章要解决什么问题？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"实现实时辐射场渲染，同时保持高质量的视觉效果，并且保持较短的训练时间\"}}]]}]]}],\"$undefined\",\"$undefined\",\"$undefined\",\"$undefined\"]]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-blue\",\"children\":[[\"$\",\"h4\",null,{\"className\":\"x-h3\",\"children\":\"更多笔记\"}],[\"$\",\"h4\",null,{\"className\":\"x-h3\",\"children\":\"文章的相关工作部分\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"传统的场景重建与渲染：基于光场的，密集采样、非结构化捕获；\u003cspan class=\\\"x-inline-strong\\\"\u003e运动恢复结构\u003c/span\u003e\u003ccode class=\\\"x-inline-highlight\\\"\u003e(Structure from Motion, SFM)\u003c/code\u003e用一组照片估计稀疏点云合成新视图；\u003cspan class=\\\"x-inline-strong\\\"\u003e多视点立体视觉\u003c/span\u003e\u003ccode class=\\\"x-inline-highlight\\\"\u003e(Multi-View Stereo, MVS)\u003c/code\u003e；\u003cbr/\u003e 神经渲染和辐射场：用CNN估计混合权重，用于纹理空间；Soft3D提出\u003ccode class=\\\"x-inline-highlight\\\"\u003eVolumetric representations\u003c/code\u003e；NeRF提出重要性采样和位置编码来提高质量，但使用了大型多层感知器，对速度有负面影响；\"}}],[\"$\",\"h4\",null,{\"className\":\"x-h3\",\"children\":\"稀疏重建和稠密重建\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"稀疏重建主要用于定位，得到每张图片的相机参数，提取特征点，例如SFM；稠密重建是假设相机参数已知的情况下，从不同视角的图像中找到匹配的对应点，对整个图像或图像中绝大部分像素进行重建。\"}}]]}],[\"$\",\"h3\",null,{\"className\":\"x-h2\",\"children\":[\"$\",\"$L7\",null,{\"excludeFromContents\":\"$undefined\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/pdf/2201.05989.pdf\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":\"【Instant NGP】Instant Neural Graphics Primitives with a Multiresolution Hash Encoding (2022)\"}]}]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-gray\",\"children\":[false,[[\"$\",\"div\",null,{\"className\":\"x-oli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-oli-number\",\"children\":\"1.\"}],[\"$\",\"div\",null,{\"className\":\"x-oli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的主题 / 文章要解决什么问题？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"实现实时辐射场渲染，同时保持高质量的视觉效果，并且保持较短的训练时间\"}}]]}]]}],\"$undefined\",\"$undefined\",\"$undefined\",\"$undefined\"]]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-blue\",\"children\":[[\"$\",\"h4\",null,{\"className\":\"x-h3\",\"children\":\"更多笔记\"}],[\"$\",\"h4\",null,{\"className\":\"x-h3\",\"children\":\"Instant NGP与NeRF的异同\"}],[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"转载自\u003ca href=\\\"https://zhuanlan.zhihu.com/p/631284285\\\" target=\\\"_blank\\\" rel=\\\"noreferrer\\\" class=\\\"x-inline-link\\\"\u003e知乎：从NeRF到Instant-NGP\u003c/a\u003e\"}}],[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"同样基于体渲染\"}}]}]]}],[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"不同于NeRF的MLP，Instant NGP使用稀疏的参数化的\u003ccode class=\\\"x-inline-highlight\\\"\u003evoxel grid\u003c/code\u003e作为场景表达\"}}]}]]}]]}],[\"$\",\"h3\",null,{\"className\":\"x-h2\",\"children\":[\"$\",\"$L7\",null,{\"excludeFromContents\":\"$undefined\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/pdf/2211.11646.pdf\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":\"【NeRF RPN】NeRF-RPN: A general framework for object detection in NeRFs (2022)\"}]}]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-gray\",\"children\":[false,[[\"$\",\"div\",null,{\"className\":\"x-oli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-oli-number\",\"children\":\"1.\"}],[\"$\",\"div\",null,{\"className\":\"x-oli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的主题 / 文章要解决什么问题？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"在NeRF中直接进行3D物体检测\"}}]]}]]}],[\"$\",\"div\",null,{\"className\":\"x-oli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-oli-number\",\"children\":\"2.\"}],[\"$\",\"div\",null,{\"className\":\"x-oli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的核心方法 / 具体是如何做的？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"第一部分：特征提取器\u003cbr/\u003e从NeRF采样的辐射度和密度网格作为输入，生成特征金字塔作为输出。\u003cbr/\u003e 第二部分：RPN头\u003cbr/\u003e对特征金字塔进行操作并生成对象建议。\"}}]]}]]}],\"$undefined\",[\"$\",\"div\",null,{\"className\":\"x-oli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-oli-number\",\"children\":\"3.\"}],[\"$\",\"div\",null,{\"className\":\"x-oli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e研究的创新点\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"第一次将RPN引入NeRF以进行3D物体检测和相关任务\u003cbr/\u003e利用Hypersim和3D-FRONT数据集构建了第一个用于3D目标检测的NeRF数据集\"}}]]}]]}],\"$undefined\"]]}],[\"$\",\"h3\",null,{\"className\":\"x-h2\",\"children\":[\"$\",\"$L7\",null,{\"excludeFromContents\":\"$undefined\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/pdf/2304.04395v3.pdf\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":\"【Instance NeRF】Instance Neural Radiance Field (2023)\"}]}]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-gray\",\"children\":[false,[[\"$\",\"div\",null,{\"className\":\"x-oli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-oli-number\",\"children\":\"1.\"}],[\"$\",\"div\",null,{\"className\":\"x-oli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的主题 / 文章要解决什么问题？\u003c/span\u003e\"}}],[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"输入一个以多视图RGB图像预训练的NeRF，学习给定场景的3D实例分割\"}}],[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"文章的主要贡献：\"}}],[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"第一个在NeRF中进行3D实例分割的尝试之一，而没有使用真实分割标签作为输入\"}}]}]]}],[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"提出\u003ccode class=\\\"x-inline-highlight\\\"\u003eNeural Instance Field\u003c/code\u003e的结构和训练方法，可以产生\u003cspan class=\\\"x-inline-strong\\\"\u003e多视图一致\u003c/span\u003e的2D分割和连续的3D分割\"}}]}]]}],[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"对合成室内NeRF数据集进行实验和消融研究\"}}]}]]}]]]}]]}],[\"$\",\"div\",null,{\"className\":\"x-oli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-oli-number\",\"children\":\"2.\"}],[\"$\",\"div\",null,{\"className\":\"x-oli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的核心方法 / 具体是如何做的？\u003c/span\u003e\"}}],[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"Instance NeRF有两个组件：预训练的NeRF模型、和文中提出的\u003ccode class=\\\"x-inline-highlight\\\"\u003eInstance Field\u003c/code\u003e。\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003ccode class=\\\"x-inline-highlight\\\"\u003eInstance Field\u003c/code\u003e的训练过程如下：\"}}],[\"$\",\"$L9\",null,{\"src\":\"instance_field.jpg\",\"width\":\"96%\",\"invertInDarkTheme\":true}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"NeRF-RCNN用预训练的NeRF提取的辐射场和密度场，为每个检测到的对象输出3D掩码；Mask2Former生成从NeRF渲染的图像的二维全景分割图（跨视图的实例标签并不一定一致）。然后按照相机位置，投影3D掩码去匹配不同视图中的相同实例，去产生多视图一致的2D分割图。CascadePSP用于细化2D mask。\"}}]]]}]]}],\"$undefined\",\"$undefined\",\"$undefined\"]]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-blue\",\"children\":[[\"$\",\"h4\",null,{\"className\":\"x-h3\",\"children\":\"更多笔记\"}],[\"$\",\"h4\",null,{\"className\":\"x-h3\",\"children\":\"包围体：AABB和OBB\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003eAABB\u003c/span\u003e：轴对齐包围盒\u003ccode class=\\\"x-inline-highlight\\\"\u003e(Axis-Aligned Bounding Box)\u003c/code\u003e\u003cbr/\u003e\u003cspan class=\\\"x-inline-strong\\\"\u003eOBB\u003c/span\u003e：有向包围盒\u003ccode class=\\\"x-inline-highlight\\\"\u003e(Oriented Bounding Box)\u003c/code\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"下图展示了更多种类的包围体：\"}}],[\"$\",\"$L9\",null,{\"src\":\"bounding_volumes.png\",\"width\":\"100%\",\"invertInDarkTheme\":true}]]}],[\"$\",\"h3\",null,{\"className\":\"x-h2\",\"children\":[\"$\",\"$L7\",null,{\"excludeFromContents\":\"$undefined\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://openaccess.thecvf.com/content_CVPR_2020/papers/Cheng_CascadePSP_Toward_Class-Agnostic_and_Very_High-Resolution_Segmentation_via_Global_and_CVPR_2020_paper.pdf\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":\"【CascadePSP】CascadePSP: Toward Class-Agnostic and Very High-Resolution Segmentation via Global and Local Refinement (2020)\"}]}]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-gray\",\"children\":[false,[[\"$\",\"div\",null,{\"className\":\"x-oli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-oli-number\",\"children\":\"1.\"}],[\"$\",\"div\",null,{\"className\":\"x-oli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的主题 / 文章要解决什么问题？\u003c/span\u003e\"}}],[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"提出一种不使用高分辨率训练数据，解决高分辨率分割问题的方法\u003cbr/\u003e右图是改进后的结果：\"}}],[\"$\",\"$L9\",null,{\"src\":\"cascadepsp.jpg\",\"width\":\"400px\",\"invertInDarkTheme\":true}]]]}]]}],\"$undefined\",\"$undefined\",\"$undefined\",\"$undefined\"]]}],[\"$\",\"h3\",null,{\"className\":\"x-h2\",\"children\":[\"$\",\"$L7\",null,{\"excludeFromContents\":\"$undefined\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/pdf/2312.00860.pdf\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":\"【SAGA】Segment Any 3D Gaussians (2023)\"}]}]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-gray\",\"children\":[false,[[\"$\",\"div\",null,{\"className\":\"x-oli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-oli-number\",\"children\":\"1.\"}],[\"$\",\"div\",null,{\"className\":\"x-oli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的主题 / 文章要解决什么问题？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"交互式3D分割\"}}]]}]]}],\"$undefined\",\"$undefined\",\"$undefined\",\"$undefined\"]]}],[\"$\",\"h2\",null,{\"className\":\"x-h1\",\"children\":[\"$\",\"$L7\",null,{\"excludeFromContents\":\"$undefined\",\"children\":\"学习\"}]}],[\"$\",\"h3\",null,{\"className\":\"x-h2\",\"children\":[\"$\",\"$L7\",null,{\"excludeFromContents\":\"$undefined\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":\"【R-CNN】Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation (2014)\"}]}]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-gray\",\"children\":[false,[[\"$\",\"div\",null,{\"className\":\"x-oli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-oli-number\",\"children\":\"1.\"}],[\"$\",\"div\",null,{\"className\":\"x-oli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的主题 / 文章要解决什么问题？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"提出\u003ccode class=\\\"x-inline-highlight\\\"\u003eRegions with CNN features, R-CNN\u003c/code\u003e提高目标检测性能\"}}]]}]]}],[\"$\",\"div\",null,{\"className\":\"x-oli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-oli-number\",\"children\":\"2.\"}],[\"$\",\"div\",null,{\"className\":\"x-oli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的核心方法 / 具体是如何做的？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"区域提议\u003ccode class=\\\"x-inline-highlight\\\"\u003e(Region Proposals)\u003c/code\u003e：使用\u003ccode class=\\\"x-inline-highlight\\\"\u003eselective search\u003c/code\u003e生成候选框\"}}]]}]]}],[\"$\",\"div\",null,{\"className\":\"x-oli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-oli-number\",\"children\":\"3.\"}],[\"$\",\"div\",null,{\"className\":\"x-oli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e做了什么实验，效果怎么样？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"在\u003ccode class=\\\"x-inline-highlight\\\"\u003ePASCAL VOC 2012\u003c/code\u003e取得\u003ccode class=\\\"x-inline-highlight\\\"\u003emAP 53.3%\u003c/code\u003e，在\u003ccode class=\\\"x-inline-highlight\\\"\u003eILSVRC 2013\u003c/code\u003e竞赛数据集取得\u003ccode class=\\\"x-inline-highlight\\\"\u003emAP 31.4%\u003c/code\u003e\"}}]]}]]}],\"$undefined\",\"$undefined\"]]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-blue\",\"children\":[[\"$\",\"h4\",null,{\"className\":\"x-h3\",\"children\":\"更多笔记\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"转载自\u003ca href=\\\"https://zh-v2.d2l.ai/chapter_computer-vision/rcnn.html\\\" target=\\\"_blank\\\" rel=\\\"noreferrer\\\" class=\\\"x-inline-link\\\"\u003e动手学深度学习 - 区域卷积神经网络系列\u003c/a\u003e\"}}],[\"$\",\"h4\",null,{\"className\":\"x-h3\",\"children\":\"R-CNN\"}],[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"R-CNN使用启发式搜索算法\u003ccode class=\\\"x-inline-highlight\\\"\u003eselective search\u003c/code\u003e（之前人们通常也是这样做的）来选择锚框\"}}]}]]}],[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"用预训练的模型，对每一个锚框抽取特征\"}}]}]]}],[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"训练一个SVM来对类别分类\"}}]}]]}],[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"训练一个线性回归模型来预测边缘框\"}}]}]]}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"R-CNN的速度很慢，因为可能从一张图像中选出上千个提议区域，这需要上千次的卷积神经网络的前向传播来执行目标检测。这种庞大的计算量使得R-CNN在现实世界中难以被广泛应用。\"}}],[\"$\",\"$L9\",null,{\"src\":\"rcnn.jpg\",\"width\":\"600px\"}],[\"$\",\"h4\",null,{\"className\":\"x-h3\",\"children\":\"Fast R-CNN\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"R-CNN的主要性能瓶颈在于，对每个提议区域，卷积神经网络的前向传播是独立的，而没有共享计算。由于这些区域通常有重叠，独立的特征抽取会导致重复的计算。Fast R-CNN的主要改进之一，是仅在整张图象上执行卷积神经网络的前向传播，并且引入\u003cspan class=\\\"x-inline-strong\\\"\u003e兴趣区域池化\u003c/span\u003e\u003ccode class=\\\"x-inline-highlight\\\"\u003e(ROI Pooling)\u003c/code\u003e，将卷积神经网络的输出和提议区域作为输入，输出连结后的各个提议区域抽取的特征。\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"兴趣区域池化层可以给出固定大小的输出：把给定的锚框均匀分割成\u003cspan class=\\\"katex\\\"\u003e\u003cspan class=\\\"katex-html\\\" aria-hidden=\\\"true\\\"\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:0.6667em;vertical-align:-0.0833em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003en\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2222em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mbin\\\"\u003e×\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2222em;\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:0.4306em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003em\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e块，输出每块里的最大值，这样无论锚框多大，总是输出\u003cspan class=\\\"katex\\\"\u003e\u003cspan class=\\\"katex-html\\\" aria-hidden=\\\"true\\\"\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:0.4306em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003enm\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e个值。\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"Fast R-CNN先对图片用CNN抽取特征，然后将\u003ccode class=\\\"x-inline-highlight\\\"\u003eselective search\u003c/code\u003e给出的原图上的提议区域映射到CNN特征图上，再经过\u003ccode class=\\\"x-inline-highlight\\\"\u003eROI Pooling\u003c/code\u003e就可以得到维度对齐的特征。\"}}],[\"$\",\"$L9\",null,{\"src\":\"fastrcnn.jpg\",\"width\":\"400px\"}],[\"$\",\"h4\",null,{\"className\":\"x-h3\",\"children\":\"Faster R-CNN\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"与Fast R-CNN相比，Faster R-CNN将生成提议区域的方法从\u003ccode class=\\\"x-inline-highlight\\\"\u003eselective search\u003c/code\u003e改为了\u003cspan class=\\\"x-inline-strong\\\"\u003e区域提议网络\u003c/span\u003e\u003ccode class=\\\"x-inline-highlight\\\"\u003e(Region Proposal Network, RPN)\u003c/code\u003e，模型的其余部分保持不变。区域提议网络作为Faster R-CNN模型的一部分，是和整个模型一起训练得到的。换句话说，Faster R-CNN的目标函数不仅包括目标检测中的类别和边界框预测，还包括区域提议网络中锚框的二元类别和边界框预测。作为端到端训练的结果，区域提议网络能够学习到如何生成高质量的提议区域，从而在减少了从数据中学习的提议区域的数量的情况下，仍保持目标检测的精度。\"}}],[\"$\",\"$L9\",null,{\"src\":\"fasterrcnn.jpg\",\"width\":\"600px\"}]]}],[\"$\",\"h3\",null,{\"className\":\"x-h2\",\"children\":[\"$\",\"$L7\",null,{\"excludeFromContents\":\"$undefined\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/pdf/1703.06870.pdf\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":\"【Mask R-CNN】Mask R-CNN (2017)\"}]}]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-gray\",\"children\":[false,[[\"$\",\"div\",null,{\"className\":\"x-oli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-oli-number\",\"children\":\"1.\"}],[\"$\",\"div\",null,{\"className\":\"x-oli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的主题 / 文章要解决什么问题？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"目标检测\u0026实例分割\"}}]]}]]}],[\"$\",\"div\",null,{\"className\":\"x-oli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-oli-number\",\"children\":\"2.\"}],[\"$\",\"div\",null,{\"className\":\"x-oli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的核心方法 / 具体是如何做的？\u003c/span\u003e\"}}],[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"基于Faster R-CNN，添加一个对每个ROI预测分割mask的分支。这个分支可以与分类和边界框预测分支并行。\"}}],[\"$\",\"$L9\",null,{\"src\":\"maskrcnn1.jpg\",\"width\":\"600px\"}]]]}]]}],[\"$\",\"div\",null,{\"className\":\"x-oli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-oli-number\",\"children\":\"3.\"}],[\"$\",\"div\",null,{\"className\":\"x-oli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e做了什么实验，效果怎么样？\u003c/span\u003e\"}}],[[\"$\",\"$L9\",null,{\"src\":\"maskrcnn2.jpg\",\"width\":\"600px\",\"invertInDarkTheme\":true}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"在COCO数据集上表现优异，超过了\u003ccode class=\\\"x-inline-highlight\\\"\u003e2015\u003c/code\u003e和\u003ccode class=\\\"x-inline-highlight\\\"\u003e2016\u003c/code\u003e年COCO分割任务的冠军。\"}}]]]}]]}],[\"$\",\"div\",null,{\"className\":\"x-oli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-oli-number\",\"children\":\"4.\"}],[\"$\",\"div\",null,{\"className\":\"x-oli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e研究的创新点\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\" \u003ccode class=\\\"x-inline-highlight\\\"\u003eROI Pooling\u003c/code\u003e中发生了两次浮点数取整，第一次是将锚框均匀分割成\u003cspan class=\\\"katex\\\"\u003e\u003cspan class=\\\"katex-html\\\" aria-hidden=\\\"true\\\"\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:0.6667em;vertical-align:-0.0833em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003en\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2222em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mbin\\\"\u003e×\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2222em;\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:0.4306em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003em\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e块时，第二次是把提议区域映射回CNN特征图时。\u003cbr/\u003e 分割任务的精细程度更高，因此文章提出了\u003ccode class=\\\"x-inline-highlight\\\"\u003eROI Align\u003c/code\u003e，使用双线性插值来保留特征图上的空间信息。 \"}}]]}]]}],\"$undefined\"]]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-blue\",\"children\":[[\"$\",\"h4\",null,{\"className\":\"x-h3\",\"children\":\"更多笔记\"}],[\"$\",\"h4\",null,{\"className\":\"x-h3\",\"children\":\"语义分割与实例分割\"}],[\"$\",\"$L9\",null,{\"src\":\"ss_and_is.jpg\",\"width\":\"600px\"}],[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e语义分割\u003c/span\u003e\u003ccode class=\\\"x-inline-highlight\\\"\u003e(semantic segmentation)\u003c/code\u003e：为每一个像素分配一个类别，但不区分同一类别之间的对象。\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e实例分割\u003c/span\u003e\u003ccode class=\\\"x-inline-highlight\\\"\u003e(instance segmentation)\u003c/code\u003e：会区分属于同一类别的不同实例。\"}}]]}],[\"$\",\"h3\",null,{\"className\":\"x-h2\",\"children\":[\"$\",\"$L7\",null,{\"excludeFromContents\":\"$undefined\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":\"【FCN】Fully Convolutional Networks for Semantic Segmentation (2015)\"}]}]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-gray\",\"children\":[false,[[\"$\",\"div\",null,{\"className\":\"x-oli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-oli-number\",\"children\":\"1.\"}],[\"$\",\"div\",null,{\"className\":\"x-oli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的主题 / 文章要解决什么问题？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"使用全卷积网络进行语义分割\"}}]]}]]}],[\"$\",\"div\",null,{\"className\":\"x-oli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-oli-number\",\"children\":\"2.\"}],[\"$\",\"div\",null,{\"className\":\"x-oli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的核心方法 / 具体是如何做的？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\" 是将现有的分类网络（AlexNet、VGG Net、GoogLeNet）改造为全卷积网络，以便在语义分割任务上进行端到端（输入图像，输出分割掩码）的训练。\u003cbr/\u003e 改造的方式是将最后的全连接层替换成卷积层。 \"}}]]}]]}],\"$undefined\",\"$undefined\",\"$undefined\"]]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-blue\",\"children\":[[\"$\",\"h4\",null,{\"className\":\"x-h3\",\"children\":\"更多笔记\"}],[\"$\",\"h4\",null,{\"className\":\"x-h3\",\"children\":\"转置卷积\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"卷积通常不会增大输入的高宽，而是保持不变或降低。由于语义分割任务需要像素级别的输出，转置卷积被用来增大输入的高宽。\"}}],[\"$\",\"$L9\",null,{\"src\":\"transpose_convolution.jpg\",\"width\":\"600px\",\"invertInDarkTheme\":true}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"图片转载自\u003ca href=\\\"https://zh-v2.d2l.ai/chapter_computer-vision/transposed-conv.html\\\" target=\\\"_blank\\\" rel=\\\"noreferrer\\\" class=\\\"x-inline-link\\\"\u003e动手学深度学习 - 转置卷积\u003c/a\u003e\"}}],[\"$\",\"h4\",null,{\"className\":\"x-h3\",\"children\":\"FCN中的转置卷积\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"例如对于ImageNet的图片输入，大小通常为\u003ccode class=\\\"x-inline-highlight\\\"\u003e224\u003c/code\u003e\u0026#42;\u003ccode class=\\\"x-inline-highlight\\\"\u003e224\u003c/code\u003e\u0026#42;\u003ccode class=\\\"x-inline-highlight\\\"\u003e3\u003c/code\u003e（RGB通道）；经过卷积后缩小宽高缩小\u003ccode class=\\\"x-inline-highlight\\\"\u003e32\u003c/code\u003e倍，通道增加到\u003ccode class=\\\"x-inline-highlight\\\"\u003e512\u003c/code\u003e，变成\u003ccode class=\\\"x-inline-highlight\\\"\u003e7\u003c/code\u003e\u0026#42;\u003ccode class=\\\"x-inline-highlight\\\"\u003e7\u003c/code\u003e\u0026#42;\u003ccode class=\\\"x-inline-highlight\\\"\u003e512\u003c/code\u003e的特征图。此时FCN会先通过一个\u003ccode class=\\\"x-inline-highlight\\\"\u003e1x1conv\u003c/code\u003e进行通道降维，然后通过转置卷积将特征图的高度和宽度增加\u003ccode class=\\\"x-inline-highlight\\\"\u003e32\u003c/code\u003e倍，\u003cspan class=\\\"x-inline-strong\\\"\u003e输出通道数等于类别数\u003c/span\u003e，相当于储存了对每一类的预测结果。\"}}]]}]],null]]},[\"$\",\"$La\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"(blogs)\",\"children\",\"longtime\",\"children\",\"papers\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Lb\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/70373e6f6e0beca4.css\",\"precedence\":\"next\",\"crossOrigin\":\"\"}]]}]]},[\"$\",\"$La\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"(blogs)\",\"children\",\"longtime\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Lb\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}]]},[null,[\"$\",\"div\",null,{\"id\":\"blog-layout\",\"children\":[[\"$\",\"div\",null,{\"id\":\"main\",\"children\":[\"$\",\"$La\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"(blogs)\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Lb\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}]}],[\"$\",\"$Lc\",null,{}],[\"$\",\"$Ld\",null,{}]]}],null]]},[null,[\"$\",\"html\",null,{\"lang\":\"zh-CN\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"script\",null,{\"async\":true,\"src\":\"https://www.googletagmanager.com/gtag/js?id=G-45BYSZ6WPY\"}],[\"$\",\"script\",null,{\"async\":true,\"src\":\"https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js\"}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\nwindow.dataLayer = window.dataLayer || [];\\nfunction gtag() {\\n    dataLayer.push(arguments);\\n}\\ngtag('js', new Date());\\ngtag('config', 'G-45BYSZ6WPY');\\nif (!localStorage.getItem('theme')) localStorage.setItem('theme', 'light');\\ndocument.documentElement.setAttribute('class', localStorage.getItem('theme'));\\n\"}}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"const a=z=\u003eh.getItem(z),b=(y,z)=\u003eh.setItem(y,z),c=(y,z)=\u003edocument.documentElement.setAttribute(y,z),d='theme',e='dark',f='light',g='class',h=localStorage;a(d)!==e\u0026\u0026a(d)!==f\u0026\u0026b(d,f);a(d)===e?c(g,e):c(g,f);\"}}]]}],[\"$\",\"body\",null,{\"children\":[\"$\",\"$Le\",null,{\"children\":[[\"$\",\"$Lf\",null,{}],[\"$\",\"$La\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Lb\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$\",\"div\",null,{\"id\":\"notfound\",\"children\":[[\"$\",\"img\",null,{\"alt\":\"img\",\"src\":\"/images/cry.gif\"}],[\"$\",\"div\",null,{\"id\":\"notfound-404\",\"children\":\"404\"}],[\"$\",\"div\",null,{\"id\":\"notfound-text\",\"children\":\"Page Not Found\"}]]}],\"notFoundStyles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/eccd2e7a1149e571.css\",\"precedence\":\"next\",\"crossOrigin\":\"\"}]],\"styles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/803fe687e7e31b6c.css\",\"precedence\":\"next\",\"crossOrigin\":\"\"}]]}]]}]}]]}],null]],\"initialHead\":[false,\"$L10\"],\"globalErrorComponent\":\"$11\",\"missingSlots\":\"$W12\"}]]\n"])</script><script>self.__next_f.push([1,"10:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"论文速记 - 铃木的网络日记\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"...\"}],[\"$\",\"link\",\"4\",{\"rel\":\"canonical\",\"href\":\"https://1kuzus.github.io/longtime/papers/\"}]]\n6:null\n"])</script><script>self.__next_f.push([1,""])</script></body></html>