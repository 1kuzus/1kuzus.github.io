<!DOCTYPE html><html lang="zh-CN"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/css/268aef745a7b3740.css" crossorigin="" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/20451da2c8ddd48c.css" crossorigin="" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-f7800ef344ebad69.js" crossorigin=""/><script src="/_next/static/chunks/fd9d1056-e278622fa62dc0bf.js" async="" crossorigin=""></script><script src="/_next/static/chunks/69-faab496ef6116e58.js" async="" crossorigin=""></script><script src="/_next/static/chunks/main-app-ed6799cd4e1df27b.js" async="" crossorigin=""></script><script src="/_next/static/chunks/457b8330-b5c9cf3fcf214847.js" async=""></script><script src="/_next/static/chunks/d3ac728e-0c798b3b8aa3bf53.js" async=""></script><script src="/_next/static/chunks/250-0ef8476c0fa8ee24.js" async=""></script><script src="/_next/static/chunks/612-fa632c1349770315.js" async=""></script><script src="/_next/static/chunks/551-19232c47cd1e883a.js" async=""></script><script src="/_next/static/chunks/app/%5B...slug%5D/page-954d1692838af271.js" async=""></script><script src="/_next/static/chunks/app/layout-29c171f9500c7269.js" async=""></script><script async="" src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-45BYSZ6WPY"></script><title>论文速记：深度学习 - 铃木的网络日记</title><link rel="canonical" href="https://1kuzus.github.io/longtime/papers-dl/"/><link rel="icon" href="/favicon.ico" type="image/x-icon"/><script>
window.dataLayer = window.dataLayer || [];
function gtag() {
    dataLayer.push(arguments);
}
gtag('js', new Date());
gtag('config', 'G-45BYSZ6WPY');
</script><script>const a=z=>h.getItem(z),b=(y,z)=>h.setItem(y,z),c=(y,z)=>document.documentElement.setAttribute(y,z),d='theme',e='dark',f='light',g='class',h=localStorage;a(d)!==e&&a(d)!==f&&b(d,f);a(d)===e?c(g,e):c(g,f);</script><script>
if (!Array.prototype.findLast) {
    Array.prototype.findLast = function (callback) {
        for (let i = this.length - 1; i >= 0; i--) {
            if (callback(this[i])) return this[i];
        }
        return undefined;
    };
}
if (!Array.prototype.findLastIndex) {
    Array.prototype.findLastIndex = function (callback) {
        for (let i = this.length - 1; i >= 0; i--) {
            if (callback(this[i])) return i;
        }
        return -1;
    };
}
</script><script src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js" crossorigin="" noModule=""></script></head><body><div id="header"><div id="header-left-wrapper"><a href="/"><div id="header-logo-bg"><svg xmlns="http://www.w3.org/2000/svg" width="36" height="36" viewBox="0 0 1560 1560"><path fill="#00A8C4" d="M644 97h273l272 683H916z"></path><path fill="#30303C" d="M98 97h273l409 1024L1189 97h273L916 1462H644z"></path><path fill="#00F8FF" d="M98 1462 643 97h273L371 1462z"></path></svg></div></a></div><div id="header-right-wrapper"><button id="header-show-sidebar-button" class="header-button-bg"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 1024 1024"><rect fill="#FCFCFC" x="100" y="148" width="824" height="128" rx="64"></rect><rect fill="#FCFCFC" x="100" y="448" width="824" height="128" rx="64"></rect><rect fill="#FCFCFC" x="100" y="748" width="824" height="128" rx="64"></rect></svg></button><button class="header-button-bg"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 1024 1024" class="svg-dark-theme-icon"><path fill="#FCFCFC" d="M525 939h-4a440 440 0 0 1-314-135 446 446 0 0 1-11-597A432 432 0 0 1 367 90a43 43 0 0 1 45 9 43 43 0 0 1 10 43 358 358 0 0 0 83 376 361 361 0 0 0 377 83 43 43 0 0 1 54 55 433 433 0 0 1-100 155 439 439 0 0 1-311 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 1024 1024" class="svg-light-theme-icon"><path fill="#FCFCFC" d="M512 61c13 0 25 8 30 20l28 68c11 29-5 62-36 73a66 66 0 0 1-22 3c-34 0-61-25-61-56 0-7 1-14 3-20l28-68c5-12 17-20 30-20zm0 902c-13 0-25-8-30-20l-28-68a53 53 0 0 1-3-20c0-31 27-56 61-56a66 66 0 0 1 22 3c31 11 47 44 36 73l-28 68c-5 12-17 20-30 20zm451-451c0 13-8 25-20 30l-68 28c-29 11-62-5-73-36a66 66 0 0 1-3-22c0-34 25-61 56-61 7 0 14 1 20 3l68 28c12 5 20 17 20 30zm-902 0c0-13 8-25 20-30l68-28a53 53 0 0 1 20-3c31 0 56 27 56 61a66 66 0 0 1-3 22 57 57 0 0 1-73 36l-68-28c-12-5-20-17-20-30zm132-319c10-9 24-12 35-7l68 28c29 13 41 47 26 77a66 66 0 0 1-13 18c-24 24-61 26-83 4a53 53 0 0 1-12-17l-28-68c-5-11-2-25 7-35zm638 638c-10 9-24 12-35 7l-68-28a53 53 0 0 1-17-12c-22-22-20-59 4-83a66 66 0 0 1 18-13c30-15 64-3 77 26l28 68c5 11 2 25-7 35zm0-638c9 10 12 24 7 35l-28 68a56 56 0 0 1-77 26 66 66 0 0 1-18-13c-24-24-26-61-4-83 5-5 11-9 17-12l68-28c11-5 25-2 35 7zM193 831c-9-10-12-24-7-35l28-68a53 53 0 0 1 12-17c22-22 59-20 83 4a66 66 0 0 1 13 18c15 30 3 64-26 77l-68 28c-11 5-25 2-35-7zm319-94a225 225 0 1 1 0-450 225 225 0 0 1 0 450z"></path></svg></button><a href="https://github.com/1kuzus/1kuzus.github.io" target="_blank" rel="noreferrer"><div class="header-button-bg"><svg xmlns="http://www.w3.org/2000/svg" width="36" height="36" viewBox="0 0 1024 1024"><path fill="#FCFCFC" d="M411 831c4-5 7-10 7-12v-70c-106 22-128-44-128-44-17-45-43-56-43-56-34-24 4-24 4-24 37 3 58 39 58 39 34 58 89 41 111 32 3-24 13-41 24-51-86-10-174-43-174-188 0-41 15-75 39-102-4-10-17-48 3-101 0 0 33-10 104 40 31-9 64-12 96-12s65 5 96 12c73-50 104-40 104-40 20 53 8 91 3 101 24 27 39 60 39 102 0 145-88 178-174 188 14 12 26 34 26 70v104c0 4 2 7 7 12 5 7 3 19-4 24-3 2-7 3-10 3H425c-10 0-17-6-17-17 0-5 2-8 3-10z"></path></svg></div></a></div></div><div id="contents"><h4 id="contents-header">本页目录</h4></div><div id="post-layout"><div id="main" class="center-wrapper"><h1 class="post-title">论文速记：深度学习</h1><div class="post-meta"><code class="not-loaded">0 views<!-- --> · </code><code>longtime</code></div><!--$--><h2 class="x-h1">研究</h2><h3 class="x-h2"><a href="https://arxiv.org/pdf/2003.08934.pdf" target="_blank" rel="noreferrer">【NeRF】NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis (2020)</a></h3><div class="x-highlightblock highlight-background-gray"><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">使用稀疏输入2D图片集实现场景的3D视图合成。</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><span class="x-inline-strong">文章的核心方法 / 具体是如何做的？</span></p><p class="x-p">从<code class="x-inline-highlight">(<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">ϕ</span></span></span></span>)</code>到<code class="x-inline-highlight">(<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">G</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span></span>)</code>。</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><span class="x-inline-strong">做了什么实验，效果怎么样？</span></p><p class="x-p">测试数据集是<code class="x-inline-highlight">Diffuse Synthetic 360</code>、<code class="x-inline-highlight">Realistic Synthetic 360</code>和<code class="x-inline-highlight">Real Forward-Facing</code>。</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><span class="x-inline-strong">研究的创新点</span></p><p class="x-p">将输入坐标位置编码，帮助MLP表示高频函数；分层采样</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><span class="x-inline-strong">有什么局限或可以改进的地方？</span></p><p class="x-p">有效地优化和渲染神经辐射场；可解释性</p></div></div></div><div class="x-highlightblock highlight-background-blue"><h4 class="x-h3">更多笔记</h4><p class="x-p">神经辐射场用于从2D的图片重建3D的场景。</p><p class="x-p">文中出现的三个指标PSNR、SSIM、LPIPS：</p><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p><span class="x-inline-strong">峰值信噪比</span><code class="x-inline-highlight">(Peak Signal to Noise Ratio, PSNR)</code>：用于衡量图像恢复的质量，数值越高表示图像质量越好。接近<code class="x-inline-highlight">50 dB</code>代表误差非常小，大于<code class="x-inline-highlight">30 dB</code>人眼难察觉差异。</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p><span class="x-inline-strong">结构相似性</span><code class="x-inline-highlight">(Structural Similarity Index Measure, SSIM)</code>：用于衡量图像的结构相似性，得分通常在<code class="x-inline-highlight">0</code>~<code class="x-inline-highlight">1</code>之间，数值越高表示图像结构越相似。相较于PSNR在图像质量的衡量上更能符合人眼对图像质量的判断。</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p><span class="x-inline-strong">基于学习的感知图像质量评价</span><code class="x-inline-highlight">(Learned Perceptual Image Patch Similarity, LPIPS)</code>：测量从预训练网络中提取的两个图像的特征之间的相似性，得分通常在<code class="x-inline-highlight">0</code>~<code class="x-inline-highlight">1</code>之间，数值越低表示感知质量越高。</p></div></div></div><h3 class="x-h2"><a href="https://arxiv.org/pdf/2201.05989.pdf" target="_blank" rel="noreferrer">【Instant NGP】Instant Neural Graphics Primitives with a Multiresolution Hash Encoding (2022)</a></h3><div class="x-highlightblock highlight-background-gray"><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">主要用于解决NeRF在对全连接神经网络进行参数化时的效率问题。</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><span class="x-inline-strong">研究的创新点</span></p><p class="x-p">提出了一种基于哈希搜索的编码方法。</p></div></div></div><div class="x-highlightblock highlight-background-blue"><h4 class="x-h3">更多笔记</h4><h4 class="x-h3">Instant NGP与NeRF的异同</h4><p class="x-p">转载自<a href="https://zhuanlan.zhihu.com/p/631284285" target="_blank" rel="noreferrer" class="x-inline-link">知乎：从NeRF到Instant-NGP</a></p><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p>同样基于体渲染</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p>不同于NeRF的MLP，Instant NGP使用稀疏的参数化的<code class="x-inline-highlight">voxel grid</code>作为场景表达</p></div></div></div><h3 class="x-h2"><a href="https://arxiv.org/pdf/2308.04079.pdf" target="_blank" rel="noreferrer">【3DGS】3D Gaussian Splatting for Real-Time Radiance Field Rendering (2023)</a></h3><div class="x-highlightblock highlight-background-gray"><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">实现实时辐射场渲染，同时保持高质量的视觉效果，并且保持较短的训练时间。</p></div></div></div><div class="x-highlightblock highlight-background-blue"><h4 class="x-h3">更多笔记</h4><h4 class="x-h3">文章的相关工作部分</h4><p class="x-p">传统的场景重建与渲染：基于光场的，密集采样、非结构化捕获；<span class="x-inline-strong">运动恢复结构</span><code class="x-inline-highlight">(Structure from Motion, SFM)</code>用一组照片估计稀疏点云合成新视图；<span class="x-inline-strong">多视点立体视觉</span><code class="x-inline-highlight">(Multi-View Stereo, MVS)</code>；<br/>神经渲染和辐射场：用CNN估计混合权重，用于纹理空间；Soft3D提出<code class="x-inline-highlight">Volumetric representations</code>；NeRF提出重要性采样和位置编码来提高质量，但使用了大型多层感知器，对速度有负面影响。</p><h4 class="x-h3">稀疏重建和稠密重建</h4><p class="x-p">稀疏重建主要用于定位，得到每张图片的相机参数，提取特征点，例如SFM；稠密重建是假设相机参数已知的情况下，从不同视角的图像中找到匹配的对应点，对整个图像或图像中绝大部分像素进行重建。</p></div><h3 class="x-h2"><a href="https://arxiv.org/pdf/2211.11646.pdf" target="_blank" rel="noreferrer">【NeRF RPN】NeRF-RPN: A general framework for object detection in NeRFs (2022)</a></h3><div class="x-highlightblock highlight-background-gray"><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">在NeRF中直接进行3D物体检测。</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><span class="x-inline-strong">文章的核心方法 / 具体是如何做的？</span></p><p class="x-p">第一部分：特征提取器<br/>从NeRF采样的辐射度和密度网格作为输入，生成特征金字塔作为输出。<br/>第二部分：RPN头<br/>对特征金字塔进行操作并生成对象建议。</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><span class="x-inline-strong">研究的创新点</span></p><p class="x-p">第一次将RPN引入NeRF以进行3D物体检测和相关任务；<br/>利用Hypersim和3D-FRONT数据集构建了第一个用于3D目标检测的NeRF数据集。</p></div></div></div><h3 class="x-h2"><a href="https://arxiv.org/pdf/2304.04395v3.pdf" target="_blank" rel="noreferrer">【Instance NeRF】Instance Neural Radiance Field (2023)</a></h3><div class="x-highlightblock highlight-background-gray"><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">输入一个以多视图RGB图像预训练的NeRF，学习给定场景的3D实例分割。</p><p class="x-p">文章的主要贡献：</p><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p>第一个在NeRF中进行3D实例分割的尝试之一，而没有使用真实分割标签作为输入；</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p>提出<code class="x-inline-highlight">Neural Instance Field</code>的结构和训练方法，可以产生<span class="x-inline-strong">多视图一致</span>的2D分割和连续的3D分割；</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p>对合成室内NeRF数据集进行实验和消融研究。</p></div></div></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><span class="x-inline-strong">文章的核心方法 / 具体是如何做的？</span></p><p class="x-p">Instance NeRF有两个组件：预训练的NeRF模型、和文中提出的<code class="x-inline-highlight">Instance Field</code>。</p><p class="x-p"><code class="x-inline-highlight">Instance Field</code>的训练过程如下：</p><img alt="img" loading="lazy" width="1458" height="500" decoding="async" data-nimg="1" class="x-image x-image-invert" style="color:transparent;width:96%;height:auto" src="/_next/static/media/instance_field.500af71e.jpg"/><p class="x-p">NeRF-RCNN用预训练的NeRF提取的辐射场和密度场，为每个检测到的对象输出3D掩码；Mask2Former生成从NeRF渲染的图像的二维全景分割图（跨视图的实例标签并不一定一致）。然后按照相机位置，投影3D掩码去匹配不同视图中的相同实例，去产生多视图一致的2D分割图。CascadePSP用于细化2D mask。</p></div></div></div><div class="x-highlightblock highlight-background-blue"><h4 class="x-h3">更多笔记</h4><h4 class="x-h3">包围体：AABB和OBB</h4><p class="x-p"><span class="x-inline-strong">AABB</span>：轴对齐包围盒<code class="x-inline-highlight">(Axis-Aligned Bounding Box)</code><br/><span class="x-inline-strong">OBB</span>：有向包围盒<code class="x-inline-highlight">(Oriented Bounding Box)</code></p><p class="x-p">下图展示了更多种类的包围体：</p><img alt="img" loading="lazy" width="850" height="323" decoding="async" data-nimg="1" class="x-image x-image-invert" style="color:transparent;width:100%;height:auto" src="/_next/static/media/bounding_volumes.e98c20aa.png"/></div><h3 class="x-h2"><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Cheng_CascadePSP_Toward_Class-Agnostic_and_Very_High-Resolution_Segmentation_via_Global_and_CVPR_2020_paper.pdf" target="_blank" rel="noreferrer">【CascadePSP】CascadePSP: Toward Class-Agnostic and Very High-Resolution Segmentation via Global and Local Refinement (2020)</a></h3><div class="x-highlightblock highlight-background-gray"><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">提出一种不使用高分辨率训练数据，解决高分辨率分割问题的方法。右图是改进后的结果：</p><img alt="img" loading="lazy" width="747" height="542" decoding="async" data-nimg="1" class="x-image x-image-invert" style="color:transparent;width:400px;height:auto" src="/_next/static/media/cascadepsp.0d9b618b.jpg"/></div></div></div><h3 class="x-h2"><a href="https://arxiv.org/pdf/2312.00860.pdf" target="_blank" rel="noreferrer">【SAGA】Segment Any 3D Gaussians (2023)</a></h3><div class="x-highlightblock highlight-background-gray"><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">交互式3D分割<code class="x-inline-highlight">(promptable segmentation)</code>。</p></div></div></div><h2 class="x-h1">学习</h2><h3 class="x-h2"><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf" target="_blank" rel="noreferrer">【R-CNN】Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation (2014)</a></h3><div class="x-highlightblock highlight-background-gray"><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">提出R-CNN<code class="x-inline-highlight">(Regions with CNN features)</code>提高目标检测性能。</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><span class="x-inline-strong">文章的核心方法 / 具体是如何做的？</span></p><p class="x-p">区域提议<code class="x-inline-highlight">(Region Proposals)</code>：使用<code class="x-inline-highlight">selective search</code>生成候选。</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><span class="x-inline-strong">做了什么实验，效果怎么样？</span></p><p class="x-p">在<code class="x-inline-highlight">PASCAL VOC 2012</code>取得<code class="x-inline-highlight">mAP 53.3%</code>，在<code class="x-inline-highlight">ILSVRC 2013</code>竞赛数据集取得<code class="x-inline-highlight">mAP 31.4%</code>。</p></div></div></div><div class="x-highlightblock highlight-background-blue"><h4 class="x-h3">更多笔记</h4><p class="x-p">转载自<a href="https://zh-v2.d2l.ai/chapter_computer-vision/rcnn.html" target="_blank" rel="noreferrer" class="x-inline-link">动手学深度学习 - 区域卷积神经网络系列</a></p><h4 class="x-h3">R-CNN</h4><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p>R-CNN使用启发式搜索算法<code class="x-inline-highlight">selective search</code>（之前人们通常也是这样做的）来选择锚框</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p>用预训练的模型，对每一个锚框抽取特征</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p>训练一个SVM来对类别分类</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p>训练一个线性回归模型来预测边缘框</p></div></div><p class="x-p">R-CNN的速度很慢，因为可能从一张图像中选出上千个提议区域，这需要上千次的卷积神经网络的前向传播来执行目标检测。这种庞大的计算量使得R-CNN在现实世界中难以被广泛应用。</p><img alt="img" loading="lazy" width="1563" height="467" decoding="async" data-nimg="1" class="x-image" style="color:transparent;width:600px;height:auto" src="/_next/static/media/rcnn.09a38e8a.jpg"/><h4 class="x-h3">Fast R-CNN</h4><p class="x-p">R-CNN的主要性能瓶颈在于，对每个提议区域，卷积神经网络的前向传播是独立的，而没有共享计算。由于这些区域通常有重叠，独立的特征抽取会导致重复的计算。Fast R-CNN的主要改进之一，是仅在整张图象上执行卷积神经网络的前向传播，并且引入<span class="x-inline-strong">兴趣区域池化</span><code class="x-inline-highlight">(ROI Pooling)</code>，将卷积神经网络的输出和提议区域作为输入，输出连结后的各个提议区域抽取的特征。</p><p class="x-p">兴趣区域池化层可以给出固定大小的输出：把给定的锚框均匀分割成<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">m</span></span></span></span>块，输出每块里的最大值，这样无论锚框多大，总是输出<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">nm</span></span></span></span>个值。</p><p class="x-p">Fast R-CNN先对图片用CNN抽取特征，然后将<code class="x-inline-highlight">selective search</code>给出的原图上的提议区域映射到CNN特征图上，再经过<code class="x-inline-highlight">ROI Pooling</code>就可以得到维度对齐的特征。</p><img alt="img" loading="lazy" width="917" height="838" decoding="async" data-nimg="1" class="x-image" style="color:transparent;width:400px;height:auto" src="/_next/static/media/fastrcnn.46850bc6.jpg"/><h4 class="x-h3">Faster R-CNN</h4><p class="x-p">与Fast R-CNN相比，Faster R-CNN将生成提议区域的方法从<code class="x-inline-highlight">selective search</code>改为了<span class="x-inline-strong">区域提议网络</span><code class="x-inline-highlight">(Region Proposal Network, RPN)</code>，模型的其余部分保持不变。区域提议网络作为Faster R-CNN模型的一部分，是和整个模型一起训练得到的。换句话说，Faster R-CNN的目标函数不仅包括目标检测中的类别和边界框预测，还包括区域提议网络中锚框的二元类别和边界框预测。作为端到端训练的结果，区域提议网络能够学习到如何生成高质量的提议区域，从而在减少了从数据中学习的提议区域的数量的情况下，仍保持目标检测的精度。</p><img alt="img" loading="lazy" width="1579" height="871" decoding="async" data-nimg="1" class="x-image" style="color:transparent;width:600px;height:auto" src="/_next/static/media/fasterrcnn.1f4436bb.jpg"/></div><h3 class="x-h2"><a href="https://arxiv.org/pdf/1703.06870.pdf" target="_blank" rel="noreferrer">【Mask R-CNN】Mask R-CNN (2017)</a></h3><div class="x-highlightblock highlight-background-gray"><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">目标检测&实例分割</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><span class="x-inline-strong">文章的核心方法 / 具体是如何做的？</span></p><p class="x-p">基于Faster R-CNN，添加一个对每个ROI预测分割mask的分支。这个分支可以与分类和边界框预测分支并行。</p><img alt="img" loading="lazy" width="676" height="342" decoding="async" data-nimg="1" class="x-image" style="color:transparent;width:600px;height:auto" src="/_next/static/media/maskrcnn1.6cdb9105.jpg"/></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><span class="x-inline-strong">做了什么实验，效果怎么样？</span></p><img alt="img" loading="lazy" width="763" height="184" decoding="async" data-nimg="1" class="x-image x-image-invert" style="color:transparent;width:600px;height:auto" src="/_next/static/media/maskrcnn2.fe26f764.jpg"/><p class="x-p">在COCO数据集上表现优异，超过了2015和2016年COCO分割任务的冠军。</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><span class="x-inline-strong">研究的创新点</span></p><p class="x-p"> <code class="x-inline-highlight">ROI Pooling</code>中发生了两次浮点数取整，第一次是将锚框均匀分割成<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">m</span></span></span></span>块时，第二次是把提议区域映射回CNN特征图时。<br/> 分割任务的精细程度更高，因此文章提出了<code class="x-inline-highlight">ROI Align</code>，使用双线性插值来保留特征图上的空间信息。 </p></div></div></div><div class="x-highlightblock highlight-background-blue"><h4 class="x-h3">更多笔记</h4><h4 class="x-h3">语义分割与实例分割</h4><img alt="img" loading="lazy" width="1243" height="370" decoding="async" data-nimg="1" class="x-image" style="color:transparent;width:600px;height:auto" src="/_next/static/media/ss_and_is.f6df7378.jpg"/><p class="x-p"><span class="x-inline-strong">语义分割</span><code class="x-inline-highlight">(semantic segmentation)</code>：为每一个像素分配一个类别，但不区分同一类别之间的对象。<br/><span class="x-inline-strong">实例分割</span><code class="x-inline-highlight">(instance segmentation)</code>：会区分属于同一类别的不同实例。</p></div><h3 class="x-h2"><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf" target="_blank" rel="noreferrer">【FCN】Fully Convolutional Networks for Semantic Segmentation (2015)</a></h3><div class="x-highlightblock highlight-background-gray"><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">使用全卷积网络进行语义分割。</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><span class="x-inline-strong">文章的核心方法 / 具体是如何做的？</span></p><p class="x-p"> 是将现有的分类网络（AlexNet、VGG Net、GoogLeNet）改造为全卷积网络，以便在语义分割任务上进行端到端（输入图像，输出分割掩码）的训练。<br/> 改造的方式是将最后的全连接层替换成卷积层。 </p></div></div></div><div class="x-highlightblock highlight-background-blue"><h4 class="x-h3">更多笔记</h4><h4 class="x-h3">转置卷积</h4><p class="x-p">卷积通常不会增大输入的高宽，而是保持不变或降低。由于语义分割任务需要像素级别的输出，转置卷积被用来增大输入的高宽。</p><img alt="img" loading="lazy" width="890" height="393" decoding="async" data-nimg="1" class="x-image x-image-invert" style="color:transparent;width:600px;height:auto" src="/_next/static/media/transpose_convolution.fb99cc55.jpg"/><p class="x-p">图片转载自<a href="https://zh-v2.d2l.ai/chapter_computer-vision/transposed-conv.html" target="_blank" rel="noreferrer" class="x-inline-link">动手学深度学习 - 转置卷积</a></p><h4 class="x-h3">FCN中的转置卷积</h4><p class="x-p">例如对于ImageNet的图片输入，大小通常为<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">224</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">224</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">3</span></span></span></span>（RGB通道）；经过卷积后缩小宽高缩小<code class="x-inline-highlight">32</code>倍，通道增加到<code class="x-inline-highlight">512</code>，变成<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">7</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">7</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">512</span></span></span></span>的特征图。此时FCN会先通过一个<code class="x-inline-highlight">1x1conv</code>进行通道降维，然后通过转置卷积将特征图的高度和宽度增加<code class="x-inline-highlight">32</code>倍，<span class="x-inline-strong">输出通道数等于类别数</span>，相当于储存了对每一类的预测结果。</p></div><h3 class="x-h2"><a href="https://arxiv.org/pdf/1911.11907" target="_blank" rel="noreferrer">【GhostNet】GhostNet: More Features from Cheap Operations (2020)</a></h3><div class="x-highlightblock highlight-background-gray"><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">在嵌入式设备上部署CNN通常由于内存和计算资源受限而困难，GhostNet提出了Ghost模块，用低成本的操作生成更多特征图，可以作为一种轻量化的深度卷积模型架构</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><span class="x-inline-strong">文章的核心方法 / 具体是如何做的？</span></p><p class="x-p">特征图的各层之间有很多是相似的（冗余信息），论文希望能通过更低成本的计算量去获取它们。</p><img alt="img" loading="lazy" width="647" height="592" decoding="async" data-nimg="1" class="x-image x-image-invert" style="color:transparent;width:400px;height:auto" src="/_next/static/media/ghost_module.04984732.jpg"/><p class="x-p">对于某个特征层，只用较少卷积操作生成部分真实的特征层（固有特征图），剩余的特征层通过对固有特征图的每个通道线性变换生成，最后将这些特征层拼接在一起。</p></div></div></div><!--/$--></div><div id="sidebar"><div id="sidebar-width-wrapper"><div class="category-card sidebar-card show-list"><div class="category-card-header"><div class="category-name">网络杂识<!-- --> (<!-- -->8<!-- -->)</div><button class="category-rightarrow"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 1024 1024"><path fill="#50505c" d="m761 532 2-3c9-18 6-40-10-55L400 140a48 48 0 0 0-66 70l317 299-316 305a48 48 0 0 0 67 69l350-338 1-2 2-1c3-3 4-7 6-10z"></path></svg></button></div><div class="category-card-ul-wrapper"><ul class="category-card-ul"><li><a class="link" href="/23d/database-3nf/"><span>数据库设计三大范式</span></a></li><li><a class="link" href="/23d/github-linguist-vendored/"><span>不统计Github仓库某个目录下的语言</span></a></li><li><a class="link" href="/24a/deepl-shortcut-setting/"><span>解决：DeepL该快捷键已被使用</span></a></li><li><a class="link" href="/24a/git-merge-allow-unrelated-histories/"><span>记录：使用--allow-unrelated-histories</span></a></li><li><a class="link" href="/24b/injective-surjective-bijective/"><span>单射、满射、双射</span></a></li><li><a class="link" href="/25a/tai-e-a2-additional-case/"><span>南京大学程序分析Lab2（常量传播）易错样例补充</span></a></li><li><a class="link" href="/25a/tai-e-a3-solution/"><span>南京大学程序分析Lab3（死代码消除）思路</span></a></li><li><a class="link" href="/25b/debug-x86_64-program-in-arm64-system/"><span>使用QEMU在ARM64系统上调试x86_64程序</span></a></li></ul></div></div><div class="category-card sidebar-card show-list"><div class="category-card-header"><div class="category-name">安全学习<!-- --> (<!-- -->14<!-- -->)</div><button class="category-rightarrow"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 1024 1024"><path fill="#50505c" d="m761 532 2-3c9-18 6-40-10-55L400 140a48 48 0 0 0-66 70l317 299-316 305a48 48 0 0 0 67 69l350-338 1-2 2-1c3-3 4-7 6-10z"></path></svg></button></div><div class="category-card-ul-wrapper"><ul class="category-card-ul"><li><a class="link" href="/longtime/wp-portswigger/"><span>PortSwigger Writeup</span></a></li><li><a class="link" href="/23d/hust-cas-login/"><span>Python登录华科统一身份认证接口</span></a></li><li><a class="link" href="/24b/learn-cwes/"><span>Learn CWEs &amp; Real-word Examples</span></a></li><li><a class="link" href="/24b/cross-site-scripting/"><span>Learn XSS</span></a></li><li><a class="link" href="/24b/cross-site-request-forgery/"><span>Learn CSRF</span></a></li><li><a class="link" href="/24d/wp-bluehens-2024/"><span>BlueHens CTF 2024 Writeup</span></a></li><li><a class="link" href="/24d/wp-1337uplive-2024/"><span>1337UP LIVE CTF 2024 Writeup</span></a></li><li><a class="link" href="/24d/wp-m0lecon-2025/"><span>m0leCon Beginner CTF 2025 Writeup</span></a></li><li><a class="link" href="/24d/wp-0xl4ugh-2024/"><span>0xl4ugh CTF 2024 Writeup</span></a></li><li><a class="link" href="/25a/wp-nullcon-hackim-2025/"><span>Nullcon HackIM CTF 2025 Writeup</span></a></li><li><a class="link" href="/25a/wp-picoctf-2025/"><span>picoCTF 2025 Writeup</span></a></li><li><a class="link" href="/25a/wp-cyber-apocalypse-ctf-2025/"><span>HTB Cyber Apocalypse CTF 2025 Writeup</span></a></li><li><a class="link" href="/25a/wp-swampctf-2025/"><span>SwampCTF 2025 Writeup</span></a></li><li><a class="link" href="/25b/wp-1753ctf-2025/"><span>1753CTF 2025 Writeup</span></a></li></ul></div></div><div class="category-card sidebar-card show-list"><div class="category-card-header"><div class="category-name">安全研究<!-- --> (<!-- -->10<!-- -->)</div><button class="category-rightarrow"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 1024 1024"><path fill="#50505c" d="m761 532 2-3c9-18 6-40-10-55L400 140a48 48 0 0 0-66 70l317 299-316 305a48 48 0 0 0 67 69l350-338 1-2 2-1c3-3 4-7 6-10z"></path></svg></button></div><div class="category-card-ul-wrapper"><ul class="category-card-ul"><li><a class="link" href="/longtime/papers-sec/"><span>论文速记：安全</span></a></li><li><a class="link" href="/24b/papers-sec-240514-taintmini/"><span>论文阅读：TAINTMINI: Detecting Flow of Sensitive Data in Mini-Programs with Static Taint Analysis (ICSE 2023)</span></a></li><li><a class="link" href="/24c/papers-sec-240728-2fa-consistency/"><span>论文阅读：A Systematic Study of the Consistency of Two-Factor Authentication User Journeys on Top-Ranked Websites (NDSS 2023)</span></a></li><li><a class="link" href="/24c/papers-sec-240810-conquer/"><span>论文阅读：Do Not Give A Dog Bread Every Time He Wags His Tail: Stealing Passwords through Content Queries (CONQUER) Attack (NDSS 2023)</span></a></li><li><a class="link" href="/24c/papers-sec-240817-2fa-failures/"><span>论文阅读：Security and Privacy Failures in Popular 2FA Apps (Security 2023)</span></a></li><li><a class="link" href="/24c/papers-sec-240824-zxcvbn/"><span>论文阅读：zxcvbn: Low-Budget Password Strength Estimation (Security 2016)</span></a></li><li><a class="link" href="/24c/papers-sec-240902-pafa-authentication/"><span>论文阅读：Maginot Line: Assessing a New Cross-app Threat to PII-as-Factor Authentication in Chinese Mobile Apps (NDSS 2024)</span></a></li><li><a class="link" href="/24c/papers-sec-240922-pre-hijacked-accounts/"><span>论文阅读：Pre-hijacked accounts: An Empirical Study of Security Failures in User Account Creation on the Web (Security 2022)</span></a></li><li><a class="link" href="/24d/papers-sec-241207-aimie/"><span>论文阅读：Understanding and Detecting Abused Image Hosting Modules as Malicious Services (CCS 2023)</span></a></li><li><a class="link" href="/25a/papers-sec-250225-minitracker/"><span>论文阅读：MiniTracker: Large-Scale Sensitive Information Tracking in Mini Apps (TDSC 2023)</span></a></li></ul></div></div><div class="category-card sidebar-card show-list"><div class="category-card-header"><div class="category-name">算法<!-- --> (<!-- -->26<!-- -->)</div><button class="category-rightarrow"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 1024 1024"><path fill="#50505c" d="m761 532 2-3c9-18 6-40-10-55L400 140a48 48 0 0 0-66 70l317 299-316 305a48 48 0 0 0 67 69l350-338 1-2 2-1c3-3 4-7 6-10z"></path></svg></button></div><div class="category-card-ul-wrapper"><ul class="category-card-ul"><li><a class="link" href="/24b/sorting-algorithm/"><span>排序算法总结与代码实现</span></a></li><li><a class="link" href="/24b/mst-and-sp/"><span>最小生成树与最短路算法</span></a></li><li><a class="link" href="/24a/csp-2016-04/"><span>CSP 201604 T1-T4题解</span></a></li><li><a class="link" href="/24a/csp-2018-03/"><span>CSP 201803 T1-T4题解</span></a></li><li><a class="link" href="/24a/csp-2020-06/"><span>CSP 202006 T1-T4题解</span></a></li><li><a class="link" href="/24a/csp-2020-09/"><span>CSP 202009 T1-T4题解</span></a></li><li><a class="link" href="/24a/csp-2020-12/"><span>CSP 202012 T1-T5题解</span></a></li><li><a class="link" href="/24a/csp-2022-06/"><span>CSP 202206 T1-T5题解</span></a></li><li><a class="link" href="/24a/csp-2023-05/"><span>CSP 202305 T1-T5题解</span></a></li><li><a class="link" href="/24a/csp-2023-09/"><span>CSP 202309 T1-T4题解</span></a></li><li><a class="link" href="/24b/leetcode-4/"><span>LeetCode 4.寻找两个正序数组的中位数</span></a></li><li><a class="link" href="/24b/leetcode-30/"><span>LeetCode 30.串联所有单词的子串</span></a></li><li><a class="link" href="/24b/leetcode-37/"><span>LeetCode 37.解数独</span></a></li><li><a class="link" href="/24c/leetcode-42/"><span>LeetCode 42.接雨水</span></a></li><li><a class="link" href="/24c/leetcode-51/"><span>LeetCode 51.N皇后</span></a></li><li><a class="link" href="/24c/leetcode-52/"><span>LeetCode 52.N皇后 II</span></a></li><li><a class="link" href="/24b/leetcode-60/"><span>LeetCode 60.排列序列</span></a></li><li><a class="link" href="/24b/leetcode-65/"><span>LeetCode 65.有效数字</span></a></li><li><a class="link" href="/24c/leetcode-68/"><span>LeetCode 68.文本左右对齐</span></a></li><li><a class="link" href="/24b/leetcode-84/"><span>LeetCode 84.柱状图中最大的矩形</span></a></li><li><a class="link" href="/24c/leetcode-85/"><span>LeetCode 85.最大矩形</span></a></li><li><a class="link" href="/24c/leetcode-149/"><span>LeetCode 149.直线上最多的点数</span></a></li><li><a class="link" href="/24d/leetcode-282/"><span>LeetCode 282.给表达式添加运算符</span></a></li><li><a class="link" href="/24b/leetcode-312/"><span>LeetCode 312.戳气球</span></a></li><li><a class="link" href="/24b/leetcode-1373/"><span>LeetCode 1373.二叉搜索子树的最大键值和</span></a></li><li><a class="link" href="/24b/leetcode-1739/"><span>LeetCode 1739.放置盒子</span></a></li></ul></div></div><div class="category-card sidebar-card show-list"><div class="category-card-header"><div class="category-name">深度学习<!-- --> (<!-- -->8<!-- -->)</div><button class="category-rightarrow"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 1024 1024"><path fill="#50505c" d="m761 532 2-3c9-18 6-40-10-55L400 140a48 48 0 0 0-66 70l317 299-316 305a48 48 0 0 0 67 69l350-338 1-2 2-1c3-3 4-7 6-10z"></path></svg></button></div><div class="category-card-ul-wrapper"><ul class="category-card-ul"><li><a class="link active" href="/longtime/papers-dl/"><span>论文速记：深度学习</span></a></li><li><a class="link" href="/23d/r2plus1d/"><span>行为识别R(2+1)D网络</span></a></li><li><a class="link" href="/23d/object-detection-map/"><span>目标检测评价指标mAP</span></a></li><li><a class="link" href="/23d/learn-rnn-lstm/"><span>学习RNN和LSTM</span></a></li><li><a class="link" href="/24a/reproduce-nerf-rpn/"><span>记录：复现NeRF-RPN代码</span></a></li><li><a class="link" href="/24b/yolov5-obb-nms-rotated/"><span>解决：nms_rotated报错&quot;THC/THC.h&quot;: No such file or directory</span></a></li><li><a class="link" href="/24c/winograd/"><span>Winograd算法</span></a></li><li><a class="link" href="/24c/model-optimization/"><span>模型优化概述：量化、压缩、轻量化</span></a></li></ul></div></div><div class="category-card sidebar-card show-list"><div class="category-card-header"><div class="category-name">编程语言<!-- --> (<!-- -->5<!-- -->)</div><button class="category-rightarrow"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 1024 1024"><path fill="#50505c" d="m761 532 2-3c9-18 6-40-10-55L400 140a48 48 0 0 0-66 70l317 299-316 305a48 48 0 0 0 67 69l350-338 1-2 2-1c3-3 4-7 6-10z"></path></svg></button></div><div class="category-card-ul-wrapper"><ul class="category-card-ul"><li><a class="link" href="/23c/js-array/"><span>JavaScript数组常用方法</span></a></li><li><a class="link" href="/24a/cpp-stl/"><span>C++中STL的基本使用</span></a></li><li><a class="link" href="/24a/torch-numpy-topk/"><span>在pytorch和numpy中取top-k值和索引</span></a></li><li><a class="link" href="/24a/object-oriented-programming-python/"><span>Python面向对象编程</span></a></li><li><a class="link" href="/24c/asm8086-snake/"><span>8086汇编贪吃蛇</span></a></li></ul></div></div><div class="category-card sidebar-card show-list"><div class="category-card-header"><div class="category-name">前端<!-- --> (<!-- -->2<!-- -->)</div><button class="category-rightarrow"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 1024 1024"><path fill="#50505c" d="m761 532 2-3c9-18 6-40-10-55L400 140a48 48 0 0 0-66 70l317 299-316 305a48 48 0 0 0 67 69l350-338 1-2 2-1c3-3 4-7 6-10z"></path></svg></button></div><div class="category-card-ul-wrapper"><ul class="category-card-ul"><li><a class="link" href="/23d/css-auto-height-transition/"><span>CSS实现auto高度的过渡动画</span></a></li><li><a class="link" href="/24d/android-send-explicit-intent-to-dynamically-registered-receiver/"><span>不能向动态注册的广播接收器发送显式Intent</span></a></li></ul></div></div><div class="category-card sidebar-card show-list"><div class="category-card-header"><div class="category-name">课程笔记<!-- --> (<!-- -->19<!-- -->)</div><button class="category-rightarrow"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 1024 1024"><path fill="#50505c" d="m761 532 2-3c9-18 6-40-10-55L400 140a48 48 0 0 0-66 70l317 299-316 305a48 48 0 0 0 67 69l350-338 1-2 2-1c3-3 4-7 6-10z"></path></svg></button></div><div class="category-card-ul-wrapper"><ul class="category-card-ul"><li><a class="link" href="/24b/rank-inequality/"><span>【线性代数】对秩不等式的理解</span></a></li><li><a class="link" href="/23c/pattern-recognition-1/"><span>【模式识别】统计决策方法</span></a></li><li><a class="link" href="/23c/pattern-recognition-2/"><span>【模式识别】参数估计</span></a></li><li><a class="link" href="/23c/pattern-recognition-3/"><span>【模式识别】非参数估计</span></a></li><li><a class="link" href="/23d/pattern-recognition-4/"><span>【模式识别】线性学习器与线性分类器</span></a></li><li><a class="link" href="/23d/protocols/"><span>【计算机网络】协议总结</span></a></li><li><a class="link" href="/24a/machine-learning-exercises/"><span>【机器学习】习题</span></a></li><li><a class="link" href="/24a/games101-01-transformation/"><span>【GAMES101】Transformation</span></a></li><li><a class="link" href="/24a/games101-02-rasterization/"><span>【GAMES101】Rasterization</span></a></li><li><a class="link" href="/24a/games101-03-shading/"><span>【GAMES101】Shading</span></a></li><li><a class="link" href="/24a/games101-04-geometry/"><span>【GAMES101】Geometry</span></a></li><li><a class="link" href="/24c/assembly-1/"><span>【汇编语言】访问寄存器和内存</span></a></li><li><a class="link" href="/24c/assembly-2/"><span>【汇编语言】汇编语言编程</span></a></li><li><a class="link" href="/24c/assembly-3/"><span>【汇编语言】内存寻址方式</span></a></li><li><a class="link" href="/24c/assembly-4/"><span>【汇编语言】流程转移与子程序</span></a></li><li><a class="link" href="/24c/assembly-5/"><span>【汇编语言】标志寄存器</span></a></li><li><a class="link" href="/24c/assembly-6/"><span>【汇编语言】中断及外部设备操作</span></a></li><li><a class="link" href="/24d/crypto-1/"><span>【密码学】流密码</span></a></li><li><a class="link" href="/24d/crypto-2/"><span>【密码学】块密码</span></a></li></ul></div></div><div class="category-card sidebar-card show-list"><div class="category-card-header"><div class="category-name">杂记<!-- --> (<!-- -->4<!-- -->)</div><button class="category-rightarrow"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 1024 1024"><path fill="#50505c" d="m761 532 2-3c9-18 6-40-10-55L400 140a48 48 0 0 0-66 70l317 299-316 305a48 48 0 0 0 67 69l350-338 1-2 2-1c3-3 4-7 6-10z"></path></svg></button></div><div class="category-card-ul-wrapper"><ul class="category-card-ul"><li><a class="link" href="/24b/by-questions/"><span>专业课复习</span></a></li><li><a class="link" href="/24d/about-interactive-multimedia/"><span>关于交互式多媒体</span></a></li><li><a class="link" href="/longtime/hundred-thousand/"><span>十万</span></a></li><li><a class="link" href="/longtime/linux-commands/"><span>三开</span></a></li></ul></div></div><div class="category-card sidebar-card show-list"><div class="category-card-header"><div class="category-name">交互式多媒体<!-- --> (<!-- -->3<!-- -->)</div><button class="category-rightarrow"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 1024 1024"><path fill="#50505c" d="m761 532 2-3c9-18 6-40-10-55L400 140a48 48 0 0 0-66 70l317 299-316 305a48 48 0 0 0 67 69l350-338 1-2 2-1c3-3 4-7 6-10z"></path></svg></button></div><div class="category-card-ul-wrapper"><ul class="category-card-ul"><li><a class="link" href="/24d/about-interactive-multimedia/"><span>关于交互式多媒体</span></a></li><li><a class="link" href="/23c/pattern-recognition-1/"><span>【模式识别】统计决策方法</span></a></li><li><a class="link" href="/23d/css-auto-height-transition/"><span>CSS实现auto高度的过渡动画</span></a></li></ul></div></div></div></div><div id="sidebar-mask"></div></div><script src="/_next/static/chunks/webpack-f7800ef344ebad69.js" crossorigin="" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/css/268aef745a7b3740.css\",\"style\",{\"crossOrigin\":\"\"}]\n0:\"$L2\"\n"])</script><script>self.__next_f.push([1,"3:HL[\"/_next/static/css/20451da2c8ddd48c.css\",\"style\",{\"crossOrigin\":\"\"}]\n"])</script><script>self.__next_f.push([1,"4:I[47690,[],\"\"]\n6:I[55329,[\"481\",\"static/chunks/457b8330-b5c9cf3fcf214847.js\",\"954\",\"static/chunks/d3ac728e-0c798b3b8aa3bf53.js\",\"250\",\"static/chunks/250-0ef8476c0fa8ee24.js\",\"612\",\"static/chunks/612-fa632c1349770315.js\",\"551\",\"static/chunks/551-19232c47cd1e883a.js\",\"877\",\"static/chunks/app/%5B...slug%5D/page-954d1692838af271.js\"],\"\"]\n7:I[86510,[\"481\",\"static/chunks/457b8330-b5c9cf3fcf214847.js\",\"954\",\"static/chunks/d3ac728e-0c798b3b8aa3bf53.js\",\"250\",\"static/chunks/250-0ef8476c0fa8ee24.js\",\"612\",\"static/chunks/612-fa632c1349770315.js\",\"551\",\"static/chunks/551-19232c47cd1e883a.js\",\"877\",\"static/chunks/app/%5B...slug%5D/page-954d1692838af271.js\"],\"PostMeta\"]\n8:\"$Sreact.suspense\"\na:I[30389,[\"481\",\"static/chunks/457b8330-b5c9cf3fcf214847.js\",\"954\",\"static/chunks/d3ac728e-0c798b3b8aa3bf53.js\",\"250\",\"static/chunks/250-0ef8476c0fa8ee24.js\",\"612\",\"static/chunks/612-fa632c1349770315.js\",\"551\",\"static/chunks/551-19232c47cd1e883a.js\",\"877\",\"static/chunks/app/%5B...slug%5D/page-954d1692838af271.js\"],\"\"]\nb:I[5613,[],\"\"]\nd:I[31778,[],\"\"]\ne:I[25694,[\"250\",\"static/chunks/250-0ef8476c0fa8ee24.js\",\"185\",\"static/chunks/app/layout-29c171f9500c7269.js\"],\"GlobalProvider\"]\nf:I[30397,[\"250\",\"static/chunks/250-0ef8476c0fa8ee24.js\",\"185\",\"static/chunks/app/layout-29c171f9500c7269.js\"],\"\"]\n11:I[48955,[],\"\"]\nc:[\"slug\",\"longtime/papers-dl\",\"c\"]\n12:[]\n"])</script><script>self.__next_f.push([1,"2:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/268aef745a7b3740.css\",\"precedence\":\"next\",\"crossOrigin\":\"\"}]],[\"$\",\"$L4\",null,{\"buildId\":\"VpKPZSUP5KDfGVBPJAJka\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/longtime/papers-dl/\",\"initialTree\":[\"\",{\"children\":[[\"slug\",\"longtime/papers-dl\",\"c\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":[\\\"longtime\\\",\\\"papers-dl\\\"]}\",{}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[[\"slug\",\"longtime/papers-dl\",\"c\"],{\"children\":[\"__PAGE__\",{},[\"$L5\",[[\"$\",\"$L6\",null,{}],[\"$\",\"div\",null,{\"id\":\"post-layout\",\"children\":[false,[\"$\",\"div\",null,{\"id\":\"main\",\"className\":\"center-wrapper\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"post-title\",\"children\":\"论文速记：深度学习\"}],[\"$\",\"$L7\",null,{\"path\":\"/longtime/papers-dl/\"}],[\"$\",\"$8\",null,{\"fallback\":[\"$\",\"p\",null,{\"children\":\"Loading component...\"}],\"children\":\"$L9\"}]]}],[\"$\",\"$La\",null,{}]]}]],null]]},[\"$\",\"$Lb\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"$c\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Ld\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/20451da2c8ddd48c.css\",\"precedence\":\"next\",\"crossOrigin\":\"\"}]]}]]},[null,[\"$\",\"html\",null,{\"lang\":\"zh-CN\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"link\",null,{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\"}],[\"$\",\"script\",null,{\"async\":true,\"src\":\"https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js\"}],[\"$\",\"script\",null,{\"async\":true,\"src\":\"https://www.googletagmanager.com/gtag/js?id=G-45BYSZ6WPY\"}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\nwindow.dataLayer = window.dataLayer || [];\\nfunction gtag() {\\n    dataLayer.push(arguments);\\n}\\ngtag('js', new Date());\\ngtag('config', 'G-45BYSZ6WPY');\\n\"}}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"const a=z=\u003eh.getItem(z),b=(y,z)=\u003eh.setItem(y,z),c=(y,z)=\u003edocument.documentElement.setAttribute(y,z),d='theme',e='dark',f='light',g='class',h=localStorage;a(d)!==e\u0026\u0026a(d)!==f\u0026\u0026b(d,f);a(d)===e?c(g,e):c(g,f);\"}}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\nif (!Array.prototype.findLast) {\\n    Array.prototype.findLast = function (callback) {\\n        for (let i = this.length - 1; i \u003e= 0; i--) {\\n            if (callback(this[i])) return this[i];\\n        }\\n        return undefined;\\n    };\\n}\\nif (!Array.prototype.findLastIndex) {\\n    Array.prototype.findLastIndex = function (callback) {\\n        for (let i = this.length - 1; i \u003e= 0; i--) {\\n            if (callback(this[i])) return i;\\n        }\\n        return -1;\\n    };\\n}\\n\"}}]]}],[\"$\",\"body\",null,{\"children\":[\"$\",\"$Le\",null,{\"children\":[[\"$\",\"$Lf\",null,{}],[\"$\",\"$Lb\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Ld\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$\",\"div\",null,{\"id\":\"notfound\",\"children\":[[\"$\",\"img\",null,{\"alt\":\"img\",\"src\":\"/images/cry.gif\"}],[\"$\",\"code\",null,{\"id\":\"notfound-404\",\"children\":\"404\"}],[\"$\",\"code\",null,{\"id\":\"notfound-text\",\"children\":\"Page Not Found\"}]]}],\"notFoundStyles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/98e601cf5ba3633d.css\",\"precedence\":\"next\",\"crossOrigin\":\"\"}]],\"styles\":null}]]}]}]]}],null]],\"initialHead\":[false,\"$L10\"],\"globalErrorComponent\":\"$11\",\"missingSlots\":\"$W12\"}]]\n"])</script><script>self.__next_f.push([1,"14:I[74365,[\"481\",\"static/chunks/457b8330-b5c9cf3fcf214847.js\",\"954\",\"static/chunks/d3ac728e-0c798b3b8aa3bf53.js\",\"250\",\"static/chunks/250-0ef8476c0fa8ee24.js\",\"612\",\"static/chunks/612-fa632c1349770315.js\",\"551\",\"static/chunks/551-19232c47cd1e883a.js\",\"877\",\"static/chunks/app/%5B...slug%5D/page-954d1692838af271.js\"],\"\"]\n13:T648,从\u003ccode class=\"x-inline-highlight\"\u003e(\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ex\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003ey\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.04398em;\"\u003ez\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\"\u003eθ\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eϕ\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e)\u003c/code\u003e到\u003ccode class=\"x-inline-highlight\"\u003e(\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8778em;vertical-align:-0.1944em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.00773em;\"\u003eR\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eG\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.05017em;\"\u003eB\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003eσ\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e)\u003c/code\u003e。15:T7ef,例如对于ImageNet的图片输入，大小通常为\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-html\" aria"])</script><script>self.__next_f.push([1,"-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e224\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e×\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e224\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e×\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6444em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e3\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e（RGB通道）；经过卷积后缩小宽高缩小\u003ccode class=\"x-inline-highlight\"\u003e32\u003c/code\u003e倍，通道增加到\u003ccode class=\"x-inline-highlight\"\u003e512\u003c/code\u003e，变成\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e7\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e×\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.7278em;vertical-align:-0.0833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e7\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e×\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6444em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e512\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e的特征图。此时FCN会先通过一个\u003ccode class=\"x-inline-highlight\"\u003e1x1conv\u003c/code\u003e进行通道降维，然后通过转置卷积将特征图的高度和宽度增加\u003ccode class=\"x-inline-highlight\"\u003e32\u003c/code\u003e倍，\u003cspan class=\"x-inline-strong\"\u003e输出通道数等于类别数\u003c/span\u003e，相当于储存了对每一类的预测结果。"])</script><script>self.__next_f.push([1,"9:[[\"$\",\"h2\",null,{\"className\":\"x-h1\",\"children\":\"研究\"}],[\"$\",\"h3\",null,{\"className\":\"x-h2\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/pdf/2003.08934.pdf\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":\"【NeRF】NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis (2020)\"}]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-gray\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的主题 / 文章要解决什么问题？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"使用稀疏输入2D图片集实现场景的3D视图合成。\"}}]]}]]}],[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的核心方法 / 具体是如何做的？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"$13\"}}]]}]]}],[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e做了什么实验，效果怎么样？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"测试数据集是\u003ccode class=\\\"x-inline-highlight\\\"\u003eDiffuse Synthetic 360\u003c/code\u003e、\u003ccode class=\\\"x-inline-highlight\\\"\u003eRealistic Synthetic 360\u003c/code\u003e和\u003ccode class=\\\"x-inline-highlight\\\"\u003eReal Forward-Facing\u003c/code\u003e。\"}}]]}]]}],[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e研究的创新点\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"将输入坐标位置编码，帮助MLP表示高频函数；分层采样\"}}]]}]]}],[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e有什么局限或可以改进的地方？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"有效地优化和渲染神经辐射场；可解释性\"}}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-blue\",\"children\":[[\"$\",\"h4\",null,{\"className\":\"x-h3\",\"children\":\"更多笔记\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"神经辐射场用于从2D的图片重建3D的场景。\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"文中出现的三个指标PSNR、SSIM、LPIPS：\"}}],[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[\"$\",\"p\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e峰值信噪比\u003c/span\u003e\u003ccode class=\\\"x-inline-highlight\\\"\u003e(Peak Signal to Noise Ratio, PSNR)\u003c/code\u003e：用于衡量图像恢复的质量，数值越高表示图像质量越好。接近\u003ccode class=\\\"x-inline-highlight\\\"\u003e50 dB\u003c/code\u003e代表误差非常小，大于\u003ccode class=\\\"x-inline-highlight\\\"\u003e30 dB\u003c/code\u003e人眼难察觉差异。\"}}]}]]}],[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[\"$\",\"p\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e结构相似性\u003c/span\u003e\u003ccode class=\\\"x-inline-highlight\\\"\u003e(Structural Similarity Index Measure, SSIM)\u003c/code\u003e：用于衡量图像的结构相似性，得分通常在\u003ccode class=\\\"x-inline-highlight\\\"\u003e0\u003c/code\u003e~\u003ccode class=\\\"x-inline-highlight\\\"\u003e1\u003c/code\u003e之间，数值越高表示图像结构越相似。相较于PSNR在图像质量的衡量上更能符合人眼对图像质量的判断。\"}}]}]]}],[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[\"$\",\"p\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e基于学习的感知图像质量评价\u003c/span\u003e\u003ccode class=\\\"x-inline-highlight\\\"\u003e(Learned Perceptual Image Patch Similarity, LPIPS)\u003c/code\u003e：测量从预训练网络中提取的两个图像的特征之间的相似性，得分通常在\u003ccode class=\\\"x-inline-highlight\\\"\u003e0\u003c/code\u003e~\u003ccode class=\\\"x-inline-highlight\\\"\u003e1\u003c/code\u003e之间，数值越低表示感知质量越高。\"}}]}]]}]]}],[\"$\",\"h3\",null,{\"className\":\"x-h2\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/pdf/2201.05989.pdf\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":\"【Instant NGP】Instant Neural Graphics Primitives with a Multiresolution Hash Encoding (2022)\"}]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-gray\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的主题 / 文章要解决什么问题？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"主要用于解决NeRF在对全连接神经网络进行参数化时的效率问题。\"}}]]}]]}],\"$undefined\",\"$undefined\",[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e研究的创新点\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"提出了一种基于哈希搜索的编码方法。\"}}]]}]]}],\"$undefined\"]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-blue\",\"children\":[[\"$\",\"h4\",null,{\"className\":\"x-h3\",\"children\":\"更多笔记\"}],[\"$\",\"h4\",null,{\"className\":\"x-h3\",\"children\":\"Instant NGP与NeRF的异同\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"转载自\u003ca href=\\\"https://zhuanlan.zhihu.com/p/631284285\\\" target=\\\"_blank\\\" rel=\\\"noreferrer\\\" class=\\\"x-inline-link\\\"\u003e知乎：从NeRF到Instant-NGP\u003c/a\u003e\"}}],[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[\"$\",\"p\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"同样基于体渲染\"}}]}]]}],[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[\"$\",\"p\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"不同于NeRF的MLP，Instant NGP使用稀疏的参数化的\u003ccode class=\\\"x-inline-highlight\\\"\u003evoxel grid\u003c/code\u003e作为场景表达\"}}]}]]}]]}],[\"$\",\"h3\",null,{\"className\":\"x-h2\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/pdf/2308.04079.pdf\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":\"【3DGS】3D Gaussian Splatting for Real-Time Radiance Field Rendering (2023)\"}]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-gray\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的主题 / 文章要解决什么问题？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"实现实时辐射场渲染，同时保持高质量的视觉效果，并且保持较短的训练时间。\"}}]]}]]}],\"$undefined\",\"$undefined\",\"$undefined\",\"$undefined\"]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-blue\",\"children\":[[\"$\",\"h4\",null,{\"className\":\"x-h3\",\"children\":\"更多笔记\"}],[\"$\",\"h4\",null,{\"className\":\"x-h3\",\"children\":\"文章的相关工作部分\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"传统的场景重建与渲染：基于光场的，密集采样、非结构化捕获；\u003cspan class=\\\"x-inline-strong\\\"\u003e运动恢复结构\u003c/span\u003e\u003ccode class=\\\"x-inline-highlight\\\"\u003e(Structure from Motion, SFM)\u003c/code\u003e用一组照片估计稀疏点云合成新视图；\u003cspan class=\\\"x-inline-strong\\\"\u003e多视点立体视觉\u003c/span\u003e\u003ccode class=\\\"x-inline-highlight\\\"\u003e(Multi-View Stereo, MVS)\u003c/code\u003e；\u003cbr/\u003e神经渲染和辐射场：用CNN估计混合权重，用于纹理空间；Soft3D提出\u003ccode class=\\\"x-inline-highlight\\\"\u003eVolumetric representations\u003c/code\u003e；NeRF提出重要性采样和位置编码来提高质量，但使用了大型多层感知器，对速度有负面影响。\"}}],[\"$\",\"h4\",null,{\"className\":\"x-h3\",\"children\":\"稀疏重建和稠密重建\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"稀疏重建主要用于定位，得到每张图片的相机参数，提取特征点，例如SFM；稠密重建是假设相机参数已知的情况下，从不同视角的图像中找到匹配的对应点，对整个图像或图像中绝大部分像素进行重建。\"}}]]}],[\"$\",\"h3\",null,{\"className\":\"x-h2\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/pdf/2211.11646.pdf\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":\"【NeRF RPN】NeRF-RPN: A general framework for object detection in NeRFs (2022)\"}]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-gray\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的主题 / 文章要解决什么问题？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"在NeRF中直接进行3D物体检测。\"}}]]}]]}],[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的核心方法 / 具体是如何做的？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"第一部分：特征提取器\u003cbr/\u003e从NeRF采样的辐射度和密度网格作为输入，生成特征金字塔作为输出。\u003cbr/\u003e第二部分：RPN头\u003cbr/\u003e对特征金字塔进行操作并生成对象建议。\"}}]]}]]}],\"$undefined\",[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e研究的创新点\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"第一次将RPN引入NeRF以进行3D物体检测和相关任务；\u003cbr/\u003e利用Hypersim和3D-FRONT数据集构建了第一个用于3D目标检测的NeRF数据集。\"}}]]}]]}],\"$undefined\"]}],[\"$\",\"h3\",null,{\"className\":\"x-h2\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/pdf/2304.04395v3.pdf\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":\"【Instance NeRF】Instance Neural Radiance Field (2023)\"}]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-gray\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的主题 / 文章要解决什么问题？\u003c/span\u003e\"}}],[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"输入一个以多视图RGB图像预训练的NeRF，学习给定场景的3D实例分割。\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"文章的主要贡献：\"}}],[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[\"$\",\"p\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"第一个在NeRF中进行3D实例分割的尝试之一，而没有使用真实分割标签作为输入；\"}}]}]]}],[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[\"$\",\"p\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"提出\u003ccode class=\\\"x-inline-highlight\\\"\u003eNeural Instance Field\u003c/code\u003e的结构和训练方法，可以产生\u003cspan class=\\\"x-inline-strong\\\"\u003e多视图一致\u003c/span\u003e的2D分割和连续的3D分割；\"}}]}]]}],[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[\"$\",\"p\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"对合成室内NeRF数据集进行实验和消融研究。\"}}]}]]}]]]}]]}],[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的核心方法 / 具体是如何做的？\u003c/span\u003e\"}}],[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"Instance NeRF有两个组件：预训练的NeRF模型、和文中提出的\u003ccode class=\\\"x-inline-highlight\\\"\u003eInstance Field\u003c/code\u003e。\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003ccode class=\\\"x-inline-highlight\\\"\u003eInstance Field\u003c/code\u003e的训练过程如下：\"}}],[\"$\",\"$L14\",null,{\"src\":\"instance_field.jpg\",\"width\":\"96%\",\"filterDarkTheme\":true}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"NeRF-RCNN用预训练的NeRF提取的辐射场和密度场，为每个检测到的对象输出3D掩码；Mask2Former生成从NeRF渲染的图像的二维全景分割图（跨视图的实例标签并不一定一致）。然后按照相机位置，投影3D掩码去匹配不同视图中的相同实例，去产生多视图一致的2D分割图。CascadePSP用于细化2D mask。\"}}]]]}]]}],\"$undefined\",\"$undefined\",\"$undefined\"]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-blue\",\"children\":[[\"$\",\"h4\",null,{\"className\":\"x-h3\",\"children\":\"更多笔记\"}],[\"$\",\"h4\",null,{\"className\":\"x-h3\",\"children\":\"包围体：AABB和OBB\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003eAABB\u003c/span\u003e：轴对齐包围盒\u003ccode class=\\\"x-inline-highlight\\\"\u003e(Axis-Aligned Bounding Box)\u003c/code\u003e\u003cbr/\u003e\u003cspan class=\\\"x-inline-strong\\\"\u003eOBB\u003c/span\u003e：有向包围盒\u003ccode class=\\\"x-inline-highlight\\\"\u003e(Oriented Bounding Box)\u003c/code\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"下图展示了更多种类的包围体：\"}}],[\"$\",\"$L14\",null,{\"src\":\"bounding_volumes.png\",\"width\":\"100%\",\"filterDarkTheme\":true}]]}],[\"$\",\"h3\",null,{\"className\":\"x-h2\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://openaccess.thecvf.com/content_CVPR_2020/papers/Cheng_CascadePSP_Toward_Class-Agnostic_and_Very_High-Resolution_Segmentation_via_Global_and_CVPR_2020_paper.pdf\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":\"【CascadePSP】CascadePSP: Toward Class-Agnostic and Very High-Resolution Segmentation via Global and Local Refinement (2020)\"}]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-gray\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的主题 / 文章要解决什么问题？\u003c/span\u003e\"}}],[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"提出一种不使用高分辨率训练数据，解决高分辨率分割问题的方法。右图是改进后的结果：\"}}],[\"$\",\"$L14\",null,{\"src\":\"cascadepsp.jpg\",\"width\":\"400px\",\"filterDarkTheme\":true}]]]}]]}],\"$undefined\",\"$undefined\",\"$undefined\",\"$undefined\"]}],[\"$\",\"h3\",null,{\"className\":\"x-h2\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/pdf/2312.00860.pdf\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":\"【SAGA】Segment Any 3D Gaussians (2023)\"}]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-gray\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的主题 / 文章要解决什么问题？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"交互式3D分割\u003ccode class=\\\"x-inline-highlight\\\"\u003e(promptable segmentation)\u003c/code\u003e。\"}}]]}]]}],\"$undefined\",\"$undefined\",\"$undefined\",\"$undefined\"]}],[\"$\",\"h2\",null,{\"className\":\"x-h1\",\"children\":\"学习\"}],[\"$\",\"h3\",null,{\"className\":\"x-h2\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":\"【R-CNN】Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation (2014)\"}]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-gray\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的主题 / 文章要解决什么问题？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"提出R-CNN\u003ccode class=\\\"x-inline-highlight\\\"\u003e(Regions with CNN features)\u003c/code\u003e提高目标检测性能。\"}}]]}]]}],[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的核心方法 / 具体是如何做的？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"区域提议\u003ccode class=\\\"x-inline-highlight\\\"\u003e(Region Proposals)\u003c/code\u003e：使用\u003ccode class=\\\"x-inline-highlight\\\"\u003eselective search\u003c/code\u003e生成候选。\"}}]]}]]}],[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e做了什么实验，效果怎么样？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"在\u003ccode class=\\\"x-inline-highlight\\\"\u003ePASCAL VOC 2012\u003c/code\u003e取得\u003ccode class=\\\"x-inline-highlight\\\"\u003emAP 53.3%\u003c/code\u003e，在\u003ccode class=\\\"x-inline-highlight\\\"\u003eILSVRC 2013\u003c/code\u003e竞赛数据集取得\u003ccode class=\\\"x-inline-highlight\\\"\u003emAP 31.4%\u003c/code\u003e。\"}}]]}]]}],\"$undefined\",\"$undefined\"]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-blue\",\"children\":[[\"$\",\"h4\",null,{\"className\":\"x-h3\",\"children\":\"更多笔记\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"转载自\u003ca href=\\\"https://zh-v2.d2l.ai/chapter_computer-vision/rcnn.html\\\" target=\\\"_blank\\\" rel=\\\"noreferrer\\\" class=\\\"x-inline-link\\\"\u003e动手学深度学习 - 区域卷积神经网络系列\u003c/a\u003e\"}}],[\"$\",\"h4\",null,{\"className\":\"x-h3\",\"children\":\"R-CNN\"}],[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[\"$\",\"p\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"R-CNN使用启发式搜索算法\u003ccode class=\\\"x-inline-highlight\\\"\u003eselective search\u003c/code\u003e（之前人们通常也是这样做的）来选择锚框\"}}]}]]}],[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[\"$\",\"p\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"用预训练的模型，对每一个锚框抽取特征\"}}]}]]}],[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[\"$\",\"p\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"训练一个SVM来对类别分类\"}}]}]]}],[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[\"$\",\"p\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"训练一个线性回归模型来预测边缘框\"}}]}]]}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"R-CNN的速度很慢，因为可能从一张图像中选出上千个提议区域，这需要上千次的卷积神经网络的前向传播来执行目标检测。这种庞大的计算量使得R-CNN在现实世界中难以被广泛应用。\"}}],[\"$\",\"$L14\",null,{\"src\":\"rcnn.jpg\",\"width\":\"600px\"}],[\"$\",\"h4\",null,{\"className\":\"x-h3\",\"children\":\"Fast R-CNN\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"R-CNN的主要性能瓶颈在于，对每个提议区域，卷积神经网络的前向传播是独立的，而没有共享计算。由于这些区域通常有重叠，独立的特征抽取会导致重复的计算。Fast R-CNN的主要改进之一，是仅在整张图象上执行卷积神经网络的前向传播，并且引入\u003cspan class=\\\"x-inline-strong\\\"\u003e兴趣区域池化\u003c/span\u003e\u003ccode class=\\\"x-inline-highlight\\\"\u003e(ROI Pooling)\u003c/code\u003e，将卷积神经网络的输出和提议区域作为输入，输出连结后的各个提议区域抽取的特征。\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"兴趣区域池化层可以给出固定大小的输出：把给定的锚框均匀分割成\u003cspan class=\\\"katex\\\"\u003e\u003cspan class=\\\"katex-html\\\" aria-hidden=\\\"true\\\"\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:0.6667em;vertical-align:-0.0833em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003en\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2222em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mbin\\\"\u003e×\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2222em;\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:0.4306em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003em\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e块，输出每块里的最大值，这样无论锚框多大，总是输出\u003cspan class=\\\"katex\\\"\u003e\u003cspan class=\\\"katex-html\\\" aria-hidden=\\\"true\\\"\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:0.4306em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003enm\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e个值。\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"Fast R-CNN先对图片用CNN抽取特征，然后将\u003ccode class=\\\"x-inline-highlight\\\"\u003eselective search\u003c/code\u003e给出的原图上的提议区域映射到CNN特征图上，再经过\u003ccode class=\\\"x-inline-highlight\\\"\u003eROI Pooling\u003c/code\u003e就可以得到维度对齐的特征。\"}}],[\"$\",\"$L14\",null,{\"src\":\"fastrcnn.jpg\",\"width\":\"400px\"}],[\"$\",\"h4\",null,{\"className\":\"x-h3\",\"children\":\"Faster R-CNN\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"与Fast R-CNN相比，Faster R-CNN将生成提议区域的方法从\u003ccode class=\\\"x-inline-highlight\\\"\u003eselective search\u003c/code\u003e改为了\u003cspan class=\\\"x-inline-strong\\\"\u003e区域提议网络\u003c/span\u003e\u003ccode class=\\\"x-inline-highlight\\\"\u003e(Region Proposal Network, RPN)\u003c/code\u003e，模型的其余部分保持不变。区域提议网络作为Faster R-CNN模型的一部分，是和整个模型一起训练得到的。换句话说，Faster R-CNN的目标函数不仅包括目标检测中的类别和边界框预测，还包括区域提议网络中锚框的二元类别和边界框预测。作为端到端训练的结果，区域提议网络能够学习到如何生成高质量的提议区域，从而在减少了从数据中学习的提议区域的数量的情况下，仍保持目标检测的精度。\"}}],[\"$\",\"$L14\",null,{\"src\":\"fasterrcnn.jpg\",\"width\":\"600px\"}]]}],[\"$\",\"h3\",null,{\"className\":\"x-h2\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/pdf/1703.06870.pdf\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":\"【Mask R-CNN】Mask R-CNN (2017)\"}]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-gray\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的主题 / 文章要解决什么问题？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"目标检测\u0026实例分割\"}}]]}]]}],[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的核心方法 / 具体是如何做的？\u003c/span\u003e\"}}],[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"基于Faster R-CNN，添加一个对每个ROI预测分割mask的分支。这个分支可以与分类和边界框预测分支并行。\"}}],[\"$\",\"$L14\",null,{\"src\":\"maskrcnn1.jpg\",\"width\":\"600px\"}]]]}]]}],[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e做了什么实验，效果怎么样？\u003c/span\u003e\"}}],[[\"$\",\"$L14\",null,{\"src\":\"maskrcnn2.jpg\",\"width\":\"600px\",\"filterDarkTheme\":true}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"在COCO数据集上表现优异，超过了2015和2016年COCO分割任务的冠军。\"}}]]]}]]}],[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e研究的创新点\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\" \u003ccode class=\\\"x-inline-highlight\\\"\u003eROI Pooling\u003c/code\u003e中发生了两次浮点数取整，第一次是将锚框均匀分割成\u003cspan class=\\\"katex\\\"\u003e\u003cspan class=\\\"katex-html\\\" aria-hidden=\\\"true\\\"\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:0.6667em;vertical-align:-0.0833em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003en\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2222em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mbin\\\"\u003e×\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.2222em;\\\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:0.4306em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003em\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e块时，第二次是把提议区域映射回CNN特征图时。\u003cbr/\u003e 分割任务的精细程度更高，因此文章提出了\u003ccode class=\\\"x-inline-highlight\\\"\u003eROI Align\u003c/code\u003e，使用双线性插值来保留特征图上的空间信息。 \"}}]]}]]}],\"$undefined\"]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-blue\",\"children\":[[\"$\",\"h4\",null,{\"className\":\"x-h3\",\"children\":\"更多笔记\"}],[\"$\",\"h4\",null,{\"className\":\"x-h3\",\"children\":\"语义分割与实例分割\"}],[\"$\",\"$L14\",null,{\"src\":\"ss_and_is.jpg\",\"width\":\"600px\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e语义分割\u003c/span\u003e\u003ccode class=\\\"x-inline-highlight\\\"\u003e(semantic segmentation)\u003c/code\u003e：为每一个像素分配一个类别，但不区分同一类别之间的对象。\u003cbr/\u003e\u003cspan class=\\\"x-inline-strong\\\"\u003e实例分割\u003c/span\u003e\u003ccode class=\\\"x-inline-highlight\\\"\u003e(instance segmentation)\u003c/code\u003e：会区分属于同一类别的不同实例。\"}}]]}],[\"$\",\"h3\",null,{\"className\":\"x-h2\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":\"【FCN】Fully Convolutional Networks for Semantic Segmentation (2015)\"}]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-gray\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的主题 / 文章要解决什么问题？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"使用全卷积网络进行语义分割。\"}}]]}]]}],[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的核心方法 / 具体是如何做的？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\" 是将现有的分类网络（AlexNet、VGG Net、GoogLeNet）改造为全卷积网络，以便在语义分割任务上进行端到端（输入图像，输出分割掩码）的训练。\u003cbr/\u003e 改造的方式是将最后的全连接层替换成卷积层。 \"}}]]}]]}],\"$undefined\",\"$undefined\",\"$undefined\"]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-blue\",\"children\":[[\"$\",\"h4\",null,{\"className\":\"x-h3\",\"children\":\"更多笔记\"}],[\"$\",\"h4\",null,{\"className\":\"x-h3\",\"children\":\"转置卷积\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"卷积通常不会增大输入的高宽，而是保持不变或降低。由于语义分割任务需要像素级别的输出，转置卷积被用来增大输入的高宽。\"}}],[\"$\",\"$L14\",null,{\"src\":\"transpose_convolution.jpg\",\"width\":\"600px\",\"filterDarkTheme\":true}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"图片转载自\u003ca href=\\\"https://zh-v2.d2l.ai/chapter_computer-vision/transposed-conv.html\\\" target=\\\"_blank\\\" rel=\\\"noreferrer\\\" class=\\\"x-inline-link\\\"\u003e动手学深度学习 - 转置卷积\u003c/a\u003e\"}}],[\"$\",\"h4\",null,{\"className\":\"x-h3\",\"children\":\"FCN中的转置卷积\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"$15\"}}]]}],[\"$\",\"h3\",null,{\"className\":\"x-h2\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/pdf/1911.11907\",\"target\":\"_blank\",\"rel\":\"noreferrer\",\"children\":\"【GhostNet】GhostNet: More Features from Cheap Operations (2020)\"}]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-gray\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的主题 / 文章要解决什么问题？\u003c/span\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"在嵌入式设备上部署CNN通常由于内存和计算资源受限而困难，GhostNet提出了Ghost模块，用低成本的操作生成更多特征图，可以作为一种轻量化的深度卷积模型架构\"}}]]}]]}],[\"$\",\"div\",null,{\"className\":\"x-uli\",\"children\":[[\"$\",\"div\",null,{\"className\":\"x-uli-marker\",\"children\":[\"$\",\"div\",null,{\"className\":\"x-uli-marker-dot\"}]}],[\"$\",\"div\",null,{\"className\":\"x-uli-content-wrapper\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e文章的核心方法 / 具体是如何做的？\u003c/span\u003e\"}}],[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"特征图的各层之间有很多是相似的（冗余信息），论文希望能通过更低成本的计算量去获取它们。\"}}],[\"$\",\"$L14\",null,{\"src\":\"ghost_module.jpg\",\"width\":\"400px\",\"filterDarkTheme\":true}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"对于某个特征层，只用较少卷积操作生成部分真实的特征层（固有特征图），剩余的特征层通过对固有特征图的每个通道线性变换生成，最后将这些特征层拼接在一起。\"}}]]]}]]}],\"$undefined\",\"$undefined\",\"$undefined\"]}]]\n"])</script><script>self.__next_f.push([1,"10:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"论文速记：深度学习 - 铃木的网络日记\"}],[\"$\",\"link\",\"3\",{\"rel\":\"canonical\",\"href\":\"https://1kuzus.github.io/longtime/papers-dl/\"}]]\n5:null\n"])</script><script>self.__next_f.push([1,""])</script></body></html>