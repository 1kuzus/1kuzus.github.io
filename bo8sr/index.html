<!DOCTYPE html><html class="light" lang="zh-CN"><head><meta charset="UTF-8"><meta content="width=device-width,initial-scale=1,user-scalable=no" name="viewport"><script src="https://www.googletagmanager.com/gtag/js?id=G-45BYSZ6WPY" async></script><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-45BYSZ6WPY"),localStorage.getItem("theme")||localStorage.setItem("theme","light"),document.documentElement.setAttribute("class",localStorage.getItem("theme"))</script><title>论文速记</title><script src="/static/js/main.9a97426c.js" defer></script><link href="/static/css/main.d9b1ff02.css" rel="stylesheet"><script src="//busuanzi.ibruce.info/busuanzi?jsonpCallback=BusuanziCallback_1079107206614" defer referrerpolicy="no-referrer-when-downgrade" type="text/javascript"></script><link href="https://busuanzi.ibruce.info" rel="preconnect"><link href="https://www.googletagmanager.com" rel="preconnect"><meta content="AymqwRC7u88Y4JPvfIF2F37QKylC04248hLCdJAsh8xgOfe/dVJPV3XS3wLFca1ZMVOtnBfVjaCMTVudWM//5g4AAAB7eyJvcmlnaW4iOiJodHRwczovL3d3dy5nb29nbGV0YWdtYW5hZ2VyLmNvbTo0NDMiLCJmZWF0dXJlIjoiUHJpdmFjeVNhbmRib3hBZHNBUElzIiwiZXhwaXJ5IjoxNjk1MTY3OTk5LCJpc1RoaXJkUGFydHkiOnRydWV9" http-equiv="origin-trial"><link href="http://busuanzi.ibruce.info" rel="preconnect"></head><body><div id="root"><div id="header"><div id="header-logo-bg"><svg viewBox="0 0 1560 1560" xmlns="http://www.w3.org/2000/svg"><g fill-rule="evenodd"><path d="M644 97h272.529L1189 780H916.471z" fill="#00A8C4"></path><path d="M98 97h272.84L780 1120.73 1189.162 97H1462L916.438 1462H643.562z" fill="#30303C"></path><path d="M98 1462L643.3 97H916L370.7 1462z" fill="#00F8FF"></path></g></svg></div><div id="header-right-wrapper"><div id="header-theme-bg"><svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M512 61.44c13.312 0 25.252 7.639 29.942 19.17l27.566 67.972c11.92 29.348-4.178 62.075-35.943 73.094A65.946 65.946 0 01512 225.28c-33.935 0-61.44-25.416-61.44-56.77 0-6.8 1.331-13.558 3.912-19.928l27.586-67.973C486.728 69.08 498.647 61.44 512 61.44zm0 901.12c-13.332 0-25.272-7.639-29.942-19.17l-27.586-67.972a52.961 52.961 0 01-3.912-19.927c0-31.335 27.505-56.771 61.44-56.771a66.28 66.28 0 0121.565 3.604c31.765 11.019 47.862 43.746 35.943 73.114l-27.566 67.953c-4.69 11.53-16.63 19.169-29.942 19.169zM962.56 512c0 13.312-7.639 25.252-19.17 29.942l-67.972 27.566c-29.348 11.92-62.075-4.178-73.094-35.943A65.946 65.946 0 01798.72 512c0-33.935 25.416-61.44 56.77-61.44 6.8 0 13.558 1.331 19.928 3.912l67.973 27.586c11.53 4.67 19.169 16.589 19.169 29.942zm-901.12 0c0-13.332 7.639-25.272 19.17-29.942l67.972-27.586a52.9 52.9 0 0119.927-3.912c31.335 0 56.771 27.505 56.771 61.44a66.28 66.28 0 01-3.604 21.565c-11.019 31.765-43.746 47.862-73.114 35.943l-67.953-27.566C69.08 537.252 61.44 525.312 61.44 512zm131.953-318.587c9.42-9.42 23.265-12.452 34.734-7.618l67.563 28.549c29.184 12.35 40.94 46.858 26.256 77.107a65.946 65.946 0 01-12.698 17.817c-23.982 23.983-61.399 25.457-83.558 3.277a52.961 52.961 0 01-11.346-16.855l-28.55-67.543c-4.853-11.469-1.822-25.313 7.599-34.734zm637.194 637.194c-9.42 9.421-23.265 12.452-34.734 7.598l-67.543-28.549a52.961 52.961 0 01-16.876-11.325c-22.16-22.18-20.685-59.597 3.298-83.579a65.946 65.946 0 0117.817-12.698c30.25-14.684 64.758-2.928 77.107 26.256l28.55 67.563c4.833 11.47 1.802 25.293-7.62 34.734zm0-637.214c9.42 9.42 12.452 23.265 7.618 34.734l-28.549 67.563c-12.35 29.184-46.858 40.94-77.107 26.256a65.946 65.946 0 01-17.817-12.698c-23.983-23.982-25.457-61.399-3.277-83.558 4.792-4.834 10.506-8.663 16.855-11.346l67.543-28.55c11.469-4.853 25.313-1.822 34.734 7.599zM193.393 830.587c-9.421-9.42-12.452-23.265-7.598-34.734l28.549-67.543a52.618 52.618 0 0111.325-16.876c22.18-22.16 59.597-20.685 83.579 3.298a65.842 65.842 0 0112.698 17.817c14.684 30.25 2.928 64.758-26.256 77.107l-67.563 28.55c-11.47 4.833-25.293 1.802-34.734-7.62zM512 737.28c-124.416 0-225.28-100.864-225.28-225.28S387.584 286.72 512 286.72 737.28 387.584 737.28 512 636.416 737.28 512 737.28z" fill="#FCFCFC"></path></svg></div><a href="https://github.com/1kuzus/1kuzus.github.io" rel="noreferrer" target="_blank"><div id="header-github-bg"><svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M411.306667 831.146667c3.413333-5.12 6.826667-10.24 6.826666-11.946667v-69.973333c-105.813333 22.186667-128-44.373333-128-44.373334-17.066667-44.373333-42.666667-56.32-42.666666-56.32-34.133333-23.893333 3.413333-23.893333 3.413333-23.893333 37.546667 3.413333 58.026667 39.253333 58.026667 39.253333 34.133333 58.026667 88.746667 40.96 110.933333 32.426667 3.413333-23.893333 13.653333-40.96 23.893333-51.2-85.333333-10.24-174.08-42.666667-174.08-187.733333 0-40.96 15.36-75.093333 39.253334-102.4-3.413333-10.24-17.066667-47.786667 3.413333-100.693334 0 0 32.426667-10.24 104.106667 39.253334 30.72-8.533333 63.146667-11.946667 95.573333-11.946667 32.426667 0 64.853333 5.12 95.573333 11.946667 73.386667-49.493333 104.106667-39.253333 104.106667-39.253334 20.48 52.906667 8.533333 90.453333 3.413333 100.693334 23.893333 27.306667 39.253333 59.733333 39.253334 102.4 0 145.066667-88.746667 177.493333-174.08 187.733333 13.653333 11.946667 25.6 34.133333 25.6 69.973333v104.106667c0 3.413333 1.706667 6.826667 6.826666 11.946667 5.12 6.826667 3.413333 18.773333-3.413333 23.893333-3.413333 1.706667-6.826667 3.413333-10.24 3.413333h-174.08c-10.24 0-17.066667-6.826667-17.066667-17.066666 0-5.12 1.706667-8.533333 3.413334-10.24z" fill="#FCFCFC"></path></svg></div></a></div></div><div id="sidebar"><div class="sidebarlist showlist"><div class="sidebarlist-head"><h3 class="sidebarlist-title">网络杂识 (2)</h3><div class="sidebarlist-title-rightarrow"><svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M761.056 532.128c.512-.992 1.344-1.824 1.792-2.848 8.8-18.304 5.92-40.704-9.664-55.424L399.936 139.744c-19.264-18.208-49.632-17.344-67.872 1.888-18.208 19.264-17.376 49.632 1.888 67.872l316.96 299.84L335.2 813.632c-19.072 18.4-19.648 48.768-1.248 67.872 9.408 9.792 21.984 14.688 34.56 14.688 12 0 24-4.48 33.312-13.44l350.048-337.376c.672-.672.928-1.6 1.6-2.304.512-.48 1.056-.832 1.568-1.344 2.72-2.848 4.16-6.336 6.016-9.6z" fill="#50505c"></path></svg></div></div><div class="sidebarlist-ul-wrapper"><ul class="sidebarlist-ul"><li><a href="/bykl9/" class="sidebarlist-li">数据库设计三大范式</a></li><li><a href="/bq2l8/" class="sidebarlist-li">不统计Github仓库某个文件夹下的语言</a></li></ul></div></div><div class="sidebarlist showlist"><div class="sidebarlist-head"><h3 class="sidebarlist-title">深度学习 (5)</h3><div class="sidebarlist-title-rightarrow"><svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M761.056 532.128c.512-.992 1.344-1.824 1.792-2.848 8.8-18.304 5.92-40.704-9.664-55.424L399.936 139.744c-19.264-18.208-49.632-17.344-67.872 1.888-18.208 19.264-17.376 49.632 1.888 67.872l316.96 299.84L335.2 813.632c-19.072 18.4-19.648 48.768-1.248 67.872 9.408 9.792 21.984 14.688 34.56 14.688 12 0 24-4.48 33.312-13.44l350.048-337.376c.672-.672.928-1.6 1.6-2.304.512-.48 1.056-.832 1.568-1.344 2.72-2.848 4.16-6.336 6.016-9.6z" fill="#50505c"></path></svg></div></div><div class="sidebarlist-ul-wrapper"><ul class="sidebarlist-ul"><li><a href="/bo8sr/" class="sidebarlist-li active">论文速记</a></li><li><a href="/b97t6/" class="sidebarlist-li">行为识别R(2+1)D网络</a></li><li><a href="/b24fa/" class="sidebarlist-li">目标检测评价指标mAP</a></li><li><a href="/bnwt2/" class="sidebarlist-li">学习RNN和LSTM</a></li><li><a href="/bnoay/" class="sidebarlist-li">复盘：复现NeRF-RPN代码</a></li></ul></div></div><div class="sidebarlist showlist"><div class="sidebarlist-head"><h3 class="sidebarlist-title">Python学习 (1)</h3><div class="sidebarlist-title-rightarrow"><svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M761.056 532.128c.512-.992 1.344-1.824 1.792-2.848 8.8-18.304 5.92-40.704-9.664-55.424L399.936 139.744c-19.264-18.208-49.632-17.344-67.872 1.888-18.208 19.264-17.376 49.632 1.888 67.872l316.96 299.84L335.2 813.632c-19.072 18.4-19.648 48.768-1.248 67.872 9.408 9.792 21.984 14.688 34.56 14.688 12 0 24-4.48 33.312-13.44l350.048-337.376c.672-.672.928-1.6 1.6-2.304.512-.48 1.056-.832 1.568-1.344 2.72-2.848 4.16-6.336 6.016-9.6z" fill="#50505c"></path></svg></div></div><div class="sidebarlist-ul-wrapper"><ul class="sidebarlist-ul"><li><a href="/btkp2/" class="sidebarlist-li">在numpy和pytorch中取top-k值和索引</a></li></ul></div></div><div class="sidebarlist showlist"><div class="sidebarlist-head"><h3 class="sidebarlist-title">前端与JavaScript (2)</h3><div class="sidebarlist-title-rightarrow"><svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M761.056 532.128c.512-.992 1.344-1.824 1.792-2.848 8.8-18.304 5.92-40.704-9.664-55.424L399.936 139.744c-19.264-18.208-49.632-17.344-67.872 1.888-18.208 19.264-17.376 49.632 1.888 67.872l316.96 299.84L335.2 813.632c-19.072 18.4-19.648 48.768-1.248 67.872 9.408 9.792 21.984 14.688 34.56 14.688 12 0 24-4.48 33.312-13.44l350.048-337.376c.672-.672.928-1.6 1.6-2.304.512-.48 1.056-.832 1.568-1.344 2.72-2.848 4.16-6.336 6.016-9.6z" fill="#50505c"></path></svg></div></div><div class="sidebarlist-ul-wrapper"><ul class="sidebarlist-ul"><li><a href="/bj23k/" class="sidebarlist-li">JavaScript数组常用方法</a></li><li><a href="/bpbmq/" class="sidebarlist-li">CSS实现auto高度的过渡动画</a></li></ul></div></div><div class="sidebarlist showlist"><div class="sidebarlist-head"><h3 class="sidebarlist-title">课程 (7)</h3><div class="sidebarlist-title-rightarrow"><svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M761.056 532.128c.512-.992 1.344-1.824 1.792-2.848 8.8-18.304 5.92-40.704-9.664-55.424L399.936 139.744c-19.264-18.208-49.632-17.344-67.872 1.888-18.208 19.264-17.376 49.632 1.888 67.872l316.96 299.84L335.2 813.632c-19.072 18.4-19.648 48.768-1.248 67.872 9.408 9.792 21.984 14.688 34.56 14.688 12 0 24-4.48 33.312-13.44l350.048-337.376c.672-.672.928-1.6 1.6-2.304.512-.48 1.056-.832 1.568-1.344 2.72-2.848 4.16-6.336 6.016-9.6z" fill="#50505c"></path></svg></div></div><div class="sidebarlist-ul-wrapper"><ul class="sidebarlist-ul"><li><a href="/behwl/" class="sidebarlist-li">【模式识别】统计决策方法</a></li><li><a href="/bco21/" class="sidebarlist-li">【模式识别】参数估计</a></li><li><a href="/bjjj5/" class="sidebarlist-li">【模式识别】非参数估计</a></li><li><a href="/b2fph/" class="sidebarlist-li">【模式识别】线性学习器与线性分类器</a></li><li><a href="/blnrj/" class="sidebarlist-li">【计算机网络】协议总结</a></li><li><a href="/brro7/" class="sidebarlist-li">【机器学习】习题</a></li><li><a href="/bkdzq/" class="sidebarlist-li">【GAMES101】Transformation</a></li></ul></div></div><div class="sidebarlist showlist"><div class="sidebarlist-head"><h3 class="sidebarlist-title">其他 (2)</h3><div class="sidebarlist-title-rightarrow"><svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M761.056 532.128c.512-.992 1.344-1.824 1.792-2.848 8.8-18.304 5.92-40.704-9.664-55.424L399.936 139.744c-19.264-18.208-49.632-17.344-67.872 1.888-18.208 19.264-17.376 49.632 1.888 67.872l316.96 299.84L335.2 813.632c-19.072 18.4-19.648 48.768-1.248 67.872 9.408 9.792 21.984 14.688 34.56 14.688 12 0 24-4.48 33.312-13.44l350.048-337.376c.672-.672.928-1.6 1.6-2.304.512-.48 1.056-.832 1.568-1.344 2.72-2.848 4.16-6.336 6.016-9.6z" fill="#50505c"></path></svg></div></div><div class="sidebarlist-ul-wrapper"><ul class="sidebarlist-ul"><li><a href="/b0000/" class="sidebarlist-li">示例</a></li><li><a href="/b0001/" class="sidebarlist-li">更新日志</a></li></ul></div></div></div><div class="x-blogwrapper"><h1 class="x-title">论文速记</h1><h2 class="x-h1">研究</h2><h3 class="x-h2"><a href="https://arxiv.org/pdf/2003.08934.pdf" rel="noreferrer" target="_blank">【NeRF】NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis (2020)</a></h3><div class="x-oli"><div class="x-oli-number">1.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">使用稀疏输入2D图片集实现场景的3D视图合成</p></div></div><div class="x-oli"><div class="x-oli-number">2.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的核心方法 / 具体是如何做的？</span></p><p class="x-p">从<span class="x-inline-highlight">(<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8889em;vertical-align:-.1944em"></span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal" style="margin-right:.03588em">y</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal" style="margin-right:.04398em">z</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal" style="margin-right:.02778em">θ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal">ϕ</span></span></span></span>)</span>到<span class="x-inline-highlight">(<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8778em;vertical-align:-.1944em"></span><span class="mord mathnormal" style="margin-right:.00773em">R</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal">G</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal" style="margin-right:.05017em">B</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord mathnormal" style="margin-right:.03588em">σ</span></span></span></span>)</span></p></div></div><div class="x-oli"><div class="x-oli-number">3.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">做了什么实验，效果怎么样？</span></p><p class="x-p">测试数据集是<span class="x-inline-highlight">Diffuse Synthetic 360</span>、<span class="x-inline-highlight">Realistic Synthetic 360</span>和<span class="x-inline-highlight">Real Forward-Facing</span></p></div></div><div class="x-oli"><div class="x-oli-number">4.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">研究的创新点</span></p><p class="x-p">将输入坐标位置编码，帮助MLP表示高频函数<br>分层采样</p></div></div><div class="x-oli"><div class="x-oli-number">5.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">有什么限制或可以改进的地方？</span></p><p class="x-p">有效地优化和渲染神经辐射场<br>可解释性</p></div></div><div class="x-highlightblock highlight-background-gray"><h4 class="x-h3">更多笔记</h4><p class="x-p">神经辐射场用于从2D的图片重建3D的场景。</p><p class="x-p no-margin-bottom">文中出现的三个指标：PSNR、SSIM、LPIPS</p><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><span class="x-inline-strong">峰值信噪比</span><span class="x-inline-highlight">(Peak Signal to Noise Ratio, PSNR)</span>：用于衡量图像恢复的质量，数值越高表示图像质量越好。接近<span class="x-inline-highlight">50 dB</span>代表误差非常小，大于<span class="x-inline-highlight">30 dB</span>人眼难察觉差异。</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><span class="x-inline-strong">结构相似性</span><span class="x-inline-highlight">(Structural Similarity Index Measure, SSIM)</span>：用于衡量图像的结构相似性，得分通常在<span class="x-inline-highlight">0</span>~<span class="x-inline-highlight">1</span>之间，数值越高表示图像结构越相似。相较于PSNR在图像质量的衡量上更能符合人眼对图像质量的判断。</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><span class="x-inline-strong">基于学习的感知图像质量评价</span><span class="x-inline-highlight">(Learned Perceptual Image Patch Similarity, LPIPS)</span>：测量从预训练网络中提取的两个图像的特征之间的相似性，得分通常在<span class="x-inline-highlight">0</span>~<span class="x-inline-highlight">1</span>之间，数值越低表示感知质量越高。</p></div></div></div><h3 class="x-h2"><a href="https://arxiv.org/pdf/2308.04079.pdf" rel="noreferrer" target="_blank">【3DGS】3D Gaussian Splatting for Real-Time Radiance Field Rendering (2023)</a></h3><div class="x-oli"><div class="x-oli-number">1.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">实现实时辐射场渲染，同时保持高质量的视觉效果，并且保持较短的训练时间</p></div></div><div class="x-highlightblock highlight-background-gray"><h4 class="x-h3">更多笔记</h4><h4 class="x-h3">文章的相关工作部分</h4><p class="x-p">传统的场景重建与渲染：基于光场的，密集采样、非结构化捕获；<span class="x-inline-strong">运动恢复结构</span><span class="x-inline-highlight">(Structure from Motion, SFM)</span>用一组照片估计稀疏点云合成新视图；<span class="x-inline-strong">多视点立体视觉</span><span class="x-inline-highlight">(Multi-View Stereo, MVS)</span>；<br>神经渲染和辐射场：用CNN估计混合权重，用于纹理空间；Soft3D提出<span class="x-inline-highlight">Volumetric representations</span>；NeRF提出重要性采样和位置编码来提高质量，但使用了大型多层感知器，对速度有负面影响；</p><h4 class="x-h3">稀疏重建和稠密重建</h4><p class="x-p">稀疏重建主要用于定位，得到每张图片的相机参数，提取特征点，例如SFM；稠密重建是假设相机参数已知的情况下，从不同视角的图像中找到匹配的对应点，对整个图像或图像中绝大部分像素进行重建。</p></div><h3 class="x-h2"><a href="https://arxiv.org/pdf/2201.05989.pdf" rel="noreferrer" target="_blank">【Instant NGP】Instant Neural Graphics Primitives with a Multiresolution Hash Encoding (2022)</a></h3><div class="x-oli"><div class="x-oli-number">1.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">实现实时辐射场渲染，同时保持高质量的视觉效果，并且保持较短的训练时间</p></div></div><div class="x-highlightblock highlight-background-gray"><h4 class="x-h3">更多笔记</h4><h4 class="x-h3">Instant NGP与NeRF的异同</h4><p class="x-p no-margin-bottom">转载自<a href="https://zhuanlan.zhihu.com/p/631284285" class="x-inline-link" rel="noreferrer" target="_blank">知乎：从NeRF到Instant-NGP</a></p><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p">同样基于体渲染</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p">不同于NeRF的MLP，Instant NGP使用稀疏的参数化的<span class="x-inline-highlight">voxel grid</span>作为场景表达</p></div></div></div><h3 class="x-h2"><a href="https://arxiv.org/pdf/2211.11646.pdf" rel="noreferrer" target="_blank">【NeRF RPN】NeRF-RPN: A general framework for object detection in NeRFs (2022)</a></h3><div class="x-oli"><div class="x-oli-number">1.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">在NeRF中直接进行3D物体检测</p></div></div><div class="x-oli"><div class="x-oli-number">2.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的核心方法 / 具体是如何做的？</span></p><p class="x-p">第一部分：特征提取器<br>从NeRF采样的辐射度和密度网格作为输入，生成特征金字塔作为输出。<br>第二部分：RPN头<br>对特征金字塔进行操作并生成对象建议。</p></div></div><div class="x-oli"><div class="x-oli-number">3.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">研究的创新点</span></p><p class="x-p">第一次将RPN引入NeRF以进行3D物体检测和相关任务<br>利用Hypersim和3D-FRONT数据集构建了第一个用于3D目标检测的NeRF数据集</p></div></div><h3 class="x-h2"><a href="https://arxiv.org/pdf/2304.04395v3.pdf" rel="noreferrer" target="_blank">【Instance NeRF】Instance Neural Radiance Field (2023)</a></h3><div class="x-oli"><div class="x-oli-number">1.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">输入一个以多视图RGB图像预训练的NeRF，学习给定场景的3D实例分割</p><p class="x-p no-margin-bottom">文章的主要贡献：</p><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p">第一个在NeRF中进行3D实例分割的尝试之一，而没有使用真实分割标签作为输入</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p">提出<span class="x-inline-highlight">Neural Instance Field</span>的结构和训练方法，可以产生<span class="x-inline-strong">多视图一致</span>的2D分割和连续的3D分割</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p">对合成室内NeRF数据集进行实验和消融研究</p></div></div></div></div><div class="x-oli"><div class="x-oli-number">2.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的核心方法 / 具体是如何做的？</span></p><p class="x-p">Instance NeRF有两个组件：预训练的NeRF模型、和文中提出的<span class="x-inline-highlight">Instance Field</span>。</p><p class="x-p"><span class="x-inline-highlight">Instance Field</span>的训练过程如下：</p><div class="x-img-wrapper x-img-invert"><img alt="img" src="/static/media/instance_field.debdb29f12d84ebfe6ab.jpg" width="96%"></div><p class="x-p">NeRF-RCNN用预训练的NeRF提取的辐射场和密度场，为每个检测到的对象输出3D掩码；Mask2Former生成从NeRF渲染的图像的二维全景分割图（跨视图的实例标签并不一定一致）。然后按照相机位置，投影3D掩码去匹配不同视图中的相同实例，去产生多视图一致的2D分割图。---</p></div></div><div class="x-highlightblock highlight-background-gray"><h4 class="x-h3">更多笔记</h4><h4 class="x-h3">包围体：AABB和OBB</h4><p class="x-p"><span class="x-inline-strong">AABB</span>：轴对齐包围盒<span class="x-inline-highlight">(Axis-Aligned Bounding Box)</span><br><span class="x-inline-strong">OBB</span>：有向包围盒<span class="x-inline-highlight">(Oriented Bounding Box)</span></p><p class="x-p">下图展示了更多种类的包围体：</p><div class="x-img-wrapper x-img-invert"><img alt="img" src="/static/media/bounding_volumes.b0427ddaf68c52436bfe.png" width="100%"></div></div><h3 class="x-h2"><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Cheng_CascadePSP_Toward_Class-Agnostic_and_Very_High-Resolution_Segmentation_via_Global_and_CVPR_2020_paper.pdf" rel="noreferrer" target="_blank">【CascadePSP】CascadePSP: Toward Class-Agnostic and Very High-Resolution Segmentation via Global and Local Refinement (2020)</a></h3><div class="x-oli"><div class="x-oli-number">1.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">提出一种不使用高分辨率训练数据，解决高分辨率分割问题的方法<br>右图是改进后的结果：</p><div class="x-img-wrapper x-img-invert"><img alt="img" src="/static/media/cascadepsp.6e40ef664ae3ba942f27.jpg" width="400px"></div></div></div><h2 class="x-h1">学习</h2><h3 class="x-h2"><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf" rel="noreferrer" target="_blank">【R-CNN】Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation (2014)</a></h3><div class="x-oli"><div class="x-oli-number">1.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">提出<span class="x-inline-highlight">Regions with CNN features, R-CNN</span>提高目标检测性能</p></div></div><div class="x-oli"><div class="x-oli-number">2.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的核心方法 / 具体是如何做的？</span></p><p class="x-p">区域提议<span class="x-inline-highlight">(Region Proposals)</span>：使用<span class="x-inline-highlight">selective search</span>生成候选框</p></div></div><div class="x-oli"><div class="x-oli-number">3.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">做了什么实验，效果怎么样？</span></p><p class="x-p">在<span class="x-inline-highlight">PASCAL VOC 2012</span>取得<span class="x-inline-highlight">mAP 53.3%</span>，在<span class="x-inline-highlight">ILSVRC 2013</span>竞赛数据集取得<span class="x-inline-highlight">mAP 31.4%</span></p></div></div><div class="x-highlightblock highlight-background-gray"><h4 class="x-h3">更多笔记</h4><p class="x-p">转载自<a href="https://zh-v2.d2l.ai/chapter_computer-vision/rcnn.html" class="x-inline-link" rel="noreferrer" target="_blank">动手学深度学习 - 区域卷积神经网络系列</a></p><h4 class="x-h3">R-CNN</h4><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p">R-CNN使用启发式搜索算法<span class="x-inline-highlight">selective search</span>（之前人们通常也是这样做的）来选择锚框</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p">用预训练的模型，对每一个锚框抽取特征</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p">训练一个SVM来对类别分类</p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p">训练一个线性回归模型来预测边缘框</p></div></div><p class="x-p">R-CNN的速度很慢，因为可能从一张图像中选出上千个提议区域，这需要上千次的卷积神经网络的前向传播来执行目标检测。这种庞大的计算量使得R-CNN在现实世界中难以被广泛应用。</p><div class="x-img-wrapper x-img-invert"><img alt="img" src="/static/media/rcnn.b2dfd79c8026243a9b0d.jpg" width="600px"></div><h4 class="x-h3">Fast R-CNN</h4><p class="x-p">R-CNN的主要性能瓶颈在于，对每个提议区域，卷积神经网络的前向传播是独立的，而没有共享计算。由于这些区域通常有重叠，独立的特征抽取会导致重复的计算。Fast R-CNN的主要改进之一，是仅在整张图象上执行卷积神经网络的前向传播，并且引入<span class="x-inline-strong">兴趣区域池化</span><span class="x-inline-highlight">(ROI Pooling)</span>，将卷积神经网络的输出和提议区域作为输入，输出连结后的各个提议区域抽取的特征。</p><p class="x-p">兴趣区域池化层可以给出固定大小的输出：把给定的锚框均匀分割成<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6667em;vertical-align:-.0833em"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:.4306em"></span><span class="mord mathnormal">m</span></span></span></span>块，输出每块里的最大值，这样无论锚框多大，总是输出<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.4306em"></span><span class="mord mathnormal">nm</span></span></span></span>个值。</p><p class="x-p">Fast R-CNN先对图片用CNN抽取特征，然后将<span class="x-inline-highlight">selective search</span>给出的原图上的提议区域映射到CNN特征图上，再经过<span class="x-inline-highlight">ROI Pooling</span>就可以得到维度对齐的特征。</p><div class="x-img-wrapper x-img-invert"><img alt="img" src="/static/media/fastrcnn.98e804c88224ff75662e.jpg" width="400px"></div><h4 class="x-h3">Faster R-CNN</h4><p class="x-p">与Fast R-CNN相比，Faster R-CNN将生成提议区域的方法从<span class="x-inline-highlight">selective search</span>改为了<span class="x-inline-strong">区域提议网络</span><span class="x-inline-highlight">(Region Proposal Network, RPN)</span>，模型的其余部分保持不变。区域提议网络作为Faster R-CNN模型的一部分，是和整个模型一起训练得到的。换句话说，Faster R-CNN的目标函数不仅包括目标检测中的类别和边界框预测，还包括区域提议网络中锚框的二元类别和边界框预测。作为端到端训练的结果，区域提议网络能够学习到如何生成高质量的提议区域，从而在减少了从数据中学习的提议区域的数量的情况下，仍保持目标检测的精度。</p><div class="x-img-wrapper x-img-invert"><img alt="img" src="/static/media/fasterrcnn.cdec43ba18fb570890a9.jpg" width="600px"></div></div><h3 class="x-h2"><a href="https://arxiv.org/pdf/1703.06870.pdf" rel="noreferrer" target="_blank">【Mask R-CNN】Mask R-CNN (2017)</a></h3><div class="x-oli"><div class="x-oli-number">1.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">目标检测&实例分割</p></div></div><div class="x-oli"><div class="x-oli-number">2.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的核心方法 / 具体是如何做的？</span></p><p class="x-p">基于Faster R-CNN，添加一个对每个ROI预测分割mask的分支。这个分支可以与分类和边界框预测分支并行。</p><div class="x-img-wrapper"><img alt="img" src="/static/media/maskrcnn1.5e07ca64ee98f3aed573.jpg" width="600px"></div></div></div><div class="x-oli"><div class="x-oli-number">3.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">做了什么实验，效果怎么样？</span></p><div class="x-img-wrapper x-img-invert"><img alt="img" src="/static/media/maskrcnn2.4c274f794b647ba7ece2.jpg" width="600px"></div><p class="x-p">在COCO数据集上表现优异，超过了<span class="x-inline-highlight">2015</span>和<span class="x-inline-highlight">2016</span>年COCO分割任务的冠军。</p></div></div><div class="x-oli"><div class="x-oli-number">4.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">研究的创新点</span></p><p class="x-p"><span class="x-inline-highlight">ROI Pooling</span>中发生了两次浮点数取整，第一次是将锚框均匀分割成<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6667em;vertical-align:-.0833em"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:.4306em"></span><span class="mord mathnormal">m</span></span></span></span>块时，第二次是把提议区域映射回CNN特征图时。<br>分割任务的精细程度更高，因此文章提出了<span class="x-inline-highlight">ROI Align</span>，使用双线性插值来保留特征图上的空间信息。</p></div></div><div class="x-highlightblock highlight-background-gray"><h4 class="x-h3">更多笔记</h4><h4 class="x-h3">语义分割与实例分割</h4><div class="x-img-wrapper"><img alt="img" src="/static/media/ss_and_is.37cf7910ec000cdd634a.jpg" width="600px"></div><p class="x-p no-margin-bottom"><span class="x-inline-strong">语义分割</span><span class="x-inline-highlight">(semantic segmentation)</span>：为每一个像素分配一个类别，但不区分同一类别之间的对象。</p><p class="x-p"><span class="x-inline-strong">实例分割</span><span class="x-inline-highlight">(instance segmentation)</span>：会区分属于同一类别的不同实例。</p></div><h3 class="x-h2"><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf" rel="noreferrer" target="_blank">【FCN】Fully Convolutional Networks for Semantic Segmentation (2015)</a></h3><div class="x-oli"><div class="x-oli-number">1.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的主题 / 文章要解决什么问题？</span></p><p class="x-p">使用全卷积网络进行语义分割</p></div></div><div class="x-oli"><div class="x-oli-number">2.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom"><span class="x-inline-strong">文章的核心方法 / 具体是如何做的？</span></p><p class="x-p">是将现有的分类网络（AlexNet、VGG Net、GoogLeNet）改造为全卷积网络，以便在语义分割任务上进行端到端（输入图像，输出分割掩码）的训练。<br>改造的方式是将最后的全连接层替换成卷积层。</p></div></div><div class="x-highlightblock highlight-background-gray"><h4 class="x-h3">更多笔记</h4><h4 class="x-h3">转置卷积</h4><p class="x-p">卷积通常不会增大输入的高宽，而是保持不变或降低。由于语义分割任务需要像素级别的输出，转置卷积被用来增大输入的高宽。</p><div class="x-img-wrapper x-img-invert"><img alt="img" src="/static/media/transpose_convolution.4144b2ce80103d58de3c.jpg" width="600px"></div><p class="x-p">图片转载自<a href="https://zh-v2.d2l.ai/chapter_computer-vision/transposed-conv.html" class="x-inline-link" rel="noreferrer" target="_blank">动手学深度学习 - 转置卷积</a></p><h4 class="x-h3">FCN中的转置卷积</h4><p class="x-p">例如对于ImageNet的图片输入，大小通常为<span class="x-inline-highlight">224</span>*<span class="x-inline-highlight">224</span>*<span class="x-inline-highlight">3</span>（RGB通道）；经过卷积后缩小宽高缩小<span class="x-inline-highlight">32</span>倍，通道增加到<span class="x-inline-highlight">512</span>，变成<span class="x-inline-highlight">7</span>*<span class="x-inline-highlight">7</span>*<span class="x-inline-highlight">512</span>的特征图。此时FCN会先通过一个<span class="x-inline-highlight">1x1conv</span>进行通道降维，然后通过转置卷积将特征图的高度和宽度增加<span class="x-inline-highlight">32</span>倍，<span class="x-inline-strong">输出通道数等于类别数</span>，相当于储存了对每一类的预测结果。</p></div><div class="x-blogcontent"><div class="x-blogcontent-titletype-x-h1 active">研究</div><div class="x-blogcontent-titletype-x-h2">【NeRF】NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis (2020)</div><div class="x-blogcontent-titletype-x-h2">【3DGS】3D Gaussian Splatting for Real-Time Radiance Field Rendering (2023)</div><div class="x-blogcontent-titletype-x-h2">【Instant NGP】Instant Neural Graphics Primitives with a Multiresolution Hash Encoding (2022)</div><div class="x-blogcontent-titletype-x-h2">【NeRF RPN】NeRF-RPN: A general framework for object detection in NeRFs (2022)</div><div class="x-blogcontent-titletype-x-h2">【Instance NeRF】Instance Neural Radiance Field (2023)</div><div class="x-blogcontent-titletype-x-h2">【CascadePSP】CascadePSP: Toward Class-Agnostic and Very High-Resolution Segmentation via Global and Local Refinement (2020)</div><div class="x-blogcontent-titletype-x-h1">学习</div><div class="x-blogcontent-titletype-x-h2">【R-CNN】Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation (2014)</div><div class="x-blogcontent-titletype-x-h2">【Mask R-CNN】Mask R-CNN (2017)</div><div class="x-blogcontent-titletype-x-h2">【FCN】Fully Convolutional Networks for Semantic Segmentation (2015)</div></div></div></div></body></html>