<!doctype html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><script async src="https://www.googletagmanager.com/gtag/js?id=G-45BYSZ6WPY"></script><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-45BYSZ6WPY")</script><title>Blogs</title><script defer="defer" src="/static/js/main.66b1a455.js"></script><link href="/static/css/main.e91a21c0.css" rel="stylesheet"></head><body><noscript><div><h1>「模式识别」参数估计</h1><p>上一篇提到的统计决策方法，类条件密度按已知处理。但实际问题中，往往只有已知样本，需要根据已知样本推测分布。<br/>本章讨论的是参数估计，也就是分布的表达式形式是已知的，只是希望确定其中参数的值。</p><h2>最大似然估计</h2><p>假设样本的分布形式已知，现在想确定参数{'$[backslash]bm{[backslash]theta}<div id="root"></div></body></html>}。如果已经观测到了一些样本，记这些样本为{'$[backslash]bm{[backslash]chi}<div id="root"></div></body></html>}，则我们要找的参数即为：使得出现观测样本{'$[backslash]bm{[backslash]chi}<div id="root"></div></body></html>}概率最大的参数{'$[backslash]bm{[backslash]theta}<div id="root"></div></body></html>}。</p><p>考虑下面的例题：</p><div><p>每一个样本都形如$[backslash]bm{X}=[x_1, x_2, [backslash]dots, x_d]^T$，其中的每个维度都是二值变量，其分布可以由$[backslash]bm{[backslash]theta}=[[backslash]theta_1, [backslash]theta_2, [backslash]dots, [backslash]theta_d]^T$描述，即$s_i=1$的概率为$\theta_i$，相应地，$s_i=0$的概率为$1-\theta_i$。假设现在已经观察到样本{'$[backslash]bm{[backslash]chi}=[backslash]{[backslash]bm{X}_1, [backslash]bm{X}_2, [backslash]dots, [backslash]bm{X}_n[backslash]}<div id="root"></div></body></html>}，希望估计参数{'$[backslash]bm{[backslash]theta}<div id="root"></div></body></html>}。</p></div><div><p>注意，如果样本有多个维度，这里记为$[backslash]bm{X}=[x_1, x_2, [backslash]dots, x_d]^T$，$x_1$、$x_2$表示样本的分量；<br/>有时，若样本只有一个维度，则样本会直接简记为$x$；此时$x_1$、$x_2$表示两个样本。<br/>请注意语义。</p></div><p>观测到样本{'$[backslash]bm{[backslash]chi}<div id="root"></div></body></html>}的概率为：</p>P(\bm{X}_1, \bm{X}_2, \dots, \bm{X}_n|\bm{\theta}) = \prod_{k=1}^n P(\bm{X}_i) = \prod_{k=1}^n \prod_{i=1}^d \theta_i^{x_{ki}}(1-\theta_i)^{1-x_{ki}}<p>式子中的$x_{ki}$代表第$k$个样本的第$i$个分量，其值为0或1。</p><p>取对数似然函数：</p>l(\bm{\theta}) = \ln P = \sum_{k=1}^n \sum_{i=1}^d [x_{ki}\ln\theta_i + (1-x_{ki})\ln(1-\theta_i)]<p>求偏导得：</p>\frac{\partial l(\bm{\theta})}{\partial \theta_i} = \sum_{k=1}^n [\frac{x_{ki}}{\theta_i} - \frac{1-x_{ki}}{1-\theta_i}]\frac{\partial l(\bm{\theta})}{\partial \theta_i} = 0 \; 得 \; \sum_{k=1}^n (x_{ki} - \theta_i) = 0，即\theta_i = \frac{1}{n} \sum_{k=1}^n x_{ki}<p>把每一个分量都叠加起来，就得到最终的参数估计：</p>\hat{\bm{\theta}} = \frac{1}{n} \sum_{k=1}^n \bm{X}_k<p>举一个带有具体数值的例子，假如观测到了四个样本：$[1,0,0]$、$[1,1,0]$、$[1,0,0]$、$[1,1,1]$，则估计参数{'$[backslash]hat{[backslash]bm{[backslash]theta}} = [1,0.5,0.25]<div id="root"></div></body></html>}<br/>其含义为，对于第一个维度，四个样本均为1，因此估计$x_1=1$的概率为1；对于第二个样本，四个样本中有两个为1，另外两个为0，则估计$x_2=1$的概率为0.5，以此类推。</p><h3>正态分布下的最大似然估计</h3><h4>单变量正态分布</h4>\hat{\mu} = \frac{1}{n} \sum x_k\hat{\sigma}^2 = \frac{1}{n} \sum (x_k - \hat{\mu})^2<h4>多变量正态分布</h4>\hat{\bm{\mu}} = \frac{1}{n} \sum \bm{X}_k\hat{\bm{\Sigma}} = \frac{1}{n} \sum (\bm{X}_k - \hat{\bm{\mu}})(\bm{X}_k - \hat{\bm{\mu}})^T<h2>贝叶斯估计</h2><p>贝叶斯估计与最大似然估计一个根本的区别是，最大似然估计把未知参数当作固定的量，而贝叶斯估计把未知参数本身也看作随机变量。假设未知参数是$[backslash]bm{[backslash]theta}$，其分布空间为$[backslash]bm{[backslash]Theta}$，定义损失函数$[backslash]lambda([backslash]bm{[backslash]theta},[backslash]hat{[backslash]bm{[backslash]theta}})$表示估计误差的损失。如果已经观测到了样本集$[backslash]bm{[backslash]chi}$，那么我们的目标是：</p><div><p>最小化期望风险：</p>\int_{\bm{\Theta}} \lambda(\bm{\theta},\hat{\bm{\theta}}) P(\bm{\theta}|\bm{\chi}) d\bm{\theta}</div><p>通常情况下损失函数取$[backslash]lambda([backslash]bm{[backslash]theta},[backslash]hat{[backslash]bm{[backslash]theta}}) = ([backslash]bm{[backslash]theta}-[backslash]hat{[backslash]bm{[backslash]theta}})^2$，此时有结论：</p><div><p>在给定样本集下，{'$[backslash]bm{[backslash]theta}<div id="root"></div></body></html>}的贝叶斯估计量是$[backslash]int_{[backslash]bm{[backslash]Theta}} [backslash]bm{[backslash]theta} P([backslash]bm{[backslash]theta}|[backslash]bm{[backslash]chi}) d[backslash]bm{[backslash]theta}$。</p></div><p>在平方损失函数下，贝叶斯估计的步骤是：</p><div><p>猜测参数的先验分布$P([backslash]bm{[backslash]theta})$</p></div><div><p>对于参数估计问题，样本的概率密度函数形式已知为$P([backslash]bm{X}|[backslash]bm{[backslash]theta})$，形式上求出样本集分布为：</p>P(\bm{\chi}|\bm{\theta}) = \prod_i P(\bm{X}_i|\bm{\theta})</div><div><p>利用贝叶斯公式求$[backslash]bm{[backslash]theta}$的后验概率分布</p>P(\bm{\theta}|\bm{\chi}) = \frac{P(\bm{\chi}|\bm{\theta}) P(\bm{\theta})}{\int_{\bm{\Theta}} P(\bm{\chi}|\bm{\theta}) P(\bm{\theta}) d\bm{\theta}}</div><div><p>{'$[backslash]bm{[backslash]theta}<div id="root"></div></body></html>}的贝叶斯估计量是$[backslash]int_{[backslash]bm{[backslash]Theta}} [backslash]bm{[backslash]theta} P([backslash]bm{[backslash]theta}|[backslash]bm{[backslash]chi}) d[backslash]bm{[backslash]theta}$</p></div><div><h4>重新理解</h4><p>重述上面四个步骤，以免迷失在众多符号之中。</p><p>假设我们拿到了正态分布下的样本集$[backslash]bm{[backslash]chi}=[backslash]{x_1,x_2,x_3[backslash]}$，其中$x_1=1.3$，$x_2=4.1$，$x_3=3.7$，已知正态分布的方差$\sigma=3$，现在只需要估计均值$\mu$。</p><div><p>样本集中的每个样本出现的概率$P(x_i|\mu)$是可以由正态分布写出的，把所有样本的出现概率相乘，就得到了当前样本集出现的概率为$P([backslash]bm{[backslash]chi}|[backslash]mu)$，这也就是上述步骤2提到的样本集分布。</p><p>$P([backslash]bm{[backslash]chi}|[backslash]mu)$是一个只含$\mu$的表达式。我们记</p>f(\mu) = P(\bm{\chi}|\mu) =\frac{1}{\sqrt{2\pi}\times 3} e^{-\frac{1}{2} (\frac{1.3-\mu}{3})^2} \; \cdot \;\frac{1}{\sqrt{2\pi}\times 3} e^{-\frac{1}{2} (\frac{4.1-\mu}{3})^2} \; \cdot \;\frac{1}{\sqrt{2\pi}\times 3} e^{-\frac{1}{2} (\frac{3.7-\mu}{3})^2}<p>如果采用极大似然估计，那么做到这里对$f(\mu)$或$\ln f(\mu)$求导，使得导数为0的点就是估计值$[backslash]hat{[backslash]mu}$；取$[backslash]mu=[backslash]hat{[backslash]mu}$可以使得观测到样本集的概率$f(\mu)$最大。</p></div><div><p>概率论的两个学派中，频率学派认为应从客观掌握的数据来计算概率；而贝叶斯学派则认为概率是有先验和后验的，我们要计算的是后验概率，这个后验概率又是以先验概率为基础的。如果采用贝叶斯估计，会假设参数$\mu$存在先验分布（步骤1），这里假设$\mu$服从均匀分布：</p>P(\mu) = \frac{1}{5} \quad (0 \leq \mu \leq 5)</div><div><p>步骤3中，公式可以重写为：</p>P(\mu|\bm{\chi}) = \frac{f(\mu) P(\mu)}{\int_0^5 f(\mu) P(\mu) d\mu}<p>首先来看分母$\int_0^5 f(\mu) P(\mu) d\mu$，其含义是综合考虑所有可能的$\mu$取值，求出一个“平均的”样本集出现概率。这个积分式可以求出具体数值，而不是含$\mu$的式子。从数学的角度可以理解为，使等号左侧后验概率密度积分为1的归一化常数。</p><p>再看其他的三个量，$P([backslash]mu|[backslash]bm{[backslash]chi})$、$f([backslash]mu)=P([backslash]bm{[backslash]chi}|[backslash]mu)$、$P(\mu)$，这是贝叶斯学派的经典思想：用样本修正先验概率，得到后验概率。这三个式子都可以写为仅含$\mu$的函数。当然，因为假设的先验分布简单（均匀分布），$P(\mu)$为常数。<br/>这三个式子虽然都是$\mu$的函数，但描述的含义有所区别。$P([backslash]mu|[backslash]bm{[backslash]chi})$、$P(\mu)$描述的是$\mu$的分布概率，而$f(\mu)$描述的是样本集的出现概率。</p></div><div><p>与最大似然估计一样，贝叶斯估计也会给出参数具体的估计值。但到此步，我们只给出了$\mu$的后验分布。要给出估计值，首先要明确我们的目标是什么：<br/>最大似然估计的目标是，带入估计值可以使得样本集出现概率$f(\mu)$最大；<br/>而贝叶斯估计的目标是，带入估计值可以使得期望风险（定义见上文）最小。<br/>进一步地，在损失函数为平方误差时，有结论可以给出估计值$[backslash]hat{[backslash]mu}$的具体数值，也就是步骤4。</p></div></div><h3>贝叶斯学习</h3><p>现在我们逐一考虑每一个样本。没有样本的时候，要估计的参数先验分布为$P([backslash]bm{[backslash]theta})$。</p><p>观测到第一个样本$[backslash]bm{X}_1$的时候，用其来修正先验分布，也就是：</p>P(\bm{\theta}|\bm{X}_1) = \frac{P(\bm{X}_1|\bm{\theta}) \cdot P(\bm{\theta})}{\int_{\bm{\Theta}} P(\bm{X}_1|\bm{\theta}) \cdot P(\bm{\theta}) d\bm{\theta}}<p>现在继续观测到第二个样本$[backslash]bm{X}_2$，此时根据贝叶斯学派用数据修正先验的思想，先验分布是上一轮得到的$P([backslash]bm{[backslash]theta}|[backslash]bm{X}_1)$，修正后的后验分布为</p>P(\bm{\theta}|\bm{X}_1, \bm{X}_2) = \frac{P(\bm{X}_2|\bm{\theta}) \cdot P(\bm{\theta}|\bm{X}_1)}{\int_{\bm{\Theta}} P(\bm{X}_2|\bm{\theta}) \cdot P(\bm{\theta}|\bm{X}_1) d\bm{\theta}}<p>以此递推：</p>P(\bm{\theta}|\bm{X}_1, \bm{X}_2, \bm{X}_3) = \frac{P(\bm{X}_3|\bm{\theta}) \cdot P(\bm{\theta}|\bm{X}_1, \bm{X}_2)}{\int_{\bm{\Theta}} P(\bm{X}_3|\bm{\theta}) \cdot P(\bm{\theta}|\bm{X}_1, \bm{X}_2) d\bm{\theta}}P(\bm{\theta}|\bm{X}_1, \bm{X}_2, \dots, \bm{X}_N) =\frac{P(\bm{X}_N|\bm{\theta}) \cdot P(\bm{\theta}|\bm{X}_1, \bm{X}_2, \dots, \bm{X}_{N-1})}{\int_{\bm{\Theta}} P(\bm{X}_N|\bm{\theta}) \cdot P(\bm{\theta}|\bm{X}_1, \bm{X}_2, \dots, \bm{X}_{N-1}) d\bm{\theta}}<h4>贝叶斯学习程序实现</h4><p>假设我们有一个方差$s=3$的正态分布样本集，均值$m$待估计。</p><div>[CODEBLOCK]</div><p>观察迭代结果：</p><div>[IMAGE]</div><p>如果随着样本数增加，后验概率序列逐渐尖锐，最终趋向于以参数真实值为中心的一个尖峰，则这一过程称为贝叶斯学习。</p><h3>正态分布下的贝叶斯估计</h3><p>假设要估计的正态分布均值$\mu$未知，方差$\sigma^2$已知。假定$\mu$的先验分布也是正态分布，均值为$\mu_0$，方差为$\sigma_0^2$。</p><p>假设观测到的$N$个样本的均值为$m$，这里直接给出结论：</p>\hat{\mu} = \frac{N\sigma_0^2}{N\sigma_0^2+\sigma^2}m + \frac{\sigma^2}{N\sigma_0^2+\sigma^2}\mu_0<p>可以看到贝叶斯估计结果由两部分构成，第一项是样本知识，第二项是先验知识。</p><div><p>样本数量为0时，估计值完全等于先验$\mu_0$；样本数量为无穷时，估计值趋于样本均值$m$；</p></div><div><p>若$\sigma_0^2=0$，则先验知识绝对可靠，样本不起作用；</p></div><div><p>若$\sigma_0 \gg \sigma$，则先验知识十分不确定，估计值近似等于样本均值。</p></div><p>贝叶斯估计的优势在于，可以结合样本信息和先验知识，并且根据样本数量和先验知识的确定程度调和两部分信息的相对贡献。</p></div></noscript><div id="root"></div></body></html>