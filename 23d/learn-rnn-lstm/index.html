<!DOCTYPE html><html class="light" lang="zh-CN"><head><meta charset="UTF-8"><meta content="width=device-width,initial-scale=1" name="viewport"><meta content="铃木的网络日记" name="description"><meta content="1kuzus" name="author"><script src="https://www.googletagmanager.com/gtag/js?id=G-45BYSZ6WPY" async></script><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-45BYSZ6WPY"),localStorage.getItem("theme")||localStorage.setItem("theme","light"),document.documentElement.setAttribute("class",localStorage.getItem("theme"))</script><title>学习RNN和LSTM</title><script src="/static/js/main.ee04bf46.js" defer></script><link href="/static/css/main.0d4ce448.css" rel="stylesheet"><link href="https://busuanzi.ibruce.info" rel="preconnect"><link href="https://www.googletagmanager.com" rel="preconnect"><meta content="AymqwRC7u88Y4JPvfIF2F37QKylC04248hLCdJAsh8xgOfe/dVJPV3XS3wLFca1ZMVOtnBfVjaCMTVudWM//5g4AAAB7eyJvcmlnaW4iOiJodHRwczovL3d3dy5nb29nbGV0YWdtYW5hZ2VyLmNvbTo0NDMiLCJmZWF0dXJlIjoiUHJpdmFjeVNhbmRib3hBZHNBUElzIiwiZXhwaXJ5IjoxNjk1MTY3OTk5LCJpc1RoaXJkUGFydHkiOnRydWV9" http-equiv="origin-trial"><link href="http://busuanzi.ibruce.info" rel="preconnect"></head><body><div id="root"><div class="" id="header"><div id="header-left-wrapper"><div id="header-logo-bg"><svg height="36px" viewBox="0 0 1560 1560" width="36px" xmlns="http://www.w3.org/2000/svg"><g fill-rule="evenodd"><path d="M644 97h272.529L1189 780H916.471z" fill="#00A8C4"></path><path d="M98 97h272.84L780 1120.73 1189.162 97H1462L916.438 1462H643.562z" fill="#30303C"></path><path d="M98 1462L643.3 97H916L370.7 1462z" fill="#00F8FF"></path></g></svg></div><div id="header-archive"><h3 id="header-archive-text">归档</h3><div class="header-archive-rightarrow"><svg height="16px" viewBox="0 0 1024 1024" width="16px" xmlns="http://www.w3.org/2000/svg"><path d="M761.056 532.128c.512-.992 1.344-1.824 1.792-2.848 8.8-18.304 5.92-40.704-9.664-55.424L399.936 139.744c-19.264-18.208-49.632-17.344-67.872 1.888-18.208 19.264-17.376 49.632 1.888 67.872l316.96 299.84L335.2 813.632c-19.072 18.4-19.648 48.768-1.248 67.872 9.408 9.792 21.984 14.688 34.56 14.688 12 0 24-4.48 33.312-13.44l350.048-337.376c.672-.672.928-1.6 1.6-2.304.512-.48 1.056-.832 1.568-1.344 2.72-2.848 4.16-6.336 6.016-9.6z" fill="#50505c"></path></svg></div></div></div><div id="header-right-wrapper"><div id="header-theme-bg"><svg height="20px" viewBox="0 0 1024 1024" width="20px" xmlns="http://www.w3.org/2000/svg"><path d="M512 61.44c13.312 0 25.252 7.639 29.942 19.17l27.566 67.972c11.92 29.348-4.178 62.075-35.943 73.094A65.946 65.946 0 01512 225.28c-33.935 0-61.44-25.416-61.44-56.77 0-6.8 1.331-13.558 3.912-19.928l27.586-67.973C486.728 69.08 498.647 61.44 512 61.44zm0 901.12c-13.332 0-25.272-7.639-29.942-19.17l-27.586-67.972a52.961 52.961 0 01-3.912-19.927c0-31.335 27.505-56.771 61.44-56.771a66.28 66.28 0 0121.565 3.604c31.765 11.019 47.862 43.746 35.943 73.114l-27.566 67.953c-4.69 11.53-16.63 19.169-29.942 19.169zM962.56 512c0 13.312-7.639 25.252-19.17 29.942l-67.972 27.566c-29.348 11.92-62.075-4.178-73.094-35.943A65.946 65.946 0 01798.72 512c0-33.935 25.416-61.44 56.77-61.44 6.8 0 13.558 1.331 19.928 3.912l67.973 27.586c11.53 4.67 19.169 16.589 19.169 29.942zm-901.12 0c0-13.332 7.639-25.272 19.17-29.942l67.972-27.586a52.9 52.9 0 0119.927-3.912c31.335 0 56.771 27.505 56.771 61.44a66.28 66.28 0 01-3.604 21.565c-11.019 31.765-43.746 47.862-73.114 35.943l-67.953-27.566C69.08 537.252 61.44 525.312 61.44 512zm131.953-318.587c9.42-9.42 23.265-12.452 34.734-7.618l67.563 28.549c29.184 12.35 40.94 46.858 26.256 77.107a65.946 65.946 0 01-12.698 17.817c-23.982 23.983-61.399 25.457-83.558 3.277a52.961 52.961 0 01-11.346-16.855l-28.55-67.543c-4.853-11.469-1.822-25.313 7.599-34.734zm637.194 637.194c-9.42 9.421-23.265 12.452-34.734 7.598l-67.543-28.549a52.961 52.961 0 01-16.876-11.325c-22.16-22.18-20.685-59.597 3.298-83.579a65.946 65.946 0 0117.817-12.698c30.25-14.684 64.758-2.928 77.107 26.256l28.55 67.563c4.833 11.47 1.802 25.293-7.62 34.734zm0-637.214c9.42 9.42 12.452 23.265 7.618 34.734l-28.549 67.563c-12.35 29.184-46.858 40.94-77.107 26.256a65.946 65.946 0 01-17.817-12.698c-23.983-23.982-25.457-61.399-3.277-83.558 4.792-4.834 10.506-8.663 16.855-11.346l67.543-28.55c11.469-4.853 25.313-1.822 34.734 7.599zM193.393 830.587c-9.421-9.42-12.452-23.265-7.598-34.734l28.549-67.543a52.618 52.618 0 0111.325-16.876c22.18-22.16 59.597-20.685 83.579 3.298a65.842 65.842 0 0112.698 17.817c14.684 30.25 2.928 64.758-26.256 77.107l-67.563 28.55c-11.47 4.833-25.293 1.802-34.734-7.62zM512 737.28c-124.416 0-225.28-100.864-225.28-225.28S387.584 286.72 512 286.72 737.28 387.584 737.28 512 636.416 737.28 512 737.28z" fill="#FCFCFC"></path></svg></div><a href="https://github.com/1kuzus/1kuzus.github.io" rel="noreferrer" target="_blank"><div id="header-github-bg"><svg height="36px" viewBox="0 0 1024 1024" width="36px" xmlns="http://www.w3.org/2000/svg"><path d="M411.306667 831.146667c3.413333-5.12 6.826667-10.24 6.826666-11.946667v-69.973333c-105.813333 22.186667-128-44.373333-128-44.373334-17.066667-44.373333-42.666667-56.32-42.666666-56.32-34.133333-23.893333 3.413333-23.893333 3.413333-23.893333 37.546667 3.413333 58.026667 39.253333 58.026667 39.253333 34.133333 58.026667 88.746667 40.96 110.933333 32.426667 3.413333-23.893333 13.653333-40.96 23.893333-51.2-85.333333-10.24-174.08-42.666667-174.08-187.733333 0-40.96 15.36-75.093333 39.253334-102.4-3.413333-10.24-17.066667-47.786667 3.413333-100.693334 0 0 32.426667-10.24 104.106667 39.253334 30.72-8.533333 63.146667-11.946667 95.573333-11.946667 32.426667 0 64.853333 5.12 95.573333 11.946667 73.386667-49.493333 104.106667-39.253333 104.106667-39.253334 20.48 52.906667 8.533333 90.453333 3.413333 100.693334 23.893333 27.306667 39.253333 59.733333 39.253334 102.4 0 145.066667-88.746667 177.493333-174.08 187.733333 13.653333 11.946667 25.6 34.133333 25.6 69.973333v104.106667c0 3.413333 1.706667 6.826667 6.826666 11.946667 5.12 6.826667 3.413333 18.773333-3.413333 23.893333-3.413333 1.706667-6.826667 3.413333-10.24 3.413333h-174.08c-10.24 0-17.066667-6.826667-17.066667-17.066666 0-5.12 1.706667-8.533333 3.413334-10.24z" fill="#FCFCFC"></path></svg></div></a></div></div><div id="blog-layout"><div id="main"><h1 class="x-title">学习RNN和LSTM</h1><h2 class="x-h1">RNN：一个简单的例子</h2><p class="x-p">传统神经网络每次的输入是独立的，每次输出只依赖于当前的输入；但在某些任务中需要更好的处理序列信息，即前面的输入和后面的输入是有关系的；<span class="x-inline-strong">循环神经网络</span><code class="x-inline-highlight">(Recurrent Neural Networks, RNN)</code>通过使用带自反馈的神经元，能够处理任意长度的序列。</p><p class="x-p">下面是一个非常常见的RNN结构描述图。它展示了RNN的自反馈机制和与时间的依赖关系，但是对网络结构的描述容易引起误解：右侧的展开形式并不意味着网络有<code class="x-inline-highlight">t</code>层，而是反映了随着时间增加（有时也可以理解为随着程序中循环的迭代），上一次输出的隐藏状态，和<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5806em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.2806em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>共同作为网络的下一次的输入。</p><p class="x-p">或者，如果说CNN是从空间维度上堆叠卷积层，不断加深，RNN就是从时间维度上的延展，而其网络真正的参数是很少的。</p><div class="x-image-wrapper x-image-invert"><img alt="img" src="/static/media/rnn1.b3c9ddf03092ac5d3039.png" width="600px"></div><p class="x-p">下面以一个简单的正弦序列预测任务出发，结合代码理解RNN网络的部分细节。</p><h3 class="x-h2">预测一个正弦序列</h3><p class="x-p">这个例子中，我们对一个加了噪声的正弦序列进行预测。</p><div class="x-codeblock"><pre class="language-python" tabindex="0"><code class="language-python"><span class="token keyword">import</span> numpy
<span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt

<span class="token comment">#生成加噪声的正弦序列数据</span>
xlim<span class="token operator">=</span>numpy<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">36</span><span class="token punctuation">,</span><span class="token number">400</span><span class="token punctuation">)</span>
y<span class="token operator">=</span>numpy<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>xlim<span class="token punctuation">)</span><span class="token operator">+</span>numpy<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token operator">*</span>xlim<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token operator">*</span><span class="token number">0.2</span>

<span class="token comment">#转换dtype和size，保持和后面的训练数据统一</span>
y<span class="token operator">=</span>y<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">"float32"</span><span class="token punctuation">)</span>

plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>xlim<span class="token punctuation">,</span>y<span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre></div><p class="x-p">代码中我们在区间<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord">36</span><span class="mclose">]</span></span></span></span>取了<code class="x-inline-highlight">400</code>点数据，如果把横轴看成时间轴，可以认为数据集中有<code class="x-inline-highlight">400</code>个连续时间点的数据。</p><div class="x-image-wrapper x-image-invert"><img alt="img" src="/static/media/fig1.4a575f65ce410d11b945.png" width="600px"></div><p class="x-p no-margin-bottom">现在明确一下我们的方案：</p><div class="x-oli"><div class="x-oli-number">1.</div><div class="x-oli-content-wrapper"><p class="x-p">使用前<code class="x-inline-highlight">80%</code>也就是前<code class="x-inline-highlight">320</code>个数据作为训练集，剩余的作为测试集，观察预测结果。</p></div></div><div class="x-oli"><div class="x-oli-number">2.</div><div class="x-oli-content-wrapper"><p class="x-p">序列长度为<code class="x-inline-highlight">10</code>，也就是模型根据前<code class="x-inline-highlight">10</code>个时间点的数据去预测下一个时间点的数据。</p></div></div><div class="x-oli"><div class="x-oli-number">3.</div><div class="x-oli-content-wrapper"><p class="x-p">这个例子中输入特征的维度是<code class="x-inline-highlight">1</code>，也就是只有<code class="x-inline-highlight">y</code>值一个指标。此外，也不考虑批量大小。</p></div></div><h3 class="x-h2">定义RNN网络</h3><div class="x-codeblock"><pre class="language-python" tabindex="0"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MyRNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>input_size<span class="token punctuation">,</span>hidden_size<span class="token punctuation">,</span>output_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear_ih<span class="token operator">=</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span>hidden_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear_hh<span class="token operator">=</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span>hidden_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear_ho<span class="token operator">=</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span>output_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>tanh<span class="token operator">=</span>nn<span class="token punctuation">.</span>Tanh<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">,</span>prev_state<span class="token punctuation">)</span><span class="token punctuation">:</span>
        curr_state<span class="token operator">=</span>self<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>
            self<span class="token punctuation">.</span>linear_ih<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token operator">+</span>self<span class="token punctuation">.</span>linear_hh<span class="token punctuation">(</span>prev_state<span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
        output<span class="token operator">=</span>self<span class="token punctuation">.</span>linear_ho<span class="token punctuation">(</span>curr_state<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> output<span class="token punctuation">,</span>curr_state

hidden_size<span class="token operator">=</span><span class="token number">12</span>
my_rnn<span class="token operator">=</span>MyRNN<span class="token punctuation">(</span>input_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>hidden_size<span class="token operator">=</span>hidden_size<span class="token punctuation">,</span>output_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
loss_func<span class="token operator">=</span>nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
optimizer<span class="token operator">=</span>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>my_rnn<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>
</code></pre></div><p class="x-p">这个是一个简单的RNN结构，从网络参数和结构来看很像一个<code class="x-inline-highlight">输入层-隐藏层-输出层</code>的感知机，但是多了一步<code class="x-inline-highlight">隐藏层-隐藏层</code>的连接，RNN的反馈结构就是由此体现的。</p><p class="x-p">并且，注意到<code class="x-inline-highlight">forward</code>函数的输入也需要两个参数：当前时刻输入<code class="x-inline-highlight">x</code>和前一时刻状态<code class="x-inline-highlight">prev_state</code>，同时也会把计算后的新状态<code class="x-inline-highlight">curr_state</code>和<code class="x-inline-highlight">output</code>一起返回，供下一次计算使用。在这里，经过<code class="x-inline-highlight">linear_ho</code>后，<code class="x-inline-highlight">output</code>的<code class="x-inline-highlight">size</code>为<code class="x-inline-highlight">(1,1)</code>，考虑到它仅仅是一个标量，我们把它<code class="x-inline-highlight">resize</code>为<code class="x-inline-highlight">(1)</code>。</p><p class="x-p">接下来设置了一些超参数，隐藏层有<code class="x-inline-highlight">12</code>个神经元，损失函数使用<code class="x-inline-highlight">MSELoss()</code>。</p><h3 class="x-h2">训练</h3><p class="x-p">首先定义这样的训练函数：它传入一个序列<code class="x-inline-highlight">train_seq</code>和目标<code class="x-inline-highlight">target</code>。<code class="x-inline-highlight">train_seq</code>的<code class="x-inline-highlight">size</code>应该为<code class="x-inline-highlight">(10,1)</code>，因为我们用前<code class="x-inline-highlight">10</code>个时间点的数据去预测下一个，而输入特征维度是<code class="x-inline-highlight">1</code>；<code class="x-inline-highlight">target</code>的<code class="x-inline-highlight">size</code>应该为<code class="x-inline-highlight">(1)</code>，因为输出只是一个标量。注意我们循环依次输入<code class="x-inline-highlight">train_seq</code>中的<code class="x-inline-highlight">10</code>个数据，迭代更新<code class="x-inline-highlight">state</code>，用最后一次的<code class="x-inline-highlight">output</code>作为最终的输出计算损失。</p><div class="x-codeblock"><pre class="language-python" tabindex="0"><code class="language-python"><span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>train_seq<span class="token punctuation">,</span>target<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment">#初始状态</span>
    state<span class="token operator">=</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>hidden_size<span class="token punctuation">)</span>

    <span class="token keyword">for</span> x <span class="token keyword">in</span> train_seq<span class="token punctuation">:</span>
        output<span class="token punctuation">,</span>state<span class="token operator">=</span>my_rnn<span class="token punctuation">(</span>x<span class="token punctuation">,</span>state<span class="token punctuation">)</span>

    loss<span class="token operator">=</span>loss_func<span class="token punctuation">(</span>output<span class="token punctuation">,</span>target<span class="token punctuation">)</span>
    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment">#返回损失</span>
    <span class="token keyword">return</span> loss<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
</code></pre></div><p class="x-p">下面代码把真实数据划分成：</p><table class="x-table"><tbody><tr><th><p class="x-p"><code class="x-inline-highlight">train_seq</code></p></th><th><p class="x-p"><code class="x-inline-highlight">target</code></p></th></tr><tr><td><p class="x-p"><code class="x-inline-highlight">y[0]</code>,<code class="x-inline-highlight">y[1]</code>,<code class="x-inline-highlight">y[2]</code>, ... ,<code class="x-inline-highlight">y[9]</code></p></td><td><p class="x-p"><code class="x-inline-highlight">y[10]</code></p></td></tr><tr><td><p class="x-p"><code class="x-inline-highlight">y[1]</code>,<code class="x-inline-highlight">y[2]</code>,<code class="x-inline-highlight">y[3]</code>, ... ,<code class="x-inline-highlight">y[10]</code></p></td><td><p class="x-p"><code class="x-inline-highlight">y[11]</code></p></td></tr><tr><td><p class="x-p"><code class="x-inline-highlight">y[2]</code>,<code class="x-inline-highlight">y[3]</code>,<code class="x-inline-highlight">y[4]</code>, ... ,<code class="x-inline-highlight">y[11]</code></p></td><td><p class="x-p"><code class="x-inline-highlight">y[12]</code></p></td></tr><tr><td>...</td><td>...</td></tr><tr><td><p class="x-p"><code class="x-inline-highlight">y[309]</code>,<code class="x-inline-highlight">y[310]</code>,<code class="x-inline-highlight">y[311]</code>, ... ,<code class="x-inline-highlight">y[318]</code></p></td><td><p class="x-p"><code class="x-inline-highlight">y[319]</code></p></td></tr></tbody></table><div class="x-codeblock"><pre class="language-python" tabindex="0"><code class="language-python">train_datas<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">320</span><span class="token operator">-</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    train_seq<span class="token operator">=</span>torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>y<span class="token punctuation">[</span>i<span class="token punctuation">:</span>i<span class="token operator">+</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    target<span class="token operator">=</span>torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>y<span class="token punctuation">[</span>i<span class="token operator">+</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    train_datas<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">(</span>train_seq<span class="token punctuation">,</span>target<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre></div><p class="x-p">训练网络，绘制误差：</p><div class="x-codeblock"><pre class="language-python" tabindex="0"><code class="language-python">metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span>
my_rnn<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> train_seq<span class="token punctuation">,</span>target <span class="token keyword">in</span> train_datas<span class="token punctuation">:</span>
    loss<span class="token operator">=</span>train<span class="token punctuation">(</span>train_seq<span class="token punctuation">,</span>target<span class="token punctuation">)</span>
    metrics<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>

plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">211</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>metrics<span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">"loss"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre></div><h3 class="x-h2">预测</h3><p class="x-p">这个例子中我们用<span class="x-inline-strong">单步预测</span>观察模型的效果。在单步预测时，每次预测都全部使用真实值；当然，我们可以这样做是因为验证集中本来就包含了真实的数据，换句话说，我们是在已知<code class="x-inline-highlight">t+1</code>时刻的真实数据的情况下，去看看模型使用<code class="x-inline-highlight">t-9</code>~<code class="x-inline-highlight">t</code>时刻的数据，对<code class="x-inline-highlight">t+1</code>时刻的预测值。</p><table class="x-table"><tbody><tr><th><p class="x-p"><code class="x-inline-highlight">input</code></p></th><th><p class="x-p"><code class="x-inline-highlight">prediction</code></p></th></tr><tr><td><p class="x-p"><code class="x-inline-highlight">y[310]</code>,<code class="x-inline-highlight">y[311]</code>,<code class="x-inline-highlight">y[312]</code>, ... ,<code class="x-inline-highlight">y[319]</code></p></td><td><p class="x-p"><code class="x-inline-highlight">pred[320]</code></p></td></tr><tr><td><p class="x-p"><code class="x-inline-highlight">y[311]</code>,<code class="x-inline-highlight">y[312]</code>,<code class="x-inline-highlight">y[313]</code>, ... ,<code class="x-inline-highlight">y[320]</code></p></td><td><p class="x-p"><code class="x-inline-highlight">pred[321]</code></p></td></tr><tr><td><p class="x-p"><code class="x-inline-highlight">y[312]</code>,<code class="x-inline-highlight">y[313]</code>,<code class="x-inline-highlight">y[314]</code>, ... ,<code class="x-inline-highlight">y[321]</code></p></td><td><p class="x-p"><code class="x-inline-highlight">pred[322]</code></p></td></tr><tr><td>...</td><td>...</td></tr></tbody></table><div class="x-codeblock"><pre class="language-python" tabindex="0"><code class="language-python"><span class="token keyword">def</span> <span class="token function">pred</span><span class="token punctuation">(</span>truth_seq<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        state<span class="token operator">=</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>hidden_size<span class="token punctuation">)</span>
        <span class="token keyword">for</span> x <span class="token keyword">in</span> truth_seq<span class="token punctuation">:</span>
            output<span class="token punctuation">,</span>state<span class="token operator">=</span>my_rnn<span class="token punctuation">(</span>x<span class="token punctuation">,</span>state<span class="token punctuation">)</span>
        <span class="token keyword">return</span> output

preds<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">320</span><span class="token operator">-</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">400</span><span class="token operator">-</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    truth_seq<span class="token operator">=</span>torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>y<span class="token punctuation">[</span>i<span class="token punctuation">:</span>i<span class="token operator">+</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    preds<span class="token punctuation">.</span>append<span class="token punctuation">(</span>pred<span class="token punctuation">(</span>truth_seq<span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
preds<span class="token operator">=</span>numpy<span class="token punctuation">.</span>array<span class="token punctuation">(</span>preds<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>

plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">212</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>xlim<span class="token punctuation">,</span>y<span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">"truth"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>xlim<span class="token punctuation">[</span><span class="token number">320</span><span class="token punctuation">:</span><span class="token number">400</span><span class="token punctuation">]</span><span class="token punctuation">,</span>preds<span class="token punctuation">,</span><span class="token string">"red"</span><span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">"predict"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre></div><p class="x-p">如果我们不只是在测试集上评估模型性能，而是去预测真实生活中的问题，例如未来<code class="x-inline-highlight">7</code>天的温度；或者假如我们的数据集到<code class="x-inline-highlight">y[319]</code>就截止了，这时如果想得到后面多个时刻的数据，就需要<span class="x-inline-strong">多步预测</span>，此时上一时刻的预测会被当做新的输入：</p><table class="x-table"><tbody><tr><th><p class="x-p"><code class="x-inline-highlight">input</code></p></th><th><p class="x-p"><code class="x-inline-highlight">prediction</code></p></th></tr><tr><td><p class="x-p"><code class="x-inline-highlight">y[310]</code>,<code class="x-inline-highlight">y[311]</code>,<code class="x-inline-highlight">y[312]</code>, ... ,<code class="x-inline-highlight">y[319]</code></p></td><td><p class="x-p"><code class="x-inline-highlight">pred[320]</code></p></td></tr><tr><td><p class="x-p"><code class="x-inline-highlight">y[311]</code>,<code class="x-inline-highlight">y[312]</code>,<code class="x-inline-highlight">y[313]</code>, ... ,<code class="x-inline-highlight">pred[320]</code></p></td><td><p class="x-p"><code class="x-inline-highlight">pred[321]</code></p></td></tr><tr><td><p class="x-p"><code class="x-inline-highlight">y[312]</code>,<code class="x-inline-highlight">y[313]</code>,<code class="x-inline-highlight">y[314]</code>, ... ,<code class="x-inline-highlight">pred[320]</code>,<code class="x-inline-highlight">pred[321]</code></p></td><td><p class="x-p"><code class="x-inline-highlight">pred[322]</code></p></td></tr><tr><td>...</td><td>...</td></tr></tbody></table><p class="x-p">多步预测会导致误差的累积。</p><h3 class="x-h2">看看效果</h3><p class="x-p">如果不执行训练步骤的代码，使用初始随机参数的模型预测结果是：</p><div class="x-image-wrapper x-image-invert"><img alt="img" src="/static/media/fig2.830b5ae23a383516d9b4.png" width="600px"></div><p class="x-p">经过训练后，每次训练的<code class="x-inline-highlight">loss</code>和最终的预测：</p><div class="x-image-wrapper x-image-invert"><img alt="img" src="/static/media/fig3.2cfaa1d28f3c1df226e6.png" width="600px"></div><h3 class="x-h2">使用torch.nn.RNN</h3><p class="x-p">使用<code class="x-inline-highlight">torch.nn.RNN</code>模块时，与上面例子中手动实现的RNN有几处细小的区别，下面给出了使用<code class="x-inline-highlight">torch.nn.RNN</code>时需要做出的修改：</p><div class="x-oli"><div class="x-oli-number">1.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom">定义模型时，不再需要显式指定<code class="x-inline-highlight">linear_ih</code>和<code class="x-inline-highlight">linear_hh</code>两层，将由<code class="x-inline-highlight">nn.RNN</code>模块实现；<code class="x-inline-highlight">nn.RNN</code>模块没有定义输出层，因此输出层<code class="x-inline-highlight">linear_ho</code>需要设置。<br>在<code class="x-inline-highlight">forward</code>函数中，手动实现时为了直观展示出RNN的迭代过程，只进行了一次隐藏状态的更新；而对于输入序列迭代更新隐藏状态是在训练和预测时实现的。而<code class="x-inline-highlight">nn.RNN</code>模块的一次<code class="x-inline-highlight">forward</code>就已经完成了迭代更新，其输入是整个序列<code class="x-inline-highlight">seq</code>和<code class="x-inline-highlight">prev_state</code>，返回值是<code class="x-inline-highlight">output_hidden,curr_state</code>，对于不考虑批量大小的数据，它们的<code class="x-inline-highlight">size</code>为：</p><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><code class="x-inline-highlight">seq</code>: <code class="x-inline-highlight">(sequence_length, input_size)</code></p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><code class="x-inline-highlight">init_state</code>: <code class="x-inline-highlight">(1, hidden_size)</code></p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><code class="x-inline-highlight">output_hidden</code>: <code class="x-inline-highlight">(sequence_length, hidden_size)</code></p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><code class="x-inline-highlight">state</code>: <code class="x-inline-highlight">(1, hidden_size)</code></p></div></div><p class="x-p">最后在我们定义的<code class="x-inline-highlight">MyRNN</code>模块中，用<code class="x-inline-highlight">output_hidden</code>的最后一个时间点的输出，经过输出层得到最终的<code class="x-inline-highlight">output</code>。</p></div></div><div class="x-highlightblock highlight-background-gray"><h4 class="x-h3">手动实现</h4><div class="x-codeblock"><pre class="language-python" tabindex="0"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MyRNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>input_size<span class="token punctuation">,</span>hidden_size<span class="token punctuation">,</span>output_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear_ih<span class="token operator">=</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span>hidden_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear_hh<span class="token operator">=</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span>hidden_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear_ho<span class="token operator">=</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span>output_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>tanh<span class="token operator">=</span>nn<span class="token punctuation">.</span>Tanh<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">,</span>prev_state<span class="token punctuation">)</span><span class="token punctuation">:</span>
        curr_state<span class="token operator">=</span>self<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>
            self<span class="token punctuation">.</span>linear_ih<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token operator">+</span>self<span class="token punctuation">.</span>linear_hh<span class="token punctuation">(</span>prev_state<span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
        output<span class="token operator">=</span>self<span class="token punctuation">.</span>linear_ho<span class="token punctuation">(</span>curr_state<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> output<span class="token punctuation">,</span>curr_state
</code></pre></div><h4 class="x-h3">使用torch.nn.RNN</h4><div class="x-codeblock"><pre class="language-python" tabindex="0"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MyRNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>input_size<span class="token punctuation">,</span>hidden_size<span class="token punctuation">,</span>output_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>rnn<span class="token operator">=</span>nn<span class="token punctuation">.</span>RNN<span class="token punctuation">(</span>input_size<span class="token operator">=</span>input_size<span class="token punctuation">,</span>hidden_size<span class="token operator">=</span>hidden_size<span class="token punctuation">,</span>batch_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear_ho<span class="token operator">=</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span>output_size<span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>seq<span class="token punctuation">,</span>init_state<span class="token punctuation">)</span><span class="token punctuation">:</span>
        output_hidden<span class="token punctuation">,</span>tate<span class="token operator">=</span>self<span class="token punctuation">.</span>rnn<span class="token punctuation">(</span>seq<span class="token punctuation">,</span>init_state<span class="token punctuation">)</span>
        output<span class="token operator">=</span>self<span class="token punctuation">.</span>linear_ho<span class="token punctuation">(</span>output_hidden<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment">#取最后一个时间点的输出</span>
        <span class="token keyword">return</span> output<span class="token punctuation">,</span>state
</code></pre></div></div><div class="x-oli"><div class="x-oli-number">2.</div><div class="x-oli-content-wrapper"><p class="x-p">训练和预测时，也不需要再遍历序列，迭代的过程已经在<code class="x-inline-highlight">nn.RNN</code>模块内部实现。</p></div></div><div class="x-highlightblock highlight-background-gray"><h4 class="x-h3">手动实现</h4><div class="x-codeblock"><pre class="language-python" tabindex="0"><code class="language-python"><span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>train_seq<span class="token punctuation">,</span>target<span class="token punctuation">)</span><span class="token punctuation">:</span>
    state<span class="token operator">=</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>hidden_size<span class="token punctuation">)</span>
    <span class="token keyword">for</span> x <span class="token keyword">in</span> train_seq<span class="token punctuation">:</span>
        output<span class="token punctuation">,</span>state<span class="token operator">=</span>my_rnn<span class="token punctuation">(</span>x<span class="token punctuation">,</span>state<span class="token punctuation">)</span>
    loss<span class="token operator">=</span>loss_func<span class="token punctuation">(</span>output<span class="token punctuation">,</span>target<span class="token punctuation">)</span>
    <span class="token comment"># ......</span>
</code></pre></div><div class="x-codeblock"><pre class="language-python" tabindex="0"><code class="language-python"><span class="token keyword">def</span> <span class="token function">pred</span><span class="token punctuation">(</span>truth_seq<span class="token punctuation">,</span>net<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        state<span class="token operator">=</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>hidden_size<span class="token punctuation">)</span>
        <span class="token keyword">for</span> x <span class="token keyword">in</span> truth_seq<span class="token punctuation">:</span>
            output<span class="token punctuation">,</span>state<span class="token operator">=</span>net<span class="token punctuation">(</span>x<span class="token punctuation">,</span>state<span class="token punctuation">)</span>
        <span class="token keyword">return</span> output
</code></pre></div><h4 class="x-h3">使用torch.nn.RNN</h4><div class="x-codeblock"><pre class="language-python" tabindex="0"><code class="language-python"><span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>train_seq<span class="token punctuation">,</span>target<span class="token punctuation">)</span><span class="token punctuation">:</span>
    state<span class="token operator">=</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>hidden_size<span class="token punctuation">)</span>
    output<span class="token punctuation">,</span>_<span class="token operator">=</span>my_rnn<span class="token punctuation">(</span>train_seq<span class="token punctuation">,</span>state<span class="token punctuation">)</span> <span class="token comment">#一次得到输出</span>
    loss<span class="token operator">=</span>loss_func<span class="token punctuation">(</span>output<span class="token punctuation">,</span>target<span class="token punctuation">)</span>
    <span class="token comment"># ......</span>
</code></pre></div><div class="x-codeblock"><pre class="language-python" tabindex="0"><code class="language-python"><span class="token keyword">def</span> <span class="token function">pred</span><span class="token punctuation">(</span>truth_seq<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        state<span class="token operator">=</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>hidden_size<span class="token punctuation">)</span>
        output<span class="token punctuation">,</span>_<span class="token operator">=</span>my_rnn<span class="token punctuation">(</span>truth_seq<span class="token punctuation">,</span>state<span class="token punctuation">)</span> <span class="token comment">#一次得到输出</span>
        <span class="token keyword">return</span> output
</code></pre></div></div><h3 class="x-h2">FAQ</h3><p class="x-p">初次了解RNN时，我在一些问题上困惑了很久。这个版块是对它们的再次整理。（尽管有些已经包含在上述例子中了！）</p><h4 class="x-h3">用10步预测下1步，为什么 input_size 不是10，而是1？</h4><p class="x-p"><code class="x-inline-highlight">input_size</code>与序列长度并非同一个概念。用前<code class="x-inline-highlight">10</code>个时间点的数据去预测下一个，这里的<code class="x-inline-highlight">10</code>是序列长度；而<code class="x-inline-highlight">input_size</code>是输入特征的维度。由于这个例子较为简单，只是用历史的<code class="x-inline-highlight">y</code>值预测新的<code class="x-inline-highlight">y</code>值，因此特征只有<code class="x-inline-highlight">1</code>维。</p><h4 class="x-h3">什么时候 input_size 不是1？</h4><p class="x-p">例如我们在预测未来气温时，历史气温数据并不是唯一的参考，还可能参考历史的风速、气压、天气情况等等，此时输入数据将会是一个<code class="x-inline-highlight">input_size</code>维的向量。</p><h4 class="x-h3">hidden_size=12，12是在哪里体现的？</h4><p class="x-p"><code class="x-inline-highlight">12</code>只是模型的超参数，和MLP中隐藏层大小一样，并没有太多的物理含义。</p><h4 class="x-h3">训练时，每次迭代用哪些数据？应该遍历几遍数据集？每个 epoch 会使用哪些数据进行参数优化？</h4><p class="x-p no-margin-bottom">在训练一个CNN网络时（例如一个图片分类网络），策略通常是：</p><div class="x-oli"><div class="x-oli-number">1.</div><div class="x-oli-content-wrapper"><p class="x-p">指定超参数<code class="x-inline-highlight">num_epoch</code>，在每个<code class="x-inline-highlight">epoch</code>中随机遍历训练集中的所有图像进行参数优化；</p></div></div><div class="x-oli"><div class="x-oli-number">2.</div><div class="x-oli-content-wrapper"><p class="x-p">重复执行<code class="x-inline-highlight">num_epoch</code>次。</p></div></div><p class="x-p with-margin-top no-margin-bottom">然而对于RNN来说这个概念似乎并不清晰，例如上述例子的训练策略是：</p><div class="x-oli"><div class="x-oli-number">1.</div><div class="x-oli-content-wrapper"><p class="x-p">从<code class="x-inline-highlight">0</code>到<code class="x-inline-highlight">(训练集大小 - 序列长度)</code>依次遍历起始时间<code class="x-inline-highlight">t</code>；</p></div></div><div class="x-oli"><div class="x-oli-number">2.</div><div class="x-oli-content-wrapper"><p class="x-p">对于每个起始时间<code class="x-inline-highlight">t</code>，将<code class="x-inline-highlight">y[t]</code>~<code class="x-inline-highlight">y[t+9]</code>为输入，<code class="x-inline-highlight">y[t+10]</code>为真值作为一组训练样本。</p></div></div><div class="x-oli"><div class="x-oli-number">3.</div><div class="x-oli-content-wrapper"><p class="x-p">第<code class="x-inline-highlight">1</code>步只遍历了一次！</p></div></div><p class="x-p with-margin-top no-margin-bottom">或者：</p><p class="x-p no-margin-bottom">...</p><div class="x-oli"><div class="x-oli-number">3.</div><div class="x-oli-content-wrapper"><p class="x-p">前两步同上，但多次遍历训练集。</p></div></div><p class="x-p with-margin-top no-margin-bottom">另一个常用的策略是：</p><div class="x-oli"><div class="x-oli-number">1.</div><div class="x-oli-content-wrapper"><p class="x-p">指定超参数：训练轮次<code class="x-inline-highlight">num_iter</code>；</p></div></div><div class="x-oli"><div class="x-oli-number">2.</div><div class="x-oli-content-wrapper"><p class="x-p">重复执行<code class="x-inline-highlight">num_iter</code>次，每次随机抽取一个起始时间<code class="x-inline-highlight">t</code>，并且将<code class="x-inline-highlight">y[t]</code>~<code class="x-inline-highlight">y[t+9]</code>为输入，<code class="x-inline-highlight">y[t+10]</code>为真值作为一组训练样本。</p></div></div><h2 class="x-h1">LSTM：一篇很好的博客</h2><p class="x-p">以下的内容和插图总结或翻译自这篇的英文博客：<a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" class="x-inline-link" rel="noreferrer" target="_blank">Understanding LSTM Networks</a></p><h3 class="x-h2">长期依赖问题</h3><p class="x-p">RNN可以利用先前的信息理解当前的任务，这点非常不错；有时我们只需要短期的信息，例如一个语言模型预测下面的句子：<br><code class="x-inline-highlight">天空中飘着一朵白色的【云】</code>，这很简单。但有些时候我们需要更多背景信息，例如：<br><code class="x-inline-highlight">我出生在法国，…… ，我可以说流利的【法语】</code>，这个情况下，随着前后文距离变大，RNN对长期依赖关系的学习会变得困难。</p><h3 class="x-h2">LSTM</h3><p class="x-p"><span class="x-inline-strong">长短期记忆网络</span><code class="x-inline-highlight">(Long Short-Term Memory, LSTM)</code>是一种特殊的RNN，可以学习长期依赖。以RNN为例，循环神经网络随时间展开通常具有如下的示意图：</p><div class="x-image-wrapper x-image-invert"><img alt="img" src="/static/media/rnn2.fa69f5128a81f0a65cb5.png" width="600px"></div><p class="x-p">对于RNN来说，利用历史状态和输入得到新的状态，只经过一个简单的<code class="x-inline-highlight">tanh</code>激活层，而对于LSTM来说，它的示意图略显复杂：</p><div class="x-image-wrapper x-image-invert"><img alt="img" src="/static/media/lstm1.8d9261d2af3d24b9b049.png" width="600px"></div><p class="x-p">在上图中，每条线表示一个向量，粉红色圆圈表示逐点式操作，黄色的方框是神经网络的层。这看起来很眼晕，不过我们接下来会一点点的解释图里的内容。</p><h3 class="x-h2">门控单元</h3><p class="x-p">下面的结构称为门控单元：</p><div class="x-image-wrapper x-image-invert"><img alt="img" src="data:image/png;base64,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" width="100px"></div><p class="x-p">门控单元控制信息量通过的多少，通过向量<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.4306em"></span><span class="mord mathnormal" style="margin-right:.04398em">z</span></span></span></span>来控制<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.4306em"></span><span class="mord mathnormal">x</span></span></span></span>通过的信息量：</p><div class="x-formula"><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.4306em"></span><span class="mord mathnormal">o</span><span class="mspace" style="margin-right:.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.0359em">σ</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.044em">z</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2222em"></span><span class="mbin">⊗</span><span class="mspace" style="margin-right:.2222em"></span></span><span class="base"><span class="strut" style="height:.4306em"></span><span class="mord mathnormal">x</span></span></span></span></div><p class="x-p">式子中<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6667em;vertical-align:-.0833em"></span><span class="mord">⊗</span></span></span></span>表示按位置相乘，<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.03588em">σ</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.04398em">z</span><span class="mclose">)</span></span></span></span>的每个元素输出范围是<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span>，某个元素接近<code class="x-inline-highlight">1</code>，<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.4306em"></span><span class="mord mathnormal">x</span></span></span></span>对应位置保留的信息就越多，反之就越少。</p><h3 class="x-h2">逐部分分析LSTM</h3><h4 class="x-h3">遗忘门</h4><p class="x-p">LSTM的第一步是决定什么应该被遗忘，也就是对上一个<span class="x-inline-strong">单元</span><code class="x-inline-highlight">(cell)</code>状态信息选择性的遗忘。<br>这个操作由遗忘门<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8889em;vertical-align:-.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.2806em"><span style="top:-2.55em;margin-left:-.1076em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>实现，将其<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.1667em"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span>范围的输出按位置与单元上一时刻状态相乘。</p><div class="x-image-wrapper x-image-invert"><img alt="img" src="/static/media/lstm3.8d78b5d9dd1eba978c83.png" width="600px"></div><div class="x-highlightblock highlight-background-gray"><p class="x-p">举一个概念性的例子：</p><p class="x-p">考虑一个语言模型，输入一个句子：<code class="x-inline-highlight">Alice是一名女教师，她喜欢给学生讲课；Bob是一位男司机，他喝酒上瘾。</code></p><p class="x-p">当模型看到<code class="x-inline-highlight">Alice是一名女教师，……</code>时，单元状态中可能存储了和主语<code class="x-inline-highlight">Alice</code>和<code class="x-inline-highlight">女教师</code>有关的语义信息，以便在后文输出合适的代词<code class="x-inline-highlight">她</code>；然后，当模型看到<code class="x-inline-highlight">Alice是一名女教师，她喜欢给学生讲课；Bob是一位男司机，……</code>时，我们希望在看到新主语<code class="x-inline-highlight">Bob</code>和<code class="x-inline-highlight">男司机</code>之后，忘记此前存储的旧主语的性别语义。也就是对旧单元状态<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8917em;vertical-align:-.2083em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3011em"><span style="top:-2.55em;margin-left:-.0715em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.2083em"><span></span></span></span></span></span></span></span></span></span>乘上较小的<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8889em;vertical-align:-.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.2806em"><span style="top:-2.55em;margin-left:-.1076em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>。</p></div><h4 class="x-h3">输入门</h4><p class="x-p">下一步就是决定要在单元中存入什么新的信息。这一部分有两路：<code class="x-inline-highlight">tanh</code>这一路与普通RNN很像，生成一个中间状态；<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.4306em"></span><span class="mord mathnormal" style="margin-right:.03588em">σ</span></span></span></span>这一路被称为输入门<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8095em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">i</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.2806em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>，控制这个中间状态有多少信息被存入单元。</p><div class="x-image-wrapper x-image-invert"><img alt="img" src="/static/media/lstm4.f3b5a4b013cfb0961f16.png" width="600px"></div><p class="x-p">经历这两步之后，便可以相加得到新的单元状态：</p><div class="x-image-wrapper x-image-invert"><img alt="img" src="/static/media/lstm5.77488b91d8ee0ab88150.png" width="600px"></div><div class="x-highlightblock highlight-background-gray"><p class="x-p">同理，当模型看到<code class="x-inline-highlight">Bob是一位男司机</code>时，我们可能会想丢掉此前的语义信息<code class="x-inline-highlight">女性</code>，并把新的语义信息<code class="x-inline-highlight">男性</code>存入单元状态，使得后文输出正确的代词<code class="x-inline-highlight">他</code>。</p></div><h4 class="x-h3">输出门</h4><p class="x-p">最后是决定新的隐藏状态，这个输出会基于单元状态，但会经过门控单元。输出门<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.5806em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.2806em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>决定经过<code class="x-inline-highlight">tanh</code>的单元状态<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.2806em"><span style="top:-2.55em;margin-left:-.0715em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>有多少被输出到下一时刻的隐藏状态。</p><div class="x-image-wrapper x-image-invert"><img alt="img" src="/static/media/lstm6.92c381737b838cd6a865.png" width="600px"></div><div class="x-highlightblock highlight-background-gray"><p class="x-p">当看到<code class="x-inline-highlight">Bob是一位男司机，他……</code>时，由于出现了主语<code class="x-inline-highlight">他</code>，模型可能会输出和<code class="x-inline-highlight">谓语动词</code>有关的语义信息。</p></div></div><div id="contents"><h4 id="contents-header">本页目录</h4><ul><li class="titletype-x-h1 active">RNN：一个简单的例子</li><li class="titletype-x-h2">预测一个正弦序列</li><li class="titletype-x-h2">定义RNN网络</li><li class="titletype-x-h2">训练</li><li class="titletype-x-h2">预测</li><li class="titletype-x-h2">看看效果</li><li class="titletype-x-h2">使用torch.nn.RNN</li><li class="titletype-x-h2">FAQ</li><li class="titletype-x-h1">LSTM：一篇很好的博客</li><li class="titletype-x-h2">长期依赖问题</li><li class="titletype-x-h2">LSTM</li><li class="titletype-x-h2">门控单元</li><li class="titletype-x-h2">逐部分分析LSTM</li></ul></div><div id="sidebar"><div class="sidebar-list show-list"><div class="sidebar-list-head"><h3 class="sidebar-list-category">网络杂识 (4)</h3><div class="sidebar-list-category-rightarrow"><svg height="16px" viewBox="0 0 1024 1024" width="16px" xmlns="http://www.w3.org/2000/svg"><path d="M761.056 532.128c.512-.992 1.344-1.824 1.792-2.848 8.8-18.304 5.92-40.704-9.664-55.424L399.936 139.744c-19.264-18.208-49.632-17.344-67.872 1.888-18.208 19.264-17.376 49.632 1.888 67.872l316.96 299.84L335.2 813.632c-19.072 18.4-19.648 48.768-1.248 67.872 9.408 9.792 21.984 14.688 34.56 14.688 12 0 24-4.48 33.312-13.44l350.048-337.376c.672-.672.928-1.6 1.6-2.304.512-.48 1.056-.832 1.568-1.344 2.72-2.848 4.16-6.336 6.016-9.6z" fill="#50505c"></path></svg></div></div><div class="sidebar-list-ul-wrapper"><ul class="sidebar-list-ul"><li class="sidebar-list-li"><a href="/23d/database-3nf/" class="sidebar-list-link"><span class="sidebar-list-title">数据库设计三大范式</span></a></li><li class="sidebar-list-li"><a href="/23d/github-linguist-vendored/" class="sidebar-list-link"><span class="sidebar-list-title">不统计Github仓库某个目录下的语言</span></a></li><li class="sidebar-list-li"><a href="/24a/deepl-shortcut-setting/" class="sidebar-list-link"><span class="sidebar-list-title">解决：DeepL该快捷键已被使用</span></a></li><li class="sidebar-list-li"><a href="/24a/git-merge-allow-unrelated-histories/" class="sidebar-list-link"><span class="sidebar-list-title">记录：使用--allow-unrelated-histories</span></a></li></ul></div></div><div class="sidebar-list show-list"><div class="sidebar-list-head"><h3 class="sidebar-list-category">深度学习 (5)</h3><div class="sidebar-list-category-rightarrow"><svg height="16px" viewBox="0 0 1024 1024" width="16px" xmlns="http://www.w3.org/2000/svg"><path d="M761.056 532.128c.512-.992 1.344-1.824 1.792-2.848 8.8-18.304 5.92-40.704-9.664-55.424L399.936 139.744c-19.264-18.208-49.632-17.344-67.872 1.888-18.208 19.264-17.376 49.632 1.888 67.872l316.96 299.84L335.2 813.632c-19.072 18.4-19.648 48.768-1.248 67.872 9.408 9.792 21.984 14.688 34.56 14.688 12 0 24-4.48 33.312-13.44l350.048-337.376c.672-.672.928-1.6 1.6-2.304.512-.48 1.056-.832 1.568-1.344 2.72-2.848 4.16-6.336 6.016-9.6z" fill="#50505c"></path></svg></div></div><div class="sidebar-list-ul-wrapper"><ul class="sidebar-list-ul"><li class="sidebar-list-li"><a href="/longtime/papers/" class="sidebar-list-link"><span class="sidebar-list-title">论文速记</span></a></li><li class="sidebar-list-li"><a href="/23d/r2plus1d/" class="sidebar-list-link"><span class="sidebar-list-title">行为识别R(2+1)D网络</span></a></li><li class="sidebar-list-li"><a href="/23d/object-detection-map/" class="sidebar-list-link"><span class="sidebar-list-title">目标检测评价指标mAP</span></a></li><li class="sidebar-list-li active"><a href="/23d/learn-rnn-lstm/" class="sidebar-list-link"><span class="sidebar-list-title">学习RNN和LSTM</span></a></li><li class="sidebar-list-li"><a href="/24a/reproduce-nerf-rpn/" class="sidebar-list-link"><span class="sidebar-list-title">记录：复现NeRF-RPN代码</span></a></li></ul></div></div><div class="sidebar-list show-list"><div class="sidebar-list-head"><h3 class="sidebar-list-category">Python学习 (1)</h3><div class="sidebar-list-category-rightarrow"><svg height="16px" viewBox="0 0 1024 1024" width="16px" xmlns="http://www.w3.org/2000/svg"><path d="M761.056 532.128c.512-.992 1.344-1.824 1.792-2.848 8.8-18.304 5.92-40.704-9.664-55.424L399.936 139.744c-19.264-18.208-49.632-17.344-67.872 1.888-18.208 19.264-17.376 49.632 1.888 67.872l316.96 299.84L335.2 813.632c-19.072 18.4-19.648 48.768-1.248 67.872 9.408 9.792 21.984 14.688 34.56 14.688 12 0 24-4.48 33.312-13.44l350.048-337.376c.672-.672.928-1.6 1.6-2.304.512-.48 1.056-.832 1.568-1.344 2.72-2.848 4.16-6.336 6.016-9.6z" fill="#50505c"></path></svg></div></div><div class="sidebar-list-ul-wrapper"><ul class="sidebar-list-ul"><li class="sidebar-list-li"><a href="/24a/torch-numpy-topk/" class="sidebar-list-link"><span class="sidebar-list-title">在pytorch和numpy中取top-k值和索引</span></a></li></ul></div></div><div class="sidebar-list show-list"><div class="sidebar-list-head"><h3 class="sidebar-list-category">前端与JavaScript (2)</h3><div class="sidebar-list-category-rightarrow"><svg height="16px" viewBox="0 0 1024 1024" width="16px" xmlns="http://www.w3.org/2000/svg"><path d="M761.056 532.128c.512-.992 1.344-1.824 1.792-2.848 8.8-18.304 5.92-40.704-9.664-55.424L399.936 139.744c-19.264-18.208-49.632-17.344-67.872 1.888-18.208 19.264-17.376 49.632 1.888 67.872l316.96 299.84L335.2 813.632c-19.072 18.4-19.648 48.768-1.248 67.872 9.408 9.792 21.984 14.688 34.56 14.688 12 0 24-4.48 33.312-13.44l350.048-337.376c.672-.672.928-1.6 1.6-2.304.512-.48 1.056-.832 1.568-1.344 2.72-2.848 4.16-6.336 6.016-9.6z" fill="#50505c"></path></svg></div></div><div class="sidebar-list-ul-wrapper"><ul class="sidebar-list-ul"><li class="sidebar-list-li"><a href="/23c/js-array/" class="sidebar-list-link"><span class="sidebar-list-title">JavaScript数组常用方法</span></a></li><li class="sidebar-list-li"><a href="/23d/css-auto-height-transition/" class="sidebar-list-link"><span class="sidebar-list-title">CSS实现auto高度的过渡动画</span></a></li></ul></div></div><div class="sidebar-list show-list"><div class="sidebar-list-head"><h3 class="sidebar-list-category">课程 (10)</h3><div class="sidebar-list-category-rightarrow"><svg height="16px" viewBox="0 0 1024 1024" width="16px" xmlns="http://www.w3.org/2000/svg"><path d="M761.056 532.128c.512-.992 1.344-1.824 1.792-2.848 8.8-18.304 5.92-40.704-9.664-55.424L399.936 139.744c-19.264-18.208-49.632-17.344-67.872 1.888-18.208 19.264-17.376 49.632 1.888 67.872l316.96 299.84L335.2 813.632c-19.072 18.4-19.648 48.768-1.248 67.872 9.408 9.792 21.984 14.688 34.56 14.688 12 0 24-4.48 33.312-13.44l350.048-337.376c.672-.672.928-1.6 1.6-2.304.512-.48 1.056-.832 1.568-1.344 2.72-2.848 4.16-6.336 6.016-9.6z" fill="#50505c"></path></svg></div></div><div class="sidebar-list-ul-wrapper"><ul class="sidebar-list-ul"><li class="sidebar-list-li"><a href="/23c/pattern-recognition-1/" class="sidebar-list-link"><span class="sidebar-list-title">【模式识别】统计决策方法</span></a></li><li class="sidebar-list-li"><a href="/23c/pattern-recognition-2/" class="sidebar-list-link"><span class="sidebar-list-title">【模式识别】参数估计</span></a></li><li class="sidebar-list-li"><a href="/23c/pattern-recognition-3/" class="sidebar-list-link"><span class="sidebar-list-title">【模式识别】非参数估计</span></a></li><li class="sidebar-list-li"><a href="/23d/pattern-recognition-4/" class="sidebar-list-link"><span class="sidebar-list-title">【模式识别】线性学习器与线性分类器</span></a></li><li class="sidebar-list-li"><a href="/23d/protocols/" class="sidebar-list-link"><span class="sidebar-list-title">【计算机网络】协议总结</span></a></li><li class="sidebar-list-li"><a href="/24a/machine-learning-exercises/" class="sidebar-list-link"><span class="sidebar-list-title">【机器学习】习题</span></a></li><li class="sidebar-list-li"><a href="/24a/games101-01-transformation/" class="sidebar-list-link"><span class="sidebar-list-title">【GAMES101】Transformation</span></a></li><li class="sidebar-list-li"><a href="/24a/games101-02-rasterization/" class="sidebar-list-link"><span class="sidebar-list-title">【GAMES101】Rasterization</span></a></li><li class="sidebar-list-li"><a href="/24a/games101-03-shading/" class="sidebar-list-link"><span class="sidebar-list-title">【GAMES101】Shading</span></a></li><li class="sidebar-list-li"><a href="/24a/games101-04-geometry/" class="sidebar-list-link"><span class="sidebar-list-title">【GAMES101】Geometry</span></a></li></ul></div></div><div class="sidebar-list show-list"><div class="sidebar-list-head"><h3 class="sidebar-list-category">其他 (2)</h3><div class="sidebar-list-category-rightarrow"><svg height="16px" viewBox="0 0 1024 1024" width="16px" xmlns="http://www.w3.org/2000/svg"><path d="M761.056 532.128c.512-.992 1.344-1.824 1.792-2.848 8.8-18.304 5.92-40.704-9.664-55.424L399.936 139.744c-19.264-18.208-49.632-17.344-67.872 1.888-18.208 19.264-17.376 49.632 1.888 67.872l316.96 299.84L335.2 813.632c-19.072 18.4-19.648 48.768-1.248 67.872 9.408 9.792 21.984 14.688 34.56 14.688 12 0 24-4.48 33.312-13.44l350.048-337.376c.672-.672.928-1.6 1.6-2.304.512-.48 1.056-.832 1.568-1.344 2.72-2.848 4.16-6.336 6.016-9.6z" fill="#50505c"></path></svg></div></div><div class="sidebar-list-ul-wrapper"><ul class="sidebar-list-ul"><li class="sidebar-list-li"><a href="/longtime/demo/" class="sidebar-list-link"><span class="sidebar-list-title">示例</span></a></li><li class="sidebar-list-li"><a href="/longtime/updates/" class="sidebar-list-link"><span class="sidebar-list-title">更新日志</span></a></li></ul></div></div></div><div id="sidebar-mask"></div></div></div></body></html>