<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/css/8ac0b0e7f508d966.css" crossorigin="" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/c84b60895b980f14.css" crossorigin="" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/f70c26425a044d6b.css" crossorigin="" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-c5d6fd738a8d0587.js" crossorigin=""/><script src="/_next/static/chunks/fd9d1056-317889f780d115a2.js" async="" crossorigin=""></script><script src="/_next/static/chunks/472-d5046572d4bcdf16.js" async="" crossorigin=""></script><script src="/_next/static/chunks/main-app-cfdfd0e1c1afc4e3.js" async="" crossorigin=""></script><script src="/_next/static/chunks/app/layout-e12e822557cc964c.js" async=""></script><script src="/_next/static/chunks/176-6509553bec92a789.js" async=""></script><script src="/_next/static/chunks/app/(blogs)/layout-75f88dfdcbd0ba18.js" async=""></script><script src="/_next/static/chunks/d3ac728e-1e5d8b71e3d43fec.js" async=""></script><script src="/_next/static/chunks/202-7b9f876bcb1bf7b4.js" async=""></script><script src="/_next/static/chunks/app/(blogs)/23d/learn-rnn-lstm/page-703f76d67075a74e.js" async=""></script><title>Create Next App</title><meta name="description" content="Generated by create next app"/><link rel="icon" href="/favicon.ico" type="image/x-icon" sizes="16x16"/><script src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js" crossorigin="" noModule=""></script></head><body><div id="header" class=""><div id="header-left-wrapper"><div id="header-logo-bg"><svg viewBox="0 0 1560 1560" width="36px" height="36px" xmlns="http://www.w3.org/2000/svg"><g fill-rule="evenodd"><path d="M644 97h272.529L1189 780H916.471z" fill="#00A8C4"></path><path d="M98 97h272.84L780 1120.73 1189.162 97H1462L916.438 1462H643.562z" fill="#30303C"></path><path d="M98 1462L643.3 97H916L370.7 1462z" fill="#00F8FF"></path></g></svg></div><div id="header-archive"><h3 id="header-archive-text">归档</h3><div class="header-archive-rightarrow"><svg viewBox="0 0 1024 1024" width="16px" height="16px" xmlns="http://www.w3.org/2000/svg"><path d="M761.056 532.128c.512-.992 1.344-1.824 1.792-2.848 8.8-18.304 5.92-40.704-9.664-55.424L399.936 139.744c-19.264-18.208-49.632-17.344-67.872 1.888-18.208 19.264-17.376 49.632 1.888 67.872l316.96 299.84L335.2 813.632c-19.072 18.4-19.648 48.768-1.248 67.872 9.408 9.792 21.984 14.688 34.56 14.688 12 0 24-4.48 33.312-13.44l350.048-337.376c.672-.672.928-1.6 1.6-2.304.512-.48 1.056-.832 1.568-1.344 2.72-2.848 4.16-6.336 6.016-9.6z" fill="#50505c"></path></svg></div></div></div><div id="header-right-wrapper"><div id="header-theme-bg"><svg viewBox="0 0 1024 1024" width="20px" height="20px" xmlns="http://www.w3.org/2000/svg"><path fill="#FCFCFC" d="M524.8 938.667h-4.267a439.893 439.893 0 01-313.173-134.4 446.293 446.293 0 01-11.093-597.334A432.213 432.213 0 01366.933 90.027a42.667 42.667 0 0145.227 9.386 42.667 42.667 0 0110.24 42.667 358.4 358.4 0 0082.773 375.893 361.387 361.387 0 00376.747 82.774 42.667 42.667 0 0154.187 55.04 433.493 433.493 0 01-99.84 154.88 438.613 438.613 0 01-311.467 128z"></path></svg></div><a href="https://github.com/1kuzus/1kuzus.github.io" target="_blank" rel="noreferrer"><div id="header-github-bg"><svg viewBox="0 0 1024 1024" width="36px" height="36px" xmlns="http://www.w3.org/2000/svg"><path d="M411.306667 831.146667c3.413333-5.12 6.826667-10.24 6.826666-11.946667v-69.973333c-105.813333 22.186667-128-44.373333-128-44.373334-17.066667-44.373333-42.666667-56.32-42.666666-56.32-34.133333-23.893333 3.413333-23.893333 3.413333-23.893333 37.546667 3.413333 58.026667 39.253333 58.026667 39.253333 34.133333 58.026667 88.746667 40.96 110.933333 32.426667 3.413333-23.893333 13.653333-40.96 23.893333-51.2-85.333333-10.24-174.08-42.666667-174.08-187.733333 0-40.96 15.36-75.093333 39.253334-102.4-3.413333-10.24-17.066667-47.786667 3.413333-100.693334 0 0 32.426667-10.24 104.106667 39.253334 30.72-8.533333 63.146667-11.946667 95.573333-11.946667 32.426667 0 64.853333 5.12 95.573333 11.946667 73.386667-49.493333 104.106667-39.253333 104.106667-39.253334 20.48 52.906667 8.533333 90.453333 3.413333 100.693334 23.893333 27.306667 39.253333 59.733333 39.253334 102.4 0 145.066667-88.746667 177.493333-174.08 187.733333 13.653333 11.946667 25.6 34.133333 25.6 69.973333v104.106667c0 3.413333 1.706667 6.826667 6.826666 11.946667 5.12 6.826667 3.413333 18.773333-3.413333 23.893333-3.413333 1.706667-6.826667 3.413333-10.24 3.413333h-174.08c-10.24 0-17.066667-6.826667-17.066667-17.066666 0-5.12 1.706667-8.533333 3.413334-10.24z" fill="#FCFCFC"></path></svg></div></a></div></div><div id="blog-layout"><div id="main"><h1 class="x-title"></h1><h2 class="x-h1">RNN：一个简单的例子</h2><p class="x-p">传统神经网络每次的输入是独立的，每次输出只依赖于当前的输入；但在某些任务中需要更好的处理序列信息，即前面的输入和后面的输入是有关系的；<span class="x-inline-strong">循环神经网络</span><code class="x-inline-highlight">(Recurrent Neural Networks, RNN)</code>通过使用带自反馈的神经元，能够处理任意长度的序列。</p><p class="x-p">下面是一个非常常见的RNN结构描述图。它展示了RNN的自反馈机制和与时间的依赖关系，但是对网络结构的描述容易引起误解：右侧的展开形式并不意味着网络有<code class="x-inline-highlight">t</code>层，而是反映了随着时间增加（有时也可以理解为随着程序中循环的迭代），上一次输出的隐藏状态，和<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>共同作为网络的下一次的输入。</p><p class="x-p">或者，如果说CNN是从空间维度上堆叠卷积层，不断加深，RNN就是从时间维度上的延展，而其网络真正的参数是很少的。</p><div class="x-image-wrapper x-image-invert"><img alt="img" decoding="async" data-nimg="1" style="color:transparent;width:600px" src=""/></div><p class="x-p">下面以一个简单的正弦序列预测任务出发，结合代码理解RNN网络的部分细节。</p><h3 class="x-h2">预测一个正弦序列</h3><p class="x-p">这个例子中，我们对一个加了噪声的正弦序列进行预测。</p><div class="x-codeblock"><pre><code class="lang-python">import numpy
import torch
from torch import nn
import matplotlib.pyplot as plt

#生成加噪声的正弦序列数据
xlim=numpy.linspace(0,36,400)
y=numpy.sin(xlim)+numpy.random.rand(*xlim.shape)*0.2

#转换dtype和size，保持和后面的训练数据统一
y=y.reshape(-1,1).astype(&quot;float32&quot;)

plt.plot(xlim,y)
plt.show()
</code></pre></div><p class="x-p">代码中我们在区间<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">36</span><span class="mclose">]</span></span></span></span>取了<code class="x-inline-highlight">400</code>点数据，如果把横轴看成时间轴，可以认为数据集中有<code class="x-inline-highlight">400</code>个连续时间点的数据。</p><div class="x-image-wrapper x-image-invert"><img alt="img" decoding="async" data-nimg="1" style="color:transparent;width:600px" src=""/></div><p class="x-p no-margin-bottom">现在明确一下我们的方案：</p><div class="x-oli"><div class="x-oli-number">1.</div><div class="x-oli-content-wrapper"><p class="x-p">使用前<code class="x-inline-highlight">80%</code>也就是前<code class="x-inline-highlight">320</code>个数据作为训练集，剩余的作为测试集，观察预测结果。</p></div></div><div class="x-oli"><div class="x-oli-number">2.</div><div class="x-oli-content-wrapper"><p class="x-p">序列长度为<code class="x-inline-highlight">10</code>，也就是模型根据前<code class="x-inline-highlight">10</code>个时间点的数据去预测下一个时间点的数据。</p></div></div><div class="x-oli"><div class="x-oli-number">3.</div><div class="x-oli-content-wrapper"><p class="x-p">这个例子中输入特征的维度是<code class="x-inline-highlight">1</code>，也就是只有<code class="x-inline-highlight">y</code>值一个指标。此外，也不考虑批量大小。</p></div></div><h3 class="x-h2">定义RNN网络</h3><div class="x-codeblock"><pre><code class="lang-python">class MyRNN(nn.Module):
    def __init__(self,input_size,hidden_size,output_size):
        super().__init__()
        self.linear_ih=nn.Linear(input_size,hidden_size)
        self.linear_hh=nn.Linear(hidden_size,hidden_size)
        self.linear_ho=nn.Linear(hidden_size,output_size)
        self.tanh=nn.Tanh()
    def forward(self,x,prev_state):
        curr_state=self.tanh(
            self.linear_ih(x)+self.linear_hh(prev_state)
        )
        output=self.linear_ho(curr_state).reshape(1)
        return output,curr_state

hidden_size=12
my_rnn=MyRNN(input_size=1,hidden_size=hidden_size,output_size=1)
loss_func=nn.MSELoss()
optimizer=torch.optim.SGD(my_rnn.parameters(),lr=0.01)
</code></pre></div><p class="x-p">这个是一个简单的RNN结构，从网络参数和结构来看很像一个<code class="x-inline-highlight">输入层-隐藏层-输出层</code>的感知机，但是多了一步<code class="x-inline-highlight">隐藏层-隐藏层</code>的连接，RNN的反馈结构就是由此体现的。</p><p class="x-p">并且，注意到<code class="x-inline-highlight">forward</code>函数的输入也需要两个参数：当前时刻输入<code class="x-inline-highlight">x</code>和前一时刻状态<code class="x-inline-highlight">prev_state</code>，同时也会把计算后的新状态<code class="x-inline-highlight">curr_state</code>和<code class="x-inline-highlight">output</code>一起返回，供下一次计算使用。在这里，经过<code class="x-inline-highlight">linear_ho</code>后，<code class="x-inline-highlight">output</code>的<code class="x-inline-highlight">size</code>为<code class="x-inline-highlight">(1,1)</code>，考虑到它仅仅是一个标量，我们把它<code class="x-inline-highlight">resize</code>为<code class="x-inline-highlight">(1)</code>。</p><p class="x-p">接下来设置了一些超参数，隐藏层有<code class="x-inline-highlight">12</code>个神经元，损失函数使用<code class="x-inline-highlight">MSELoss()</code>。</p><h3 class="x-h2">训练</h3><p class="x-p">首先定义这样的训练函数：它传入一个序列<code class="x-inline-highlight">train_seq</code>和目标<code class="x-inline-highlight">target</code>。<code class="x-inline-highlight">train_seq</code>的<code class="x-inline-highlight">size</code>应该为<code class="x-inline-highlight">(10,1)</code>，因为我们用前<code class="x-inline-highlight">10</code>个时间点的数据去预测下一个，而输入特征维度是<code class="x-inline-highlight">1</code>；<code class="x-inline-highlight">target</code>的<code class="x-inline-highlight">size</code>应该为<code class="x-inline-highlight">(1)</code>，因为输出只是一个标量。注意我们循环依次输入<code class="x-inline-highlight">train_seq</code>中的<code class="x-inline-highlight">10</code>个数据，迭代更新<code class="x-inline-highlight">state</code>，用最后一次的<code class="x-inline-highlight">output</code>作为最终的输出计算损失。</p><div class="x-codeblock"><pre><code class="lang-python">def train(train_seq,target):
    #初始状态
    state=torch.zeros(1,hidden_size)

    for x in train_seq:
        output,state=my_rnn(x,state)

    loss=loss_func(output,target)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    #返回损失
    return loss.detach().numpy().reshape(1)
</code></pre></div><p class="x-p">下面代码把真实数据划分成：</p><table class="x-table"><tbody><tr><th><p class="x-p"><code class="x-inline-highlight">train_seq</code></p></th><th><p class="x-p"><code class="x-inline-highlight">target</code></p></th></tr><tr><td><p class="x-p"><code class="x-inline-highlight">y[0]</code>,<code class="x-inline-highlight">y[1]</code>,<code class="x-inline-highlight">y[2]</code>, ... ,<code class="x-inline-highlight">y[9]</code></p></td><td><p class="x-p"><code class="x-inline-highlight">y[10]</code></p></td></tr><tr><td><p class="x-p"><code class="x-inline-highlight">y[1]</code>,<code class="x-inline-highlight">y[2]</code>,<code class="x-inline-highlight">y[3]</code>, ... ,<code class="x-inline-highlight">y[10]</code></p></td><td><p class="x-p"><code class="x-inline-highlight">y[11]</code></p></td></tr><tr><td><p class="x-p"><code class="x-inline-highlight">y[2]</code>,<code class="x-inline-highlight">y[3]</code>,<code class="x-inline-highlight">y[4]</code>, ... ,<code class="x-inline-highlight">y[11]</code></p></td><td><p class="x-p"><code class="x-inline-highlight">y[12]</code></p></td></tr><tr><td>...</td><td>...</td></tr><tr><td><p class="x-p"><code class="x-inline-highlight">y[309]</code>,<code class="x-inline-highlight">y[310]</code>,<code class="x-inline-highlight">y[311]</code>, ... ,<code class="x-inline-highlight">y[318]</code></p></td><td><p class="x-p"><code class="x-inline-highlight">y[319]</code></p></td></tr></tbody></table><div class="x-codeblock"><pre><code class="lang-python">train_datas=[]
for i in range(320-10):
    train_seq=torch.from_numpy(y[i:i+10])
    target=torch.from_numpy(y[i+10])
    train_datas.append((train_seq,target))
</code></pre></div><p class="x-p">训练网络，绘制误差：</p><div class="x-codeblock"><pre><code class="lang-python">metrics=[]
my_rnn.train()
for train_seq,target in train_datas:
    loss=train(train_seq,target)
    metrics.append(loss)

plt.subplot(211)
plt.plot(metrics,label=&quot;loss&quot;)
plt.legend()
</code></pre></div><h3 class="x-h2">预测</h3><p class="x-p">这个例子中我们用<span class="x-inline-strong">单步预测</span>观察模型的效果。在单步预测时，每次预测都全部使用真实值；当然，我们可以这样做是因为验证集中本来就包含了真实的数据，换句话说，我们是在已知<code class="x-inline-highlight">t+1</code>时刻的真实数据的情况下，去看看模型使用<code class="x-inline-highlight">t-9</code>~<code class="x-inline-highlight">t</code>时刻的数据，对<code class="x-inline-highlight">t+1</code>时刻的预测值。</p><table class="x-table"><tbody><tr><th><p class="x-p"><code class="x-inline-highlight">input</code></p></th><th><p class="x-p"><code class="x-inline-highlight">prediction</code></p></th></tr><tr><td><p class="x-p"><code class="x-inline-highlight">y[310]</code>,<code class="x-inline-highlight">y[311]</code>,<code class="x-inline-highlight">y[312]</code>, ... ,<code class="x-inline-highlight">y[319]</code></p></td><td><p class="x-p"><code class="x-inline-highlight">pred[320]</code></p></td></tr><tr><td><p class="x-p"><code class="x-inline-highlight">y[311]</code>,<code class="x-inline-highlight">y[312]</code>,<code class="x-inline-highlight">y[313]</code>, ... ,<code class="x-inline-highlight">y[320]</code></p></td><td><p class="x-p"><code class="x-inline-highlight">pred[321]</code></p></td></tr><tr><td><p class="x-p"><code class="x-inline-highlight">y[312]</code>,<code class="x-inline-highlight">y[313]</code>,<code class="x-inline-highlight">y[314]</code>, ... ,<code class="x-inline-highlight">y[321]</code></p></td><td><p class="x-p"><code class="x-inline-highlight">pred[322]</code></p></td></tr><tr><td>...</td><td>...</td></tr></tbody></table><div class="x-codeblock"><pre><code class="lang-python">def pred(truth_seq):
    with torch.no_grad():
        state=torch.zeros(1,hidden_size)
        for x in truth_seq:
            output,state=my_rnn(x,state)
        return output

preds=[]
for i in range(320-10,400-10):
    truth_seq=torch.from_numpy(y[i:i+10])
    preds.append(pred(truth_seq).numpy())
preds=numpy.array(preds).reshape(-1,1)

plt.subplot(212)
plt.plot(xlim,y,label=&quot;truth&quot;)
plt.plot(xlim[320:400],preds,&quot;red&quot;,label=&quot;predict&quot;)
plt.legend()
plt.show()
</code></pre></div><p class="x-p">如果我们不只是在测试集上评估模型性能，而是去预测真实生活中的问题，例如未来<code class="x-inline-highlight">7</code>天的温度；或者假如我们的数据集到<code class="x-inline-highlight">y[319]</code>就截止了，这时如果想得到后面多个时刻的数据，就需要<span class="x-inline-strong">多步预测</span>，此时上一时刻的预测会被当做新的输入：</p><table class="x-table"><tbody><tr><th><p class="x-p"><code class="x-inline-highlight">input</code></p></th><th><p class="x-p"><code class="x-inline-highlight">prediction</code></p></th></tr><tr><td><p class="x-p"><code class="x-inline-highlight">y[310]</code>,<code class="x-inline-highlight">y[311]</code>,<code class="x-inline-highlight">y[312]</code>, ... ,<code class="x-inline-highlight">y[319]</code></p></td><td><p class="x-p"><code class="x-inline-highlight">pred[320]</code></p></td></tr><tr><td><p class="x-p"><code class="x-inline-highlight">y[311]</code>,<code class="x-inline-highlight">y[312]</code>,<code class="x-inline-highlight">y[313]</code>, ... ,<code class="x-inline-highlight">pred[320]</code></p></td><td><p class="x-p"><code class="x-inline-highlight">pred[321]</code></p></td></tr><tr><td><p class="x-p"><code class="x-inline-highlight">y[312]</code>,<code class="x-inline-highlight">y[313]</code>,<code class="x-inline-highlight">y[314]</code>, ... ,<code class="x-inline-highlight">pred[320]</code>,<code class="x-inline-highlight">pred[321]</code></p></td><td><p class="x-p"><code class="x-inline-highlight">pred[322]</code></p></td></tr><tr><td>...</td><td>...</td></tr></tbody></table><p class="x-p">多步预测会导致误差的累积。</p><h3 class="x-h2">看看效果</h3><p class="x-p">如果不执行训练步骤的代码，使用初始随机参数的模型预测结果是：</p><div class="x-image-wrapper x-image-invert"><img alt="img" decoding="async" data-nimg="1" style="color:transparent;width:600px" src=""/></div><p class="x-p">经过训练后，每次训练的<code class="x-inline-highlight">loss</code>和最终的预测：</p><div class="x-image-wrapper x-image-invert"><img alt="img" decoding="async" data-nimg="1" style="color:transparent;width:600px" src=""/></div><h3 class="x-h2">使用torch.nn.RNN</h3><p class="x-p">使用<code class="x-inline-highlight">torch.nn.RNN</code>模块时，与上面例子中手动实现的RNN有几处细小的区别，下面给出了使用<code class="x-inline-highlight">torch.nn.RNN</code>时需要做出的修改：</p><div class="x-oli"><div class="x-oli-number">1.</div><div class="x-oli-content-wrapper"><p class="x-p no-margin-bottom">定义模型时，不再需要显式指定<code class="x-inline-highlight">linear_ih</code>和<code class="x-inline-highlight">linear_hh</code>两层，将由<code class="x-inline-highlight">nn.RNN</code>模块实现；<code class="x-inline-highlight">nn.RNN</code>模块没有定义输出层，因此输出层<code class="x-inline-highlight">linear_ho</code>需要设置。<br/> 在<code class="x-inline-highlight">forward</code>函数中，手动实现时为了直观展示出RNN的迭代过程，只进行了一次隐藏状态的更新；而对于输入序列迭代更新隐藏状态是在训练和预测时实现的。而<code class="x-inline-highlight">nn.RNN</code>模块的一次<code class="x-inline-highlight">forward</code>就已经完成了迭代更新，其输入是整个序列<code class="x-inline-highlight">seq</code>和<code class="x-inline-highlight">prev_state</code>，返回值是<code class="x-inline-highlight">output_hidden,curr_state</code>，对于不考虑批量大小的数据，它们的<code class="x-inline-highlight">size</code>为：</p><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><code class="x-inline-highlight">seq</code>: <code class="x-inline-highlight">(sequence_length, input_size)</code></p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><code class="x-inline-highlight">init_state</code>: <code class="x-inline-highlight">(1, hidden_size)</code></p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><code class="x-inline-highlight">output_hidden</code>: <code class="x-inline-highlight">(sequence_length, hidden_size)</code></p></div></div><div class="x-uli"><div class="x-uli-marker"><div class="x-uli-marker-dot"></div></div><div class="x-uli-content-wrapper"><p class="x-p"><code class="x-inline-highlight">state</code>: <code class="x-inline-highlight">(1, hidden_size)</code></p></div></div><p class="x-p">最后在我们定义的<code class="x-inline-highlight">MyRNN</code>模块中，用<code class="x-inline-highlight">output_hidden</code>的最后一个时间点的输出，经过输出层得到最终的<code class="x-inline-highlight">output</code>。</p></div></div><div class="x-highlightblock highlight-background-gray"><h4 class="x-h3">手动实现</h4><div class="x-codeblock"><pre><code class="lang-python">class MyRNN(nn.Module):
    def __init__(self,input_size,hidden_size,output_size):
        super().__init__()
        self.linear_ih=nn.Linear(input_size,hidden_size)
        self.linear_hh=nn.Linear(hidden_size,hidden_size)
        self.linear_ho=nn.Linear(hidden_size,output_size)
        self.tanh=nn.Tanh()
    def forward(self,x,prev_state):
        curr_state=self.tanh(
            self.linear_ih(x)+self.linear_hh(prev_state)
        )
        output=self.linear_ho(curr_state).reshape(1)
        return output,curr_state
</code></pre></div><h4 class="x-h3">使用torch.nn.RNN</h4><div class="x-codeblock"><pre><code class="lang-python">class MyRNN(nn.Module):
    def __init__(self,input_size,hidden_size,output_size):
        super().__init__()
        self.rnn=nn.RNN(input_size=input_size,hidden_size=hidden_size,batch_first=True)
        self.linear_ho=nn.Linear(hidden_size,output_size)
    def forward(self,seq,init_state):
        output_hidden,tate=self.rnn(seq,init_state)
        output=self.linear_ho(output_hidden[-1,:]) #取最后一个时间点的输出
        return output,state
</code></pre></div></div><div class="x-oli"><div class="x-oli-number">2.</div><div class="x-oli-content-wrapper"><p class="x-p">训练和预测时，也不需要再遍历序列，迭代的过程已经在<code class="x-inline-highlight">nn.RNN</code>模块内部实现。</p></div></div><div class="x-highlightblock highlight-background-gray"><h4 class="x-h3">手动实现</h4><div class="x-codeblock"><pre><code class="lang-python">def train(train_seq,target):
    state=torch.zeros(1,hidden_size)
    for x in train_seq:
        output,state=my_rnn(x,state)
    loss=loss_func(output,target)
    # ......
</code></pre></div><div class="x-codeblock"><pre><code class="lang-python">def pred(truth_seq,net):
    with torch.no_grad():
        state=torch.zeros(1,hidden_size)
        for x in truth_seq:
            output,state=net(x,state)
        return output
</code></pre></div><h4 class="x-h3">使用torch.nn.RNN</h4><div class="x-codeblock"><pre><code class="lang-python">def train(train_seq,target):
    state=torch.zeros(1,hidden_size)
    output,_=my_rnn(train_seq,state) #一次得到输出
    loss=loss_func(output,target)
    # ......
</code></pre></div><div class="x-codeblock"><pre><code class="lang-python">def pred(truth_seq):
    with torch.no_grad():
        state=torch.zeros(1,hidden_size)
        output,_=my_rnn(truth_seq,state) #一次得到输出
        return output
</code></pre></div></div><h3 class="x-h2">FAQ</h3><p class="x-p">初次了解RNN时，我在一些问题上困惑了很久。这个版块是对它们的再次整理。（尽管有些已经包含在上述例子中了！）</p><h4 class="x-h3">用10步预测下1步，为什么 input_size 不是10，而是1？</h4><p class="x-p"><code class="x-inline-highlight">input_size</code>与序列长度并非同一个概念。用前<code class="x-inline-highlight">10</code>个时间点的数据去预测下一个，这里的<code class="x-inline-highlight">10</code>是序列长度；而<code class="x-inline-highlight">input_size</code>是输入特征的维度。由于这个例子较为简单，只是用历史的<code class="x-inline-highlight">y</code>值预测新的<code class="x-inline-highlight">y</code>值，因此特征只有<code class="x-inline-highlight">1</code>维。</p><h4 class="x-h3">什么时候 input_size 不是1？</h4><p class="x-p">例如我们在预测未来气温时，历史气温数据并不是唯一的参考，还可能参考历史的风速、气压、天气情况等等，此时输入数据将会是一个<code class="x-inline-highlight">input_size</code>维的向量。</p><h4 class="x-h3">hidden_size=12，12是在哪里体现的？</h4><p class="x-p"><code class="x-inline-highlight">12</code>只是模型的超参数，和MLP中隐藏层大小一样，并没有太多的物理含义。</p><h4 class="x-h3">训练时，每次迭代用哪些数据？应该遍历几遍数据集？每个 epoch 会使用哪些数据进行参数优化？</h4><p class="x-p no-margin-bottom">在训练一个CNN网络时（例如一个图片分类网络），策略通常是：</p><div class="x-oli"><div class="x-oli-number">1.</div><div class="x-oli-content-wrapper"><p class="x-p">指定超参数<code class="x-inline-highlight">num_epoch</code>，在每个<code class="x-inline-highlight">epoch</code>中随机遍历训练集中的所有图像进行参数优化；</p></div></div><div class="x-oli"><div class="x-oli-number">2.</div><div class="x-oli-content-wrapper"><p class="x-p">重复执行<code class="x-inline-highlight">num_epoch</code>次。</p></div></div><p class="x-p with-margin-top no-margin-bottom">然而对于RNN来说这个概念似乎并不清晰，例如上述例子的训练策略是：</p><div class="x-oli"><div class="x-oli-number">1.</div><div class="x-oli-content-wrapper"><p class="x-p">从<code class="x-inline-highlight">0</code>到<code class="x-inline-highlight">(训练集大小 - 序列长度)</code>依次遍历起始时间<code class="x-inline-highlight">t</code>；</p></div></div><div class="x-oli"><div class="x-oli-number">2.</div><div class="x-oli-content-wrapper"><p class="x-p">对于每个起始时间<code class="x-inline-highlight">t</code>，将<code class="x-inline-highlight">y[t]</code>~<code class="x-inline-highlight">y[t+9]</code>为输入，<code class="x-inline-highlight">y[t+10]</code>为真值作为一组训练样本。</p></div></div><div class="x-oli"><div class="x-oli-number">3.</div><div class="x-oli-content-wrapper"><p class="x-p">第<code class="x-inline-highlight">1</code>步只遍历了一次！</p></div></div><p class="x-p with-margin-top no-margin-bottom">或者：</p><p class="x-p no-margin-bottom">...</p><div class="x-oli"><div class="x-oli-number">3.</div><div class="x-oli-content-wrapper"><p class="x-p">前两步同上，但多次遍历训练集。</p></div></div><p class="x-p with-margin-top no-margin-bottom">另一个常用的策略是：</p><div class="x-oli"><div class="x-oli-number">1.</div><div class="x-oli-content-wrapper"><p class="x-p">指定超参数：训练轮次<code class="x-inline-highlight">num_iter</code>；</p></div></div><div class="x-oli"><div class="x-oli-number">2.</div><div class="x-oli-content-wrapper"><p class="x-p">重复执行<code class="x-inline-highlight">num_iter</code>次，每次随机抽取一个起始时间<code class="x-inline-highlight">t</code>，并且将<code class="x-inline-highlight">y[t]</code>~<code class="x-inline-highlight">y[t+9]</code>为输入，<code class="x-inline-highlight">y[t+10]</code>为真值作为一组训练样本。</p></div></div><h2 class="x-h1">LSTM：一篇很好的博客</h2><p class="x-p">以下的内容和插图总结或翻译自这篇的英文博客：<a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noreferrer" class="x-inline-link">Understanding LSTM Networks</a></p><h3 class="x-h2">长期依赖问题</h3><p class="x-p">RNN可以利用先前的信息理解当前的任务，这点非常不错；有时我们只需要短期的信息，例如一个语言模型预测下面的句子：<br/> <code class="x-inline-highlight">天空中飘着一朵白色的【云】</code>，这很简单。但有些时候我们需要更多背景信息，例如：<br/><code class="x-inline-highlight">我出生在法国，…… ，我可以说流利的【法语】</code>，这个情况下，随着前后文距离变大，RNN对长期依赖关系的学习会变得困难。</p><h3 class="x-h2">LSTM</h3><p class="x-p"><span class="x-inline-strong">长短期记忆网络</span><code class="x-inline-highlight">(Long Short-Term Memory, LSTM)</code>是一种特殊的RNN，可以学习长期依赖。以RNN为例，循环神经网络随时间展开通常具有如下的示意图：</p><div class="x-image-wrapper x-image-invert"><img alt="img" decoding="async" data-nimg="1" style="color:transparent;width:600px" src=""/></div><p class="x-p">对于RNN来说，利用历史状态和输入得到新的状态，只经过一个简单的<code class="x-inline-highlight">tanh</code>激活层，而对于LSTM来说，它的示意图略显复杂：</p><div class="x-image-wrapper x-image-invert"><img alt="img" decoding="async" data-nimg="1" style="color:transparent;width:600px" src=""/></div><p class="x-p">在上图中，每条线表示一个向量，粉红色圆圈表示逐点式操作，黄色的方框是神经网络的层。这看起来很眼晕，不过我们接下来会一点点的解释图里的内容。</p><h3 class="x-h2">门控单元</h3><p class="x-p">下面的结构称为门控单元：</p><div class="x-image-wrapper x-image-invert"><img alt="img" decoding="async" data-nimg="1" style="color:transparent;width:100px" src=""/></div><p class="x-p">门控单元控制信息量通过的多少，通过向量<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span></span></span></span>来控制<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span>通过的信息量：</p><div class="x-formula"></div><p class="x-p">式子中<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">⊗</span></span></span></span>表示按位置相乘，<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mclose">)</span></span></span></span>的每个元素输出范围是<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span>，某个元素接近<code class="x-inline-highlight">1</code>，<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span>对应位置保留的信息就越多，反之就越少。</p><h3 class="x-h2">逐部分分析LSTM</h3><h4 class="x-h3">遗忘门</h4><p class="x-p">LSTM的第一步是决定什么应该被遗忘，也就是对上一个<span class="x-inline-strong">单元</span><code class="x-inline-highlight">(cell)</code>状态信息选择性的遗忘。<br/> 这个操作由遗忘门<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>实现，将其<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span>范围的输出按位置与单元上一时刻状态相乘。</p><div class="x-image-wrapper x-image-invert"><img alt="img" decoding="async" data-nimg="1" style="color:transparent;width:600px" src=""/></div><div class="x-highlightblock highlight-background-gray"><p class="x-p">举一个概念性的例子：</p><p class="x-p">考虑一个语言模型，输入一个句子：<code class="x-inline-highlight">Alice是一名女教师，她喜欢给学生讲课；Bob是一位男司机，他喝酒上瘾。</code></p><p class="x-p">当模型看到<code class="x-inline-highlight">Alice是一名女教师，……</code>时，单元状态中可能存储了和主语<code class="x-inline-highlight">Alice</code>和<code class="x-inline-highlight">女教师</code>有关的语义信息，以便在后文输出合适的代词<code class="x-inline-highlight">她</code>；然后，当模型看到<code class="x-inline-highlight">Alice是一名女教师，她喜欢给学生讲课；Bob是一位男司机，……</code>时，我们希望在看到新主语<code class="x-inline-highlight">Bob</code>和<code class="x-inline-highlight">男司机</code>之后，忘记此前存储的旧主语的性别语义。也就是对旧单元状态<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8917em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span>乘上较小的<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。</p></div><h4 class="x-h3">输入门</h4><p class="x-p">下一步就是决定要在单元中存入什么新的信息。这一部分有两路：<code class="x-inline-highlight">tanh</code>这一路与普通RNN很像，生成一个中间状态；<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span></span>这一路被称为输入门<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8095em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">i</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，控制这个中间状态有多少信息被存入单元。</p><div class="x-image-wrapper x-image-invert"><img alt="img" decoding="async" data-nimg="1" style="color:transparent;width:600px" src=""/></div><p class="x-p">经历这两步之后，便可以相加得到新的单元状态：</p><div class="x-image-wrapper x-image-invert"><img alt="img" decoding="async" data-nimg="1" style="color:transparent;width:600px" src=""/></div><div class="x-highlightblock highlight-background-gray"><p class="x-p">同理，当模型看到<code class="x-inline-highlight">Bob是一位男司机</code>时，我们可能会想丢掉此前的语义信息<code class="x-inline-highlight">女性</code>，并把新的语义信息<code class="x-inline-highlight">男性</code>存入单元状态，使得后文输出正确的代词<code class="x-inline-highlight">他</code>。</p></div><h4 class="x-h3">输出门</h4><p class="x-p">最后是决定新的隐藏状态，这个输出会基于单元状态，但会经过门控单元。输出门<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>决定经过<code class="x-inline-highlight">tanh</code>的单元状态<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>有多少被输出到下一时刻的隐藏状态。</p><div class="x-image-wrapper x-image-invert"><img alt="img" decoding="async" data-nimg="1" style="color:transparent;width:600px" src=""/></div><div class="x-highlightblock highlight-background-gray"><p class="x-p">当看到<code class="x-inline-highlight">Bob是一位男司机，他……</code>时，由于出现了主语<code class="x-inline-highlight">他</code>，模型可能会输出和<code class="x-inline-highlight">谓语动词</code>有关的语义信息。</p></div></div><div id="sidebar"><div class="sidebar-list show-list"><div class="sidebar-list-head"><h3 class="sidebar-list-category">网络杂识 (4)</h3><div class="sidebar-list-category-rightarrow"><svg viewBox="0 0 1024 1024" width="16px" height="16px" xmlns="http://www.w3.org/2000/svg"><path d="M761.056 532.128c.512-.992 1.344-1.824 1.792-2.848 8.8-18.304 5.92-40.704-9.664-55.424L399.936 139.744c-19.264-18.208-49.632-17.344-67.872 1.888-18.208 19.264-17.376 49.632 1.888 67.872l316.96 299.84L335.2 813.632c-19.072 18.4-19.648 48.768-1.248 67.872 9.408 9.792 21.984 14.688 34.56 14.688 12 0 24-4.48 33.312-13.44l350.048-337.376c.672-.672.928-1.6 1.6-2.304.512-.48 1.056-.832 1.568-1.344 2.72-2.848 4.16-6.336 6.016-9.6z" fill="#50505c"></path></svg></div></div><div class="sidebar-list-ul-wrapper"><ul class="sidebar-list-ul"><li class="sidebar-list-li"><a class="sidebar-list-link" href="/23d/database-3nf/"><span class="sidebar-list-title">数据库设计三大范式</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/23d/github-linguist-vendored/"><span class="sidebar-list-title">不统计Github仓库某个目录下的语言</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/24a/deepl-shortcut-setting/"><span class="sidebar-list-title">解决：DeepL该快捷键已被使用</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/24a/git-merge-allow-unrelated-histories/"><span class="sidebar-list-title">记录：使用--allow-unrelated-histories</span></a></li></ul></div></div><div class="sidebar-list show-list"><div class="sidebar-list-head"><h3 class="sidebar-list-category">深度学习 (5)</h3><div class="sidebar-list-category-rightarrow"><svg viewBox="0 0 1024 1024" width="16px" height="16px" xmlns="http://www.w3.org/2000/svg"><path d="M761.056 532.128c.512-.992 1.344-1.824 1.792-2.848 8.8-18.304 5.92-40.704-9.664-55.424L399.936 139.744c-19.264-18.208-49.632-17.344-67.872 1.888-18.208 19.264-17.376 49.632 1.888 67.872l316.96 299.84L335.2 813.632c-19.072 18.4-19.648 48.768-1.248 67.872 9.408 9.792 21.984 14.688 34.56 14.688 12 0 24-4.48 33.312-13.44l350.048-337.376c.672-.672.928-1.6 1.6-2.304.512-.48 1.056-.832 1.568-1.344 2.72-2.848 4.16-6.336 6.016-9.6z" fill="#50505c"></path></svg></div></div><div class="sidebar-list-ul-wrapper"><ul class="sidebar-list-ul"><li class="sidebar-list-li"><a class="sidebar-list-link" href="/longtime/papers/"><span class="sidebar-list-title">论文速记</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/23d/r2plus1d/"><span class="sidebar-list-title">行为识别R(2+1)D网络</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/23d/object-detection-map/"><span class="sidebar-list-title">目标检测评价指标mAP</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/23d/learn-rnn-lstm/"><span class="sidebar-list-title">学习RNN和LSTM</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/24a/reproduce-nerf-rpn/"><span class="sidebar-list-title">记录：复现NeRF-RPN代码</span></a></li></ul></div></div><div class="sidebar-list show-list"><div class="sidebar-list-head"><h3 class="sidebar-list-category">Python学习 (2)</h3><div class="sidebar-list-category-rightarrow"><svg viewBox="0 0 1024 1024" width="16px" height="16px" xmlns="http://www.w3.org/2000/svg"><path d="M761.056 532.128c.512-.992 1.344-1.824 1.792-2.848 8.8-18.304 5.92-40.704-9.664-55.424L399.936 139.744c-19.264-18.208-49.632-17.344-67.872 1.888-18.208 19.264-17.376 49.632 1.888 67.872l316.96 299.84L335.2 813.632c-19.072 18.4-19.648 48.768-1.248 67.872 9.408 9.792 21.984 14.688 34.56 14.688 12 0 24-4.48 33.312-13.44l350.048-337.376c.672-.672.928-1.6 1.6-2.304.512-.48 1.056-.832 1.568-1.344 2.72-2.848 4.16-6.336 6.016-9.6z" fill="#50505c"></path></svg></div></div><div class="sidebar-list-ul-wrapper"><ul class="sidebar-list-ul"><li class="sidebar-list-li"><a class="sidebar-list-link" href="/24a/torch-numpy-topk/"><span class="sidebar-list-title">在pytorch和numpy中取top-k值和索引</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/24a/object-oriented-programming-python/"><span class="sidebar-list-title">Python面向对象编程</span></a></li></ul></div></div><div class="sidebar-list show-list"><div class="sidebar-list-head"><h3 class="sidebar-list-category">前端与JavaScript (2)</h3><div class="sidebar-list-category-rightarrow"><svg viewBox="0 0 1024 1024" width="16px" height="16px" xmlns="http://www.w3.org/2000/svg"><path d="M761.056 532.128c.512-.992 1.344-1.824 1.792-2.848 8.8-18.304 5.92-40.704-9.664-55.424L399.936 139.744c-19.264-18.208-49.632-17.344-67.872 1.888-18.208 19.264-17.376 49.632 1.888 67.872l316.96 299.84L335.2 813.632c-19.072 18.4-19.648 48.768-1.248 67.872 9.408 9.792 21.984 14.688 34.56 14.688 12 0 24-4.48 33.312-13.44l350.048-337.376c.672-.672.928-1.6 1.6-2.304.512-.48 1.056-.832 1.568-1.344 2.72-2.848 4.16-6.336 6.016-9.6z" fill="#50505c"></path></svg></div></div><div class="sidebar-list-ul-wrapper"><ul class="sidebar-list-ul"><li class="sidebar-list-li"><a class="sidebar-list-link" href="/23c/js-array/"><span class="sidebar-list-title">JavaScript数组常用方法</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/23d/css-auto-height-transition/"><span class="sidebar-list-title">CSS实现auto高度的过渡动画</span></a></li></ul></div></div><div class="sidebar-list show-list"><div class="sidebar-list-head"><h3 class="sidebar-list-category">课程 (10)</h3><div class="sidebar-list-category-rightarrow"><svg viewBox="0 0 1024 1024" width="16px" height="16px" xmlns="http://www.w3.org/2000/svg"><path d="M761.056 532.128c.512-.992 1.344-1.824 1.792-2.848 8.8-18.304 5.92-40.704-9.664-55.424L399.936 139.744c-19.264-18.208-49.632-17.344-67.872 1.888-18.208 19.264-17.376 49.632 1.888 67.872l316.96 299.84L335.2 813.632c-19.072 18.4-19.648 48.768-1.248 67.872 9.408 9.792 21.984 14.688 34.56 14.688 12 0 24-4.48 33.312-13.44l350.048-337.376c.672-.672.928-1.6 1.6-2.304.512-.48 1.056-.832 1.568-1.344 2.72-2.848 4.16-6.336 6.016-9.6z" fill="#50505c"></path></svg></div></div><div class="sidebar-list-ul-wrapper"><ul class="sidebar-list-ul"><li class="sidebar-list-li"><a class="sidebar-list-link" href="/23c/pattern-recognition-1/"><span class="sidebar-list-title">【模式识别】统计决策方法</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/23c/pattern-recognition-2/"><span class="sidebar-list-title">【模式识别】参数估计</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/23c/pattern-recognition-3/"><span class="sidebar-list-title">【模式识别】非参数估计</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/23d/pattern-recognition-4/"><span class="sidebar-list-title">【模式识别】线性学习器与线性分类器</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/23d/protocols/"><span class="sidebar-list-title">【计算机网络】协议总结</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/24a/machine-learning-exercises/"><span class="sidebar-list-title">【机器学习】习题</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/24a/games101-01-transformation/"><span class="sidebar-list-title">【GAMES101】Transformation</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/24a/games101-02-rasterization/"><span class="sidebar-list-title">【GAMES101】Rasterization</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/24a/games101-03-shading/"><span class="sidebar-list-title">【GAMES101】Shading</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/24a/games101-04-geometry/"><span class="sidebar-list-title">【GAMES101】Geometry</span></a></li></ul></div></div><div class="sidebar-list show-list"><div class="sidebar-list-head"><h3 class="sidebar-list-category">其他 (2)</h3><div class="sidebar-list-category-rightarrow"><svg viewBox="0 0 1024 1024" width="16px" height="16px" xmlns="http://www.w3.org/2000/svg"><path d="M761.056 532.128c.512-.992 1.344-1.824 1.792-2.848 8.8-18.304 5.92-40.704-9.664-55.424L399.936 139.744c-19.264-18.208-49.632-17.344-67.872 1.888-18.208 19.264-17.376 49.632 1.888 67.872l316.96 299.84L335.2 813.632c-19.072 18.4-19.648 48.768-1.248 67.872 9.408 9.792 21.984 14.688 34.56 14.688 12 0 24-4.48 33.312-13.44l350.048-337.376c.672-.672.928-1.6 1.6-2.304.512-.48 1.056-.832 1.568-1.344 2.72-2.848 4.16-6.336 6.016-9.6z" fill="#50505c"></path></svg></div></div><div class="sidebar-list-ul-wrapper"><ul class="sidebar-list-ul"><li class="sidebar-list-li"><a class="sidebar-list-link" href="/longtime/demo/"><span class="sidebar-list-title">示例</span></a></li><li class="sidebar-list-li"><a class="sidebar-list-link" href="/longtime/updates/"><span class="sidebar-list-title">更新日志</span></a></li></ul></div></div></div><div id="sidebar-mask"></div></div><script src="/_next/static/chunks/webpack-c5d6fd738a8d0587.js" crossorigin="" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/css/8ac0b0e7f508d966.css\",\"style\",{\"crossOrigin\":\"\"}]\n0:\"$L2\"\n"])</script><script>self.__next_f.push([1,"3:HL[\"/_next/static/css/c84b60895b980f14.css\",\"style\",{\"crossOrigin\":\"\"}]\n4:HL[\"/_next/static/css/f70c26425a044d6b.css\",\"style\",{\"crossOrigin\":\"\"}]\n"])</script><script>self.__next_f.push([1,"5:I[3728,[],\"\"]\n7:I[9928,[],\"\"]\n8:I[9226,[\"185\",\"static/chunks/app/layout-e12e822557cc964c.js\"],\"GlobalProvider\"]\n9:I[9330,[\"185\",\"static/chunks/app/layout-e12e822557cc964c.js\"],\"\"]\na:I[6954,[],\"\"]\nb:I[7264,[],\"\"]\nc:I[5285,[\"176\",\"static/chunks/176-6509553bec92a789.js\",\"135\",\"static/chunks/app/(blogs)/layout-75f88dfdcbd0ba18.js\"],\"\"]\ne:I[4484,[\"954\",\"static/chunks/d3ac728e-1e5d8b71e3d43fec.js\",\"202\",\"static/chunks/202-7b9f876bcb1bf7b4.js\",\"443\",\"static/chunks/app/(blogs)/23d/learn-rnn-lstm/page-703f76d67075"])</script><script>self.__next_f.push([1,"a74e.js\"],\"Title\"]\nf:I[4484,[\"954\",\"static/chunks/d3ac728e-1e5d8b71e3d43fec.js\",\"202\",\"static/chunks/202-7b9f876bcb1bf7b4.js\",\"443\",\"static/chunks/app/(blogs)/23d/learn-rnn-lstm/page-703f76d67075a74e.js\"],\"H1\"]\n10:I[8411,[\"954\",\"static/chunks/d3ac728e-1e5d8b71e3d43fec.js\",\"202\",\"static/chunks/202-7b9f876bcb1bf7b4.js\",\"443\",\"static/chunks/app/(blogs)/23d/learn-rnn-lstm/page-703f76d67075a74e.js\"],\"\"]\n11:I[4484,[\"954\",\"static/chunks/d3ac728e-1e5d8b71e3d43fec.js\",\"202\",\"static/chunks/202-7b9f876bcb1bf7b4.js\",\"4"])</script><script>self.__next_f.push([1,"43\",\"static/chunks/app/(blogs)/23d/learn-rnn-lstm/page-703f76d67075a74e.js\"],\"H2\"]\n12:I[9708,[\"954\",\"static/chunks/d3ac728e-1e5d8b71e3d43fec.js\",\"202\",\"static/chunks/202-7b9f876bcb1bf7b4.js\",\"443\",\"static/chunks/app/(blogs)/23d/learn-rnn-lstm/page-703f76d67075a74e.js\"],\"\"]\n13:I[8275,[\"954\",\"static/chunks/d3ac728e-1e5d8b71e3d43fec.js\",\"202\",\"static/chunks/202-7b9f876bcb1bf7b4.js\",\"443\",\"static/chunks/app/(blogs)/23d/learn-rnn-lstm/page-703f76d67075a74e.js\"],\"Oli\"]\n14:I[8275,[\"954\",\"static/chunks/d3ac728e-1e5"])</script><script>self.__next_f.push([1,"d8b71e3d43fec.js\",\"202\",\"static/chunks/202-7b9f876bcb1bf7b4.js\",\"443\",\"static/chunks/app/(blogs)/23d/learn-rnn-lstm/page-703f76d67075a74e.js\"],\"Uli\"]\n15:I[4484,[\"954\",\"static/chunks/d3ac728e-1e5d8b71e3d43fec.js\",\"202\",\"static/chunks/202-7b9f876bcb1bf7b4.js\",\"443\",\"static/chunks/app/(blogs)/23d/learn-rnn-lstm/page-703f76d67075a74e.js\"],\"H3\"]\n16:I[848,[\"954\",\"static/chunks/d3ac728e-1e5d8b71e3d43fec.js\",\"202\",\"static/chunks/202-7b9f876bcb1bf7b4.js\",\"443\",\"static/chunks/app/(blogs)/23d/learn-rnn-lstm/page-703f7"])</script><script>self.__next_f.push([1,"6d67075a74e.js\"],\"\"]\n17:T536,"])</script><script>self.__next_f.push([1,"式子中\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e⊗\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e表示按位置相乘，\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003eσ\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.04398em;\"\u003ez\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e的每个元素输出范围是\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mopen\"\u003e[\u003c/span\u003e\u003cspan class=\"mord\"\u003e0\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e1\u003c/span\u003e\u003cspan class=\"mclose\"\u003e]\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e，某个元素接近\u003ccode class=\"x-inline-highlight\"\u003e1\u003c/code\u003e，\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.4306em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003ex\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e对应位置保留的信息就越多，反之就越少。"])</script><script>self.__next_f.push([1,"18:T5a3,"])</script><script>self.__next_f.push([1,"LSTM的第一步是决定什么应该被遗忘，也就是对上一个\u003cspan class=\"x-inline-strong\"\u003e单元\u003c/span\u003e\u003ccode class=\"x-inline-highlight\"\u003e(cell)\u003c/code\u003e状态信息选择性的遗忘。\u003cbr/\u003e 这个操作由遗忘门\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.10764em;\"\u003ef\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.2806em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003et\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.15em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e实现，将其\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mopen\"\u003e[\u003c/span\u003e\u003cspan class=\"mord\"\u003e0\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.1667em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e1\u003c/span\u003e\u003cspan class=\"mclose\"\u003e]\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e范围的输出按位置与单元上一时刻状态相乘。"])</script><script>self.__next_f.push([1,"19:T93e,"])</script><script>self.__next_f.push([1,"当模型看到\u003ccode class=\"x-inline-highlight\"\u003eAlice是一名女教师，……\u003c/code\u003e时，单元状态中可能存储了和主语\u003ccode class=\"x-inline-highlight\"\u003eAlice\u003c/code\u003e和\u003ccode class=\"x-inline-highlight\"\u003e女教师\u003c/code\u003e有关的语义信息，以便在后文输出合适的代词\u003ccode class=\"x-inline-highlight\"\u003e她\u003c/code\u003e；然后，当模型看到\u003ccode class=\"x-inline-highlight\"\u003eAlice是一名女教师，她喜欢给学生讲课；Bob是一位男司机，……\u003c/code\u003e时，我们希望在看到新主语\u003ccode class=\"x-inline-highlight\"\u003eBob\u003c/code\u003e和\u003ccode class=\"x-inline-highlight\"\u003e男司机\u003c/code\u003e之后，忘记此前存储的旧主语的性别语义。也就是对旧单元状态\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8917em;vertical-align:-0.2083em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.07153em;\"\u003eC\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.3011em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003et\u003c/span\u003e\u003cspan class=\"mbin mtight\"\u003e−\u003c/span\u003e\u003cspan class=\"mord mtight\"\u003e1\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.2083em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e乘上较小的\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.10764em;\"\u003ef\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.2806em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003et\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.15em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e。"])</script><script>self.__next_f.push([1,"1a:T4d3,"])</script><script>self.__next_f.push([1,"下一步就是决定要在单元中存入什么新的信息。这一部分有两路：\u003ccode class=\"x-inline-highlight\"\u003etanh\u003c/code\u003e这一路与普通RNN很像，生成一个中间状态；\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.4306em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003eσ\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e这一路被称为输入门\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8095em;vertical-align:-0.15em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\"\u003ei\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.2806em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003et\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.15em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e，控制这个中间状态有多少信息被存入单元。"])</script><script>self.__next_f.push([1,"1b:T6c6,"])</script><script>self.__next_f.push([1,"最后是决定新的隐藏状态，这个输出会基于单元状态，但会经过门控单元。输出门\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\"\u003eo\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.2806em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003et\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.15em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e决定经过\u003ccode class=\"x-inline-highlight\"\u003etanh\u003c/code\u003e的单元状态\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.07153em;\"\u003eC\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.2806em;\"\u003e\u003cspan style=\"top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003et\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.15em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e有多少被输出到下一时刻的隐藏状态。"])</script><script>self.__next_f.push([1,"2:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/8ac0b0e7f508d966.css\",\"precedence\":\"next\",\"crossOrigin\":\"\"}]],[\"$\",\"$L5\",null,{\"buildId\":\"a2ccQY1j6x_FTv2AsGrpW\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/23d/learn-rnn-lstm/\",\"initialTree\":[\"\",{\"children\":[\"(blogs)\",{\"children\":[\"23d\",{\"children\":[\"learn-rnn-lstm\",{\"children\":[\"__PAGE__\",{}]}]}]}]},\"$undefined\",\"$undefined\",true],\"initialHead\":[false,\"$L6\"],\"globalErrorComponent\":\"$7\",\"children\":[null,[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"children\":[\"$\",\"$L8\",null,{\"children\":[[\"$\",\"$L9\",null,{}],[\"$\",\"$La\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$Lb\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[],\"childProp\":{\"current\":[null,[\"$\",\"$Lc\",null,{\"children\":[\"$\",\"$La\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"(blogs)\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$Lb\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[],\"childProp\":{\"current\":[\"$\",\"$La\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"(blogs)\",\"children\",\"23d\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$Lb\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"childProp\":{\"current\":[\"$\",\"$La\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"(blogs)\",\"children\",\"23d\",\"children\",\"learn-rnn-lstm\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$Lb\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"childProp\":{\"current\":[\"$Ld\",[[\"$\",\"$Le\",null,{\"children\":\"$undefined\"}],[\"$\",\"$Lf\",null,{\"children\":\"RNN：一个简单的例子\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"传统神经网络每次的输入是独立的，每次输出只依赖于当前的输入；但在某些任务中需要更好的处理序列信息，即前面的输入和后面的输入是有关系的；\u003cspan class=\\\"x-inline-strong\\\"\u003e循环神经网络\u003c/span\u003e\u003ccode class=\\\"x-inline-highlight\\\"\u003e(Recurrent Neural Networks, RNN)\u003c/code\u003e通过使用带自反馈的神经元，能够处理任意长度的序列。\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"下面是一个非常常见的RNN结构描述图。它展示了RNN的自反馈机制和与时间的依赖关系，但是对网络结构的描述容易引起误解：右侧的展开形式并不意味着网络有\u003ccode class=\\\"x-inline-highlight\\\"\u003et\u003c/code\u003e层，而是反映了随着时间增加（有时也可以理解为随着程序中循环的迭代），上一次输出的隐藏状态，和\u003cspan class=\\\"katex\\\"\u003e\u003cspan class=\\\"katex-html\\\" aria-hidden=\\\"true\\\"\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:0.5806em;vertical-align:-0.15em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003ex\u003c/span\u003e\u003cspan class=\\\"msupsub\\\"\u003e\u003cspan class=\\\"vlist-t vlist-t2\\\"\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.2806em;\\\"\u003e\u003cspan style=\\\"top:-2.55em;margin-left:0em;margin-right:0.05em;\\\"\u003e\u003cspan class=\\\"pstrut\\\" style=\\\"height:2.7em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"sizing reset-size6 size3 mtight\\\"\u003e\u003cspan class=\\\"mord mathnormal mtight\\\"\u003et\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-s\\\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\\\"vlist-r\\\"\u003e\u003cspan class=\\\"vlist\\\" style=\\\"height:0.15em;\\\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e共同作为网络的下一次的输入。\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"或者，如果说CNN是从空间维度上堆叠卷积层，不断加深，RNN就是从时间维度上的延展，而其网络真正的参数是很少的。\"}}],[\"$\",\"$L10\",null,{\"src\":{\"default\":{\"src\":\"/_next/static/media/rnn1.ad2f936d.png\",\"height\":711,\"width\":2706,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAACCAYAAABllJ3tAAAATElEQVR4nGPcs29L6NsP7/bISEv9+v+Pwe7///9fGRkZeV69f3lETEDcinHb+V3+379+OcDFw/2XiYnF/t//f99ACt48f3ZYWELKGgCk1yEgUvIy7AAAAABJRU5ErkJggg==\",\"blurWidth\":8,\"blurHeight\":2}},\"width\":\"600px\",\"invertInDarkTheme\":true}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"下面以一个简单的正弦序列预测任务出发，结合代码理解RNN网络的部分细节。\"}}],[\"$\",\"$L11\",null,{\"children\":\"预测一个正弦序列\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"这个例子中，我们对一个加了噪声的正弦序列进行预测。\"}}],[\"$\",\"$L12\",null,{\"language\":\"python\",\"code\":\"\\n                import numpy\\n                import torch\\n                from torch import nn\\n                import matplotlib.pyplot as plt\\n\\n                #生成加噪声的正弦序列数据\\n                xlim=numpy.linspace(0,36,400)\\n                y=numpy.sin(xlim)+numpy.random.rand(*xlim.shape)*0.2\\n\\n                #转换dtype和size，保持和后面的训练数据统一\\n                y=y.reshape(-1,1).astype(\\\"float32\\\")\\n\\n                plt.plot(xlim,y)\\n                plt.show()\\n                \"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"代码中我们在区间\u003cspan class=\\\"katex\\\"\u003e\u003cspan class=\\\"katex-html\\\" aria-hidden=\\\"true\\\"\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:1em;vertical-align:-0.25em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mopen\\\"\u003e[\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e0\u003c/span\u003e\u003cspan class=\\\"mpunct\\\"\u003e,\u003c/span\u003e\u003cspan class=\\\"mspace\\\" style=\\\"margin-right:0.1667em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord\\\"\u003e36\u003c/span\u003e\u003cspan class=\\\"mclose\\\"\u003e]\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e取了\u003ccode class=\\\"x-inline-highlight\\\"\u003e400\u003c/code\u003e点数据，如果把横轴看成时间轴，可以认为数据集中有\u003ccode class=\\\"x-inline-highlight\\\"\u003e400\u003c/code\u003e个连续时间点的数据。\"}}],[\"$\",\"$L10\",null,{\"src\":{\"default\":{\"src\":\"/_next/static/media/fig1.2b0c9266.png\",\"height\":266,\"width\":840,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAADBAMAAABc5lN7AAAAGFBMVEX7/P37+/v6+/v6+vz6+vv5+vv5+fv5+fo/pXwpAAAAFUlEQVR42mModnMNYRBgAAKhUKcgABNsAl6NvkirAAAAAElFTkSuQmCC\",\"blurWidth\":8,\"blurHeight\":3}},\"width\":\"600px\",\"invertInDarkTheme\":true}],[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"现在明确一下我们的方案：\"}}],[\"$\",\"$L13\",null,{\"children\":\"使用前`80%`也就是前`320`个数据作为训练集，剩余的作为测试集，观察预测结果。\"}],[\"$\",\"$L13\",null,{\"children\":\"序列长度为`10`，也就是模型根据前`10`个时间点的数据去预测下一个时间点的数据。\"}],[\"$\",\"$L13\",null,{\"children\":\"这个例子中输入特征的维度是`1`，也就是只有`y`值一个指标。此外，也不考虑批量大小。\"}],[\"$\",\"$L11\",null,{\"children\":\"定义RNN网络\"}],[\"$\",\"$L12\",null,{\"language\":\"python\",\"code\":\"\\n                class MyRNN(nn.Module):\\n                    def __init__(self,input_size,hidden_size,output_size):\\n                        super().__init__()\\n                        self.linear_ih=nn.Linear(input_size,hidden_size)\\n                        self.linear_hh=nn.Linear(hidden_size,hidden_size)\\n                        self.linear_ho=nn.Linear(hidden_size,output_size)\\n                        self.tanh=nn.Tanh()\\n                    def forward(self,x,prev_state):\\n                        curr_state=self.tanh(\\n                            self.linear_ih(x)+self.linear_hh(prev_state)\\n                        )\\n                        output=self.linear_ho(curr_state).reshape(1)\\n                        return output,curr_state\\n\\n                hidden_size=12\\n                my_rnn=MyRNN(input_size=1,hidden_size=hidden_size,output_size=1)\\n                loss_func=nn.MSELoss()\\n                optimizer=torch.optim.SGD(my_rnn.parameters(),lr=0.01)\\n                \"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"这个是一个简单的RNN结构，从网络参数和结构来看很像一个\u003ccode class=\\\"x-inline-highlight\\\"\u003e输入层-隐藏层-输出层\u003c/code\u003e的感知机，但是多了一步\u003ccode class=\\\"x-inline-highlight\\\"\u003e隐藏层-隐藏层\u003c/code\u003e的连接，RNN的反馈结构就是由此体现的。\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"并且，注意到\u003ccode class=\\\"x-inline-highlight\\\"\u003eforward\u003c/code\u003e函数的输入也需要两个参数：当前时刻输入\u003ccode class=\\\"x-inline-highlight\\\"\u003ex\u003c/code\u003e和前一时刻状态\u003ccode class=\\\"x-inline-highlight\\\"\u003eprev_state\u003c/code\u003e，同时也会把计算后的新状态\u003ccode class=\\\"x-inline-highlight\\\"\u003ecurr_state\u003c/code\u003e和\u003ccode class=\\\"x-inline-highlight\\\"\u003eoutput\u003c/code\u003e一起返回，供下一次计算使用。在这里，经过\u003ccode class=\\\"x-inline-highlight\\\"\u003elinear_ho\u003c/code\u003e后，\u003ccode class=\\\"x-inline-highlight\\\"\u003eoutput\u003c/code\u003e的\u003ccode class=\\\"x-inline-highlight\\\"\u003esize\u003c/code\u003e为\u003ccode class=\\\"x-inline-highlight\\\"\u003e(1,1)\u003c/code\u003e，考虑到它仅仅是一个标量，我们把它\u003ccode class=\\\"x-inline-highlight\\\"\u003eresize\u003c/code\u003e为\u003ccode class=\\\"x-inline-highlight\\\"\u003e(1)\u003c/code\u003e。\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"接下来设置了一些超参数，隐藏层有\u003ccode class=\\\"x-inline-highlight\\\"\u003e12\u003c/code\u003e个神经元，损失函数使用\u003ccode class=\\\"x-inline-highlight\\\"\u003eMSELoss()\u003c/code\u003e。\"}}],[\"$\",\"$L11\",null,{\"children\":\"训练\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"首先定义这样的训练函数：它传入一个序列\u003ccode class=\\\"x-inline-highlight\\\"\u003etrain_seq\u003c/code\u003e和目标\u003ccode class=\\\"x-inline-highlight\\\"\u003etarget\u003c/code\u003e。\u003ccode class=\\\"x-inline-highlight\\\"\u003etrain_seq\u003c/code\u003e的\u003ccode class=\\\"x-inline-highlight\\\"\u003esize\u003c/code\u003e应该为\u003ccode class=\\\"x-inline-highlight\\\"\u003e(10,1)\u003c/code\u003e，因为我们用前\u003ccode class=\\\"x-inline-highlight\\\"\u003e10\u003c/code\u003e个时间点的数据去预测下一个，而输入特征维度是\u003ccode class=\\\"x-inline-highlight\\\"\u003e1\u003c/code\u003e；\u003ccode class=\\\"x-inline-highlight\\\"\u003etarget\u003c/code\u003e的\u003ccode class=\\\"x-inline-highlight\\\"\u003esize\u003c/code\u003e应该为\u003ccode class=\\\"x-inline-highlight\\\"\u003e(1)\u003c/code\u003e，因为输出只是一个标量。注意我们循环依次输入\u003ccode class=\\\"x-inline-highlight\\\"\u003etrain_seq\u003c/code\u003e中的\u003ccode class=\\\"x-inline-highlight\\\"\u003e10\u003c/code\u003e个数据，迭代更新\u003ccode class=\\\"x-inline-highlight\\\"\u003estate\u003c/code\u003e，用最后一次的\u003ccode class=\\\"x-inline-highlight\\\"\u003eoutput\u003c/code\u003e作为最终的输出计算损失。\"}}],[\"$\",\"$L12\",null,{\"language\":\"python\",\"code\":\"\\n                def train(train_seq,target):\\n                    #初始状态\\n                    state=torch.zeros(1,hidden_size)\\n\\n                    for x in train_seq:\\n                        output,state=my_rnn(x,state)\\n\\n                    loss=loss_func(output,target)\\n                    optimizer.zero_grad()\\n                    loss.backward()\\n                    optimizer.step()\\n\\n                    #返回损失\\n                    return loss.detach().numpy().reshape(1)\\n                \"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"下面代码把真实数据划分成：\"}}],[\"$\",\"table\",null,{\"className\":\"x-table\",\"children\":[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003ccode class=\\\"x-inline-highlight\\\"\u003etrain_seq\u003c/code\u003e\"}}]}],[\"$\",\"th\",null,{\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003ccode class=\\\"x-inline-highlight\\\"\u003etarget\u003c/code\u003e\"}}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[0]\u003c/code\u003e,\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[1]\u003c/code\u003e,\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[2]\u003c/code\u003e, ... ,\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[9]\u003c/code\u003e\"}}]}],[\"$\",\"td\",null,{\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[10]\u003c/code\u003e\"}}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[1]\u003c/code\u003e,\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[2]\u003c/code\u003e,\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[3]\u003c/code\u003e, ... ,\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[10]\u003c/code\u003e\"}}]}],[\"$\",\"td\",null,{\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[11]\u003c/code\u003e\"}}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[2]\u003c/code\u003e,\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[3]\u003c/code\u003e,\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[4]\u003c/code\u003e, ... ,\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[11]\u003c/code\u003e\"}}]}],[\"$\",\"td\",null,{\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[12]\u003c/code\u003e\"}}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"...\"}],[\"$\",\"td\",null,{\"children\":\"...\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[309]\u003c/code\u003e,\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[310]\u003c/code\u003e,\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[311]\u003c/code\u003e, ... ,\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[318]\u003c/code\u003e\"}}]}],[\"$\",\"td\",null,{\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[319]\u003c/code\u003e\"}}]}]]}]]}]}],[\"$\",\"$L12\",null,{\"language\":\"python\",\"code\":\"\\n                train_datas=[]\\n                for i in range(320-10):\\n                    train_seq=torch.from_numpy(y[i:i+10])\\n                    target=torch.from_numpy(y[i+10])\\n                    train_datas.append((train_seq,target))\\n                \"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"训练网络，绘制误差：\"}}],[\"$\",\"$L12\",null,{\"language\":\"python\",\"code\":\"\\n                metrics=[]\\n                my_rnn.train()\\n                for train_seq,target in train_datas:\\n                    loss=train(train_seq,target)\\n                    metrics.append(loss)\\n\\n                plt.subplot(211)\\n                plt.plot(metrics,label=\\\"loss\\\")\\n                plt.legend()\\n                \"}],[\"$\",\"$L11\",null,{\"children\":\"预测\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"这个例子中我们用\u003cspan class=\\\"x-inline-strong\\\"\u003e单步预测\u003c/span\u003e观察模型的效果。在单步预测时，每次预测都全部使用真实值；当然，我们可以这样做是因为验证集中本来就包含了真实的数据，换句话说，我们是在已知\u003ccode class=\\\"x-inline-highlight\\\"\u003et+1\u003c/code\u003e时刻的真实数据的情况下，去看看模型使用\u003ccode class=\\\"x-inline-highlight\\\"\u003et-9\u003c/code\u003e~\u003ccode class=\\\"x-inline-highlight\\\"\u003et\u003c/code\u003e时刻的数据，对\u003ccode class=\\\"x-inline-highlight\\\"\u003et+1\u003c/code\u003e时刻的预测值。\"}}],[\"$\",\"table\",null,{\"className\":\"x-table\",\"children\":[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003ccode class=\\\"x-inline-highlight\\\"\u003einput\u003c/code\u003e\"}}]}],[\"$\",\"th\",null,{\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003ccode class=\\\"x-inline-highlight\\\"\u003eprediction\u003c/code\u003e\"}}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[310]\u003c/code\u003e,\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[311]\u003c/code\u003e,\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[312]\u003c/code\u003e, ... ,\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[319]\u003c/code\u003e\"}}]}],[\"$\",\"td\",null,{\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003ccode class=\\\"x-inline-highlight\\\"\u003epred[320]\u003c/code\u003e\"}}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[311]\u003c/code\u003e,\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[312]\u003c/code\u003e,\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[313]\u003c/code\u003e, ... ,\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[320]\u003c/code\u003e\"}}]}],[\"$\",\"td\",null,{\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003ccode class=\\\"x-inline-highlight\\\"\u003epred[321]\u003c/code\u003e\"}}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[312]\u003c/code\u003e,\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[313]\u003c/code\u003e,\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[314]\u003c/code\u003e, ... ,\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[321]\u003c/code\u003e\"}}]}],[\"$\",\"td\",null,{\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003ccode class=\\\"x-inline-highlight\\\"\u003epred[322]\u003c/code\u003e\"}}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"...\"}],[\"$\",\"td\",null,{\"children\":\"...\"}]]}]]}]}],[\"$\",\"$L12\",null,{\"language\":\"python\",\"code\":\"\\n                def pred(truth_seq):\\n                    with torch.no_grad():\\n                        state=torch.zeros(1,hidden_size)\\n                        for x in truth_seq:\\n                            output,state=my_rnn(x,state)\\n                        return output\\n\\n                preds=[]\\n                for i in range(320-10,400-10):\\n                    truth_seq=torch.from_numpy(y[i:i+10])\\n                    preds.append(pred(truth_seq).numpy())\\n                preds=numpy.array(preds).reshape(-1,1)\\n\\n                plt.subplot(212)\\n                plt.plot(xlim,y,label=\\\"truth\\\")\\n                plt.plot(xlim[320:400],preds,\\\"red\\\",label=\\\"predict\\\")\\n                plt.legend()\\n                plt.show()\\n                \"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"如果我们不只是在测试集上评估模型性能，而是去预测真实生活中的问题，例如未来\u003ccode class=\\\"x-inline-highlight\\\"\u003e7\u003c/code\u003e天的温度；或者假如我们的数据集到\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[319]\u003c/code\u003e就截止了，这时如果想得到后面多个时刻的数据，就需要\u003cspan class=\\\"x-inline-strong\\\"\u003e多步预测\u003c/span\u003e，此时上一时刻的预测会被当做新的输入：\"}}],[\"$\",\"table\",null,{\"className\":\"x-table\",\"children\":[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003ccode class=\\\"x-inline-highlight\\\"\u003einput\u003c/code\u003e\"}}]}],[\"$\",\"th\",null,{\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003ccode class=\\\"x-inline-highlight\\\"\u003eprediction\u003c/code\u003e\"}}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[310]\u003c/code\u003e,\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[311]\u003c/code\u003e,\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[312]\u003c/code\u003e, ... ,\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[319]\u003c/code\u003e\"}}]}],[\"$\",\"td\",null,{\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003ccode class=\\\"x-inline-highlight\\\"\u003epred[320]\u003c/code\u003e\"}}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[311]\u003c/code\u003e,\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[312]\u003c/code\u003e,\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[313]\u003c/code\u003e, ... ,\u003ccode class=\\\"x-inline-highlight\\\"\u003epred[320]\u003c/code\u003e\"}}]}],[\"$\",\"td\",null,{\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003ccode class=\\\"x-inline-highlight\\\"\u003epred[321]\u003c/code\u003e\"}}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[312]\u003c/code\u003e,\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[313]\u003c/code\u003e,\u003ccode class=\\\"x-inline-highlight\\\"\u003ey[314]\u003c/code\u003e, ... ,\u003ccode class=\\\"x-inline-highlight\\\"\u003epred[320]\u003c/code\u003e,\u003ccode class=\\\"x-inline-highlight\\\"\u003epred[321]\u003c/code\u003e\"}}]}],[\"$\",\"td\",null,{\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003ccode class=\\\"x-inline-highlight\\\"\u003epred[322]\u003c/code\u003e\"}}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"...\"}],[\"$\",\"td\",null,{\"children\":\"...\"}]]}]]}]}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"多步预测会导致误差的累积。\"}}],[\"$\",\"$L11\",null,{\"children\":\"看看效果\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"如果不执行训练步骤的代码，使用初始随机参数的模型预测结果是：\"}}],[\"$\",\"$L10\",null,{\"src\":{\"default\":{\"src\":\"/_next/static/media/fig2.51ee0782.png\",\"height\":261,\"width\":840,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAACBAMAAACXuoDeAAAAGFBMVEX7+/z6+/z6+vv6+vr5+vr6+fr5+fr5+fkEDyOPAAAAEklEQVR42mNwFBRUZShjVAwGAAagAXSZPcFiAAAAAElFTkSuQmCC\",\"blurWidth\":8,\"blurHeight\":2}},\"width\":\"600px\",\"invertInDarkTheme\":true}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"经过训练后，每次训练的\u003ccode class=\\\"x-inline-highlight\\\"\u003eloss\u003c/code\u003e和最终的预测：\"}}],[\"$\",\"$L10\",null,{\"src\":{\"default\":{\"src\":\"/_next/static/media/fig3.6f6f5b35.png\",\"height\":543,\"width\":840,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAMAAABPT11nAAAAQlBMVEX///7//v7+/v79/f38/f38/P38/Pz7/P36+/z8+vr7+vv6+vv5+vv5+vr6+fr6+Pn4+fn5+Pj4+Pn3+Pn2+Pr29/mY1zi4AAAALklEQVR42g3BhQEAIAwDsDIYVlz+fxUSVIWIGA+eoqrOgvXub4DhczlhrsjC3h4eogGS5iURrwAAAABJRU5ErkJggg==\",\"blurWidth\":8,\"blurHeight\":5}},\"width\":\"600px\",\"invertInDarkTheme\":true}],[\"$\",\"$L11\",null,{\"children\":\"使用torch.nn.RNN\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"使用\u003ccode class=\\\"x-inline-highlight\\\"\u003etorch.nn.RNN\u003c/code\u003e模块时，与上面例子中手动实现的RNN有几处细小的区别，下面给出了使用\u003ccode class=\\\"x-inline-highlight\\\"\u003etorch.nn.RNN\u003c/code\u003e时需要做出的修改：\"}}],[\"$\",\"$L13\",null,{\"reset\":true,\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"定义模型时，不再需要显式指定\u003ccode class=\\\"x-inline-highlight\\\"\u003elinear_ih\u003c/code\u003e和\u003ccode class=\\\"x-inline-highlight\\\"\u003elinear_hh\u003c/code\u003e两层，将由\u003ccode class=\\\"x-inline-highlight\\\"\u003enn.RNN\u003c/code\u003e模块实现；\u003ccode class=\\\"x-inline-highlight\\\"\u003enn.RNN\u003c/code\u003e模块没有定义输出层，因此输出层\u003ccode class=\\\"x-inline-highlight\\\"\u003elinear_ho\u003c/code\u003e需要设置。\u003cbr/\u003e 在\u003ccode class=\\\"x-inline-highlight\\\"\u003eforward\u003c/code\u003e函数中，手动实现时为了直观展示出RNN的迭代过程，只进行了一次隐藏状态的更新；而对于输入序列迭代更新隐藏状态是在训练和预测时实现的。而\u003ccode class=\\\"x-inline-highlight\\\"\u003enn.RNN\u003c/code\u003e模块的一次\u003ccode class=\\\"x-inline-highlight\\\"\u003eforward\u003c/code\u003e就已经完成了迭代更新，其输入是整个序列\u003ccode class=\\\"x-inline-highlight\\\"\u003eseq\u003c/code\u003e和\u003ccode class=\\\"x-inline-highlight\\\"\u003eprev_state\u003c/code\u003e，返回值是\u003ccode class=\\\"x-inline-highlight\\\"\u003eoutput_hidden,curr_state\u003c/code\u003e，对于不考虑批量大小的数据，它们的\u003ccode class=\\\"x-inline-highlight\\\"\u003esize\u003c/code\u003e为：\"}}],[\"$\",\"$L14\",null,{\"children\":\"`seq`: `(sequence_length, input_size)`\"}],[\"$\",\"$L14\",null,{\"children\":\"`init_state`: `(1, hidden_size)`\"}],[\"$\",\"$L14\",null,{\"children\":\"`output_hidden`: `(sequence_length, hidden_size)`\"}],[\"$\",\"$L14\",null,{\"children\":\"`state`: `(1, hidden_size)`\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"最后在我们定义的\u003ccode class=\\\"x-inline-highlight\\\"\u003eMyRNN\u003c/code\u003e模块中，用\u003ccode class=\\\"x-inline-highlight\\\"\u003eoutput_hidden\u003c/code\u003e的最后一个时间点的输出，经过输出层得到最终的\u003ccode class=\\\"x-inline-highlight\\\"\u003eoutput\u003c/code\u003e。\"}}]]}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-gray\",\"children\":[[\"$\",\"$L15\",null,{\"children\":\"手动实现\"}],[\"$\",\"$L12\",null,{\"language\":\"python\",\"code\":\"\\n                    class MyRNN(nn.Module):\\n                        def __init__(self,input_size,hidden_size,output_size):\\n                            super().__init__()\\n                            self.linear_ih=nn.Linear(input_size,hidden_size)\\n                            self.linear_hh=nn.Linear(hidden_size,hidden_size)\\n                            self.linear_ho=nn.Linear(hidden_size,output_size)\\n                            self.tanh=nn.Tanh()\\n                        def forward(self,x,prev_state):\\n                            curr_state=self.tanh(\\n                                self.linear_ih(x)+self.linear_hh(prev_state)\\n                            )\\n                            output=self.linear_ho(curr_state).reshape(1)\\n                            return output,curr_state\\n                    \"}],[\"$\",\"$L15\",null,{\"children\":\"使用torch.nn.RNN\"}],[\"$\",\"$L12\",null,{\"language\":\"python\",\"code\":\"\\n                    class MyRNN(nn.Module):\\n                        def __init__(self,input_size,hidden_size,output_size):\\n                            super().__init__()\\n                            self.rnn=nn.RNN(input_size=input_size,hidden_size=hidden_size,batch_first=True)\\n                            self.linear_ho=nn.Linear(hidden_size,output_size)\\n                        def forward(self,seq,init_state):\\n                            output_hidden,tate=self.rnn(seq,init_state)\\n                            output=self.linear_ho(output_hidden[-1,:]) #取最后一个时间点的输出\\n                            return output,state\\n                    \"}]]}],[\"$\",\"$L13\",null,{\"children\":\"训练和预测时，也不需要再遍历序列，迭代的过程已经在`nn.RNN`模块内部实现。\"}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-gray\",\"children\":[[\"$\",\"$L15\",null,{\"children\":\"手动实现\"}],[\"$\",\"$L12\",null,{\"language\":\"python\",\"code\":\"\\n                    def train(train_seq,target):\\n                        state=torch.zeros(1,hidden_size)\\n                        for x in train_seq:\\n                            output,state=my_rnn(x,state)\\n                        loss=loss_func(output,target)\\n                        # ......\\n                    \"}],[\"$\",\"$L12\",null,{\"language\":\"python\",\"code\":\"\\n                    def pred(truth_seq,net):\\n                        with torch.no_grad():\\n                            state=torch.zeros(1,hidden_size)\\n                            for x in truth_seq:\\n                                output,state=net(x,state)\\n                            return output\\n                    \"}],[\"$\",\"$L15\",null,{\"children\":\"使用torch.nn.RNN\"}],[\"$\",\"$L12\",null,{\"language\":\"python\",\"code\":\"\\n                    def train(train_seq,target):\\n                        state=torch.zeros(1,hidden_size)\\n                        output,_=my_rnn(train_seq,state) #一次得到输出\\n                        loss=loss_func(output,target)\\n                        # ......\\n                    \"}],[\"$\",\"$L12\",null,{\"language\":\"python\",\"code\":\"\\n                    def pred(truth_seq):\\n                        with torch.no_grad():\\n                            state=torch.zeros(1,hidden_size)\\n                            output,_=my_rnn(truth_seq,state) #一次得到输出\\n                            return output\\n                    \"}]]}],[\"$\",\"$L11\",null,{\"children\":\"FAQ\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"初次了解RNN时，我在一些问题上困惑了很久。这个版块是对它们的再次整理。（尽管有些已经包含在上述例子中了！）\"}}],[\"$\",\"$L15\",null,{\"children\":\"用10步预测下1步，为什么 input_size 不是10，而是1？\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003ccode class=\\\"x-inline-highlight\\\"\u003einput_size\u003c/code\u003e与序列长度并非同一个概念。用前\u003ccode class=\\\"x-inline-highlight\\\"\u003e10\u003c/code\u003e个时间点的数据去预测下一个，这里的\u003ccode class=\\\"x-inline-highlight\\\"\u003e10\u003c/code\u003e是序列长度；而\u003ccode class=\\\"x-inline-highlight\\\"\u003einput_size\u003c/code\u003e是输入特征的维度。由于这个例子较为简单，只是用历史的\u003ccode class=\\\"x-inline-highlight\\\"\u003ey\u003c/code\u003e值预测新的\u003ccode class=\\\"x-inline-highlight\\\"\u003ey\u003c/code\u003e值，因此特征只有\u003ccode class=\\\"x-inline-highlight\\\"\u003e1\u003c/code\u003e维。\"}}],[\"$\",\"$L15\",null,{\"children\":\"什么时候 input_size 不是1？\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"例如我们在预测未来气温时，历史气温数据并不是唯一的参考，还可能参考历史的风速、气压、天气情况等等，此时输入数据将会是一个\u003ccode class=\\\"x-inline-highlight\\\"\u003einput_size\u003c/code\u003e维的向量。\"}}],[\"$\",\"$L15\",null,{\"children\":\"hidden_size=12，12是在哪里体现的？\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003ccode class=\\\"x-inline-highlight\\\"\u003e12\u003c/code\u003e只是模型的超参数，和MLP中隐藏层大小一样，并没有太多的物理含义。\"}}],[\"$\",\"$L15\",null,{\"children\":\"训练时，每次迭代用哪些数据？应该遍历几遍数据集？每个 epoch 会使用哪些数据进行参数优化？\"}],[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"在训练一个CNN网络时（例如一个图片分类网络），策略通常是：\"}}],[\"$\",\"$L13\",null,{\"reset\":true,\"children\":\"指定超参数`num_epoch`，在每个`epoch`中随机遍历训练集中的所有图像进行参数优化；\"}],[\"$\",\"$L13\",null,{\"children\":\"重复执行`num_epoch`次。\"}],[\"$\",\"p\",null,{\"className\":\"x-p with-margin-top no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"然而对于RNN来说这个概念似乎并不清晰，例如上述例子的训练策略是：\"}}],[\"$\",\"$L13\",null,{\"reset\":true,\"children\":\"从`0`到`(训练集大小 - 序列长度)`依次遍历起始时间`t`；\"}],[\"$\",\"$L13\",null,{\"children\":\"对于每个起始时间`t`，将`y[t]`~`y[t+9]`为输入，`y[t+10]`为真值作为一组训练样本。\"}],[\"$\",\"$L13\",null,{\"children\":\"第`1`步只遍历了一次！\"}],[\"$\",\"p\",null,{\"className\":\"x-p with-margin-top no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"或者：\"}}],[\"$\",\"p\",null,{\"className\":\"x-p no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"...\"}}],[\"$\",\"$L13\",null,{\"reset\":3,\"children\":\"前两步同上，但多次遍历训练集。\"}],[\"$\",\"p\",null,{\"className\":\"x-p with-margin-top no-margin-bottom\",\"dangerouslySetInnerHTML\":{\"__html\":\"另一个常用的策略是：\"}}],[\"$\",\"$L13\",null,{\"reset\":true,\"children\":\"指定超参数：训练轮次`num_iter`；\"}],[\"$\",\"$L13\",null,{\"children\":\"重复执行`num_iter`次，每次随机抽取一个起始时间`t`，并且将`y[t]`~`y[t+9]`为输入，`y[t+10]`为真值作为一组训练样本。\"}],[\"$\",\"$Lf\",null,{\"children\":\"LSTM：一篇很好的博客\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"以下的内容和插图总结或翻译自这篇的英文博客：\u003ca href=\\\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/\\\" target=\\\"_blank\\\" rel=\\\"noreferrer\\\" class=\\\"x-inline-link\\\"\u003eUnderstanding LSTM Networks\u003c/a\u003e\"}}],[\"$\",\"$L11\",null,{\"children\":\"长期依赖问题\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"RNN可以利用先前的信息理解当前的任务，这点非常不错；有时我们只需要短期的信息，例如一个语言模型预测下面的句子：\u003cbr/\u003e \u003ccode class=\\\"x-inline-highlight\\\"\u003e天空中飘着一朵白色的【云】\u003c/code\u003e，这很简单。但有些时候我们需要更多背景信息，例如：\u003cbr/\u003e\u003ccode class=\\\"x-inline-highlight\\\"\u003e我出生在法国，…… ，我可以说流利的【法语】\u003c/code\u003e，这个情况下，随着前后文距离变大，RNN对长期依赖关系的学习会变得困难。\"}}],[\"$\",\"$L11\",null,{\"children\":\"LSTM\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"x-inline-strong\\\"\u003e长短期记忆网络\u003c/span\u003e\u003ccode class=\\\"x-inline-highlight\\\"\u003e(Long Short-Term Memory, LSTM)\u003c/code\u003e是一种特殊的RNN，可以学习长期依赖。以RNN为例，循环神经网络随时间展开通常具有如下的示意图：\"}}],[\"$\",\"$L10\",null,{\"src\":{\"default\":{\"src\":\"/_next/static/media/rnn2.d1dd71bb.png\",\"height\":839,\"width\":2242,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAADCAYAAACuyE5IAAAAbklEQVR4nAFjAJz/Adv0yjT47v0Q7+P3/SA6D+/iyvEcBg4D6RMdCgfm0fURAdzxyv8AAQIA/P37uAH+/0gBAwLsAAAA1wIBAz0AAf/EAcffxmoaFwTg3+L6BA4MARASEQfn3+P5EhkWAgEBAAHidwIqvbX06uAAAAAASUVORK5CYII=\",\"blurWidth\":8,\"blurHeight\":3}},\"width\":\"600px\",\"invertInDarkTheme\":true}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"对于RNN来说，利用历史状态和输入得到新的状态，只经过一个简单的\u003ccode class=\\\"x-inline-highlight\\\"\u003etanh\u003c/code\u003e激活层，而对于LSTM来说，它的示意图略显复杂：\"}}],[\"$\",\"$L10\",null,{\"src\":{\"default\":{\"src\":\"/_next/static/media/lstm1.83284ab3.png\",\"height\":839,\"width\":2233,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAADCAYAAACuyE5IAAAAbklEQVR4nAFjAJz/AdjwxzP47/0Q8OP5/h85Ee7mz/IdAwr/6BQgCwXn0PUUAdvxyv8AAAEA+vj2uwD8/UX9/gDvBQoH1AQEBj3///7MAcffxmwaFwTh29z0BQsH+QwNDgfo6O4EFB4aB//8/P3nf8QzvBHgmCgAAAAASUVORK5CYII=\",\"blurWidth\":8,\"blurHeight\":3}},\"width\":\"600px\",\"invertInDarkTheme\":true}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"在上图中，每条线表示一个向量，粉红色圆圈表示逐点式操作，黄色的方框是神经网络的层。这看起来很眼晕，不过我们接下来会一点点的解释图里的内容。\"}}],[\"$\",\"$L11\",null,{\"children\":\"门控单元\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"下面的结构称为门控单元：\"}}],[\"$\",\"$L10\",null,{\"src\":{\"default\":{\"src\":\"/_next/static/media/lstm2.96d0a9a3.png\",\"height\":242,\"width\":198,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAcAAAAICAYAAAA1BOUGAAAAoklEQVR42mNQlpb2YGBgcK1OS4u9tX37/3UTJzYB+fYmWlqeDDrKyoJADn9nUZH4ieXLO7fNmKEL5PM4mpoKMcBATTcDK5BaDsSyDEiA8fmjfkYoOxAhLMLI8P//HrDEzctduUf31+y7frGjCchlYgACoOReMOPu9Z5DV8+3/39wq+8ekMvKgAxcnbXYHOxUI60slIRQJMJDTGB2egIxM0wcAHV1L781GzudAAAAAElFTkSuQmCC\",\"blurWidth\":7,\"blurHeight\":8}},\"width\":\"100px\",\"invertInDarkTheme\":true}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"门控单元控制信息量通过的多少，通过向量\u003cspan class=\\\"katex\\\"\u003e\u003cspan class=\\\"katex-html\\\" aria-hidden=\\\"true\\\"\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:0.4306em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\" style=\\\"margin-right:0.04398em;\\\"\u003ez\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e来控制\u003cspan class=\\\"katex\\\"\u003e\u003cspan class=\\\"katex-html\\\" aria-hidden=\\\"true\\\"\u003e\u003cspan class=\\\"base\\\"\u003e\u003cspan class=\\\"strut\\\" style=\\\"height:0.4306em;\\\"\u003e\u003c/span\u003e\u003cspan class=\\\"mord mathnormal\\\"\u003ex\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e通过的信息量：\"}}],[\"$\",\"$L16\",null,{\"text\":\"o=\\\\sigma(z) \\\\otimes x\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"$17\"}}],[\"$\",\"$L11\",null,{\"children\":\"逐部分分析LSTM\"}],[\"$\",\"$L15\",null,{\"children\":\"遗忘门\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"$18\"}}],[\"$\",\"$L10\",null,{\"src\":{\"default\":{\"src\":\"/_next/static/media/lstm3.ac9b9ff5.png\",\"height\":564,\"width\":1826,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAACCAYAAABllJ3tAAAATUlEQVR4nGO8/eFkDjMzcyIDIyP7v7//Wvj4+FZsXreDPTe+/Pe3Lz8YGC8+3lPLzcebzczMyvzv378SIV7BhStXrucoTK7+/f3rz/8AtBQfB6GMnsQAAAAASUVORK5CYII=\",\"blurWidth\":8,\"blurHeight\":2}},\"width\":\"600px\",\"invertInDarkTheme\":true}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-gray\",\"children\":[[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"举一个概念性的例子：\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"考虑一个语言模型，输入一个句子：\u003ccode class=\\\"x-inline-highlight\\\"\u003eAlice是一名女教师，她喜欢给学生讲课；Bob是一位男司机，他喝酒上瘾。\u003c/code\u003e\"}}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"$19\"}}]]}],[\"$\",\"$L15\",null,{\"children\":\"输入门\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"$1a\"}}],[\"$\",\"$L10\",null,{\"src\":{\"default\":{\"src\":\"/_next/static/media/lstm4.00cef2df.png\",\"height\":564,\"width\":1826,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAACCAYAAABllJ3tAAAATUlEQVR4nGO8/+Vc1r+/f+MZmZjY//371ybDJ7Wqs38SV0tF76/fv/78Z7zy4nA9E8PfXA4ubub/jEyFSjyaCypaynl7m6b9/PP7738AfRAiC3RDuCcAAAAASUVORK5CYII=\",\"blurWidth\":8,\"blurHeight\":2}},\"width\":\"600px\",\"invertInDarkTheme\":true}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"经历这两步之后，便可以相加得到新的单元状态：\"}}],[\"$\",\"$L10\",null,{\"src\":{\"default\":{\"src\":\"/_next/static/media/lstm5.3f95c1ad.png\",\"height\":564,\"width\":1826,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAACCAYAAABllJ3tAAAATUlEQVR4nAFCAL3/AdToxGkDAgJeAwQDB/n6+ogXFxaqxcHIBVNCYP3+/v/+AdrvyX///v9uBAUEA////3EWDhWfsbS2B2VXcfz49vn9JsEjA+HlngsAAAAASUVORK5CYII=\",\"blurWidth\":8,\"blurHeight\":2}},\"width\":\"600px\",\"invertInDarkTheme\":true}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-gray\",\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"同理，当模型看到\u003ccode class=\\\"x-inline-highlight\\\"\u003eBob是一位男司机\u003c/code\u003e时，我们可能会想丢掉此前的语义信息\u003ccode class=\\\"x-inline-highlight\\\"\u003e女性\u003c/code\u003e，并把新的语义信息\u003ccode class=\\\"x-inline-highlight\\\"\u003e男性\u003c/code\u003e存入单元状态，使得后文输出正确的代词\u003ccode class=\\\"x-inline-highlight\\\"\u003e他\u003c/code\u003e。\"}}]}],[\"$\",\"$L15\",null,{\"children\":\"输出门\"}],[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"$1b\"}}],[\"$\",\"$L10\",null,{\"src\":{\"default\":{\"src\":\"/_next/static/media/lstm6.7eca3c69.png\",\"height\":564,\"width\":1826,\"blurDataURL\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAACCAYAAABllJ3tAAAATUlEQVR4nAFCAL3/Ad70zWgCAQFd+fb5B/n5+okYFxWrk4ycCYh/kvv7+vz+AdTpxIAHBwVv/Pn6A/j5/HAoHSWegYWKDIl9kvn//wD8vColBmKlkBoAAAAASUVORK5CYII=\",\"blurWidth\":8,\"blurHeight\":2}},\"width\":\"600px\",\"invertInDarkTheme\":true}],[\"$\",\"div\",null,{\"className\":\"x-highlightblock highlight-background-gray\",\"children\":[\"$\",\"p\",null,{\"className\":\"x-p\",\"dangerouslySetInnerHTML\":{\"__html\":\"当看到\u003ccode class=\\\"x-inline-highlight\\\"\u003eBob是一位男司机，他……\u003c/code\u003e时，由于出现了主语\u003ccode class=\\\"x-inline-highlight\\\"\u003e他\u003c/code\u003e，模型可能会输出和\u003ccode class=\\\"x-inline-highlight\\\"\u003e谓语动词\u003c/code\u003e有关的语义信息。\"}}]}]],null],\"segment\":\"__PAGE__\"},\"styles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/f70c26425a044d6b.css\",\"precedence\":\"next\",\"crossOrigin\":\"\"}]]}],\"segment\":\"learn-rnn-lstm\"},\"styles\":[]}],\"segment\":\"23d\"},\"styles\":[]}],\"params\":{}}],null],\"segment\":\"(blogs)\"},\"styles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/c84b60895b980f14.css\",\"precedence\":\"next\",\"crossOrigin\":\"\"}]]}]]}]}]}],null]}]]\n"])</script><script>self.__next_f.push([1,"6:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"1\",{\"children\":\"Create Next App\"}],[\"$\",\"meta\",\"2\",{\"name\":\"description\",\"content\":\"Generated by create next app\"}],[\"$\",\"meta\",\"3\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"link\",\"4\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}]]\nd:null\n"])</script></body></html>