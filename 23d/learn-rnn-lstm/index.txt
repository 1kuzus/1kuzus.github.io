2:I[55329,["481","static/chunks/457b8330-b5c9cf3fcf214847.js","954","static/chunks/d3ac728e-0c798b3b8aa3bf53.js","250","static/chunks/250-0ef8476c0fa8ee24.js","612","static/chunks/612-fa632c1349770315.js","551","static/chunks/551-19232c47cd1e883a.js","877","static/chunks/app/%5B...slug%5D/page-365a68497689ccdc.js"],""]
3:I[86510,["481","static/chunks/457b8330-b5c9cf3fcf214847.js","250","static/chunks/250-0ef8476c0fa8ee24.js","612","static/chunks/612-fa632c1349770315.js","931","static/chunks/app/page-83f042a81d8e4dec.js"],"PostMeta"]
4:"$Sreact.suspense"
6:I[30389,["481","static/chunks/457b8330-b5c9cf3fcf214847.js","954","static/chunks/d3ac728e-0c798b3b8aa3bf53.js","250","static/chunks/250-0ef8476c0fa8ee24.js","612","static/chunks/612-fa632c1349770315.js","551","static/chunks/551-19232c47cd1e883a.js","877","static/chunks/app/%5B...slug%5D/page-365a68497689ccdc.js"],""]
7:I[5613,[],""]
9:I[31778,[],""]
a:I[25694,["250","static/chunks/250-0ef8476c0fa8ee24.js","185","static/chunks/app/layout-731525953a9b1826.js"],"GlobalProvider"]
b:I[30397,["250","static/chunks/250-0ef8476c0fa8ee24.js","185","static/chunks/app/layout-731525953a9b1826.js"],""]
8:["slug","23d/learn-rnn-lstm","c"]
0:["lKLdySv0qPZzwe6tWJDMG",[[["",{"children":[["slug","23d/learn-rnn-lstm","c"],{"children":["__PAGE__?{\"slug\":[\"23d\",\"learn-rnn-lstm\"]}",{}]}]},"$undefined","$undefined",true],["",{"children":[["slug","23d/learn-rnn-lstm","c"],{"children":["__PAGE__",{},["$L1",[["$","$L2",null,{}],["$","div",null,{"id":"post-layout","children":[false,["$","div",null,{"id":"main","className":"center-wrapper","children":[["$","h1",null,{"className":"post-title","children":"学习RNN和LSTM"}],["$","$L3",null,{"path":"/23d/learn-rnn-lstm/"}],["$","$4",null,{"fallback":["$","p",null,{"children":"Loading component..."}],"children":"$L5"}]]}],["$","$L6",null,{}]]}]],null]]},["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children","$8","children"],"loading":"$undefined","loadingStyles":"$undefined","loadingScripts":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L9",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/466d61e46d9c1cc6.css","precedence":"next","crossOrigin":""}]]}]]},[null,["$","html",null,{"lang":"zh-CN","suppressHydrationWarning":true,"children":[["$","head",null,{"children":[["$","link",null,{"rel":"icon","href":"/favicon.ico","type":"image/x-icon"}],["$","script",null,{"async":true,"src":"https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"}],["$","script",null,{"async":true,"src":"https://www.googletagmanager.com/gtag/js?id=G-45BYSZ6WPY"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\nwindow.dataLayer = window.dataLayer || [];\nfunction gtag() {\n    dataLayer.push(arguments);\n}\ngtag('js', new Date());\ngtag('config', 'G-45BYSZ6WPY');\n"}}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"const a=z=>h.getItem(z),b=(y,z)=>h.setItem(y,z),c=(y,z)=>document.documentElement.setAttribute(y,z),d='theme',e='dark',f='light',g='class',h=localStorage;a(d)!==e&&a(d)!==f&&b(d,f);a(d)===e?c(g,e):c(g,f);"}}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\nif (!Array.prototype.findLast) {\n    Array.prototype.findLast = function (callback) {\n        for (let i = this.length - 1; i >= 0; i--) {\n            if (callback(this[i])) return this[i];\n        }\n        return undefined;\n    };\n}\nif (!Array.prototype.findLastIndex) {\n    Array.prototype.findLastIndex = function (callback) {\n        for (let i = this.length - 1; i >= 0; i--) {\n            if (callback(this[i])) return i;\n        }\n        return -1;\n    };\n}\n"}}]]}],["$","body",null,{"children":["$","$La",null,{"children":[["$","$Lb",null,{}],["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children"],"loading":"$undefined","loadingStyles":"$undefined","loadingScripts":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L9",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","div",null,{"id":"notfound","children":[["$","img",null,{"alt":"img","src":"/images/cry.gif"}],["$","code",null,{"id":"notfound-404","children":"404"}],["$","code",null,{"id":"notfound-text","children":"Page Not Found"}]]}],"notFoundStyles":[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/98e601cf5ba3633d.css","precedence":"next","crossOrigin":""}]],"styles":null}]]}]}]]}],null]],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/17b3c4485edf7c41.css","precedence":"next","crossOrigin":""}]],"$Lc"]]]]
d:I[74365,["481","static/chunks/457b8330-b5c9cf3fcf214847.js","954","static/chunks/d3ac728e-0c798b3b8aa3bf53.js","250","static/chunks/250-0ef8476c0fa8ee24.js","612","static/chunks/612-fa632c1349770315.js","551","static/chunks/551-19232c47cd1e883a.js","877","static/chunks/app/%5B...slug%5D/page-365a68497689ccdc.js"],""]
e:I[62029,["481","static/chunks/457b8330-b5c9cf3fcf214847.js","954","static/chunks/d3ac728e-0c798b3b8aa3bf53.js","250","static/chunks/250-0ef8476c0fa8ee24.js","612","static/chunks/612-fa632c1349770315.js","551","static/chunks/551-19232c47cd1e883a.js","877","static/chunks/app/%5B...slug%5D/page-365a68497689ccdc.js"],""]
f:T88c,<span class="token keyword">import</span> numpy
<span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt

<span class="token comment">#生成加噪声的正弦序列数据</span>
xlim<span class="token operator">=</span>numpy<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">36</span><span class="token punctuation">,</span><span class="token number">400</span><span class="token punctuation">)</span>
y<span class="token operator">=</span>numpy<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>xlim<span class="token punctuation">)</span><span class="token operator">+</span>numpy<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token operator">*</span>xlim<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token operator">*</span><span class="token number">0.2</span>

<span class="token comment">#转换dtype和size，保持和后面的训练数据统一</span>
y<span class="token operator">=</span>y<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">"float32"</span><span class="token punctuation">)</span>

plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>xlim<span class="token punctuation">,</span>y<span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
10:T113e,<span class="token keyword">class</span> <span class="token class-name">MyRNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>input_size<span class="token punctuation">,</span>hidden_size<span class="token punctuation">,</span>output_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear_ih<span class="token operator">=</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span>hidden_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear_hh<span class="token operator">=</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span>hidden_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear_ho<span class="token operator">=</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span>output_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>tanh<span class="token operator">=</span>nn<span class="token punctuation">.</span>Tanh<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">,</span>prev_state<span class="token punctuation">)</span><span class="token punctuation">:</span>
        curr_state<span class="token operator">=</span>self<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>
            self<span class="token punctuation">.</span>linear_ih<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token operator">+</span>self<span class="token punctuation">.</span>linear_hh<span class="token punctuation">(</span>prev_state<span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
        output<span class="token operator">=</span>self<span class="token punctuation">.</span>linear_ho<span class="token punctuation">(</span>curr_state<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> output<span class="token punctuation">,</span>curr_state

hidden_size<span class="token operator">=</span><span class="token number">12</span>
my_rnn<span class="token operator">=</span>MyRNN<span class="token punctuation">(</span>input_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>hidden_size<span class="token operator">=</span>hidden_size<span class="token punctuation">,</span>output_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
loss_func<span class="token operator">=</span>nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
optimizer<span class="token operator">=</span>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>my_rnn<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>
11:T80b,<span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>train_seq<span class="token punctuation">,</span>target<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment">#初始状态</span>
    state<span class="token operator">=</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>hidden_size<span class="token punctuation">)</span>

    <span class="token keyword">for</span> x <span class="token keyword">in</span> train_seq<span class="token punctuation">:</span>
        output<span class="token punctuation">,</span>state<span class="token operator">=</span>my_rnn<span class="token punctuation">(</span>x<span class="token punctuation">,</span>state<span class="token punctuation">)</span>

    loss<span class="token operator">=</span>loss_func<span class="token punctuation">(</span>output<span class="token punctuation">,</span>target<span class="token punctuation">)</span>
    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment">#返回损失</span>
    <span class="token keyword">return</span> loss<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
12:T5c4,train_datas<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">320</span><span class="token operator">-</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    train_seq<span class="token operator">=</span>torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>y<span class="token punctuation">[</span>i<span class="token punctuation">:</span>i<span class="token operator">+</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    target<span class="token operator">=</span>torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>y<span class="token punctuation">[</span>i<span class="token operator">+</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    train_datas<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">(</span>train_seq<span class="token punctuation">,</span>target<span class="token punctuation">)</span><span class="token punctuation">)</span>
13:T52e,metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span>
my_rnn<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> train_seq<span class="token punctuation">,</span>target <span class="token keyword">in</span> train_datas<span class="token punctuation">:</span>
    loss<span class="token operator">=</span>train<span class="token punctuation">(</span>train_seq<span class="token punctuation">,</span>target<span class="token punctuation">)</span>
    metrics<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>

plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">211</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>metrics<span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">"loss"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>
14:T1086,<span class="token keyword">def</span> <span class="token function">pred</span><span class="token punctuation">(</span>truth_seq<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        state<span class="token operator">=</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>hidden_size<span class="token punctuation">)</span>
        <span class="token keyword">for</span> x <span class="token keyword">in</span> truth_seq<span class="token punctuation">:</span>
            output<span class="token punctuation">,</span>state<span class="token operator">=</span>my_rnn<span class="token punctuation">(</span>x<span class="token punctuation">,</span>state<span class="token punctuation">)</span>
        <span class="token keyword">return</span> output

preds<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">320</span><span class="token operator">-</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">400</span><span class="token operator">-</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    truth_seq<span class="token operator">=</span>torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>y<span class="token punctuation">[</span>i<span class="token punctuation">:</span>i<span class="token operator">+</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    preds<span class="token punctuation">.</span>append<span class="token punctuation">(</span>pred<span class="token punctuation">(</span>truth_seq<span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
preds<span class="token operator">=</span>numpy<span class="token punctuation">.</span>array<span class="token punctuation">(</span>preds<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>

plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">212</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>xlim<span class="token punctuation">,</span>y<span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">"truth"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>xlim<span class="token punctuation">[</span><span class="token number">320</span><span class="token punctuation">:</span><span class="token number">400</span><span class="token punctuation">]</span><span class="token punctuation">,</span>preds<span class="token punctuation">,</span><span class="token string">"red"</span><span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">"predict"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
15:Tcae,<span class="token keyword">class</span> <span class="token class-name">MyRNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>input_size<span class="token punctuation">,</span>hidden_size<span class="token punctuation">,</span>output_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear_ih<span class="token operator">=</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span>hidden_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear_hh<span class="token operator">=</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span>hidden_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear_ho<span class="token operator">=</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span>output_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>tanh<span class="token operator">=</span>nn<span class="token punctuation">.</span>Tanh<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">,</span>prev_state<span class="token punctuation">)</span><span class="token punctuation">:</span>
        curr_state<span class="token operator">=</span>self<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>
            self<span class="token punctuation">.</span>linear_ih<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token operator">+</span>self<span class="token punctuation">.</span>linear_hh<span class="token punctuation">(</span>prev_state<span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
        output<span class="token operator">=</span>self<span class="token punctuation">.</span>linear_ho<span class="token punctuation">(</span>curr_state<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> output<span class="token punctuation">,</span>curr_state
16:Tb25,<span class="token keyword">class</span> <span class="token class-name">MyRNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>input_size<span class="token punctuation">,</span>hidden_size<span class="token punctuation">,</span>output_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>rnn<span class="token operator">=</span>nn<span class="token punctuation">.</span>RNN<span class="token punctuation">(</span>input_size<span class="token operator">=</span>input_size<span class="token punctuation">,</span>hidden_size<span class="token operator">=</span>hidden_size<span class="token punctuation">,</span>batch_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear_ho<span class="token operator">=</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span>output_size<span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>seq<span class="token punctuation">,</span>init_state<span class="token punctuation">)</span><span class="token punctuation">:</span>
        output_hidden<span class="token punctuation">,</span>tate<span class="token operator">=</span>self<span class="token punctuation">.</span>rnn<span class="token punctuation">(</span>seq<span class="token punctuation">,</span>init_state<span class="token punctuation">)</span>
        output<span class="token operator">=</span>self<span class="token punctuation">.</span>linear_ho<span class="token punctuation">(</span>output_hidden<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment">#取最后一个时间点的输出</span>
        <span class="token keyword">return</span> output<span class="token punctuation">,</span>state
17:T45c,<span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>train_seq<span class="token punctuation">,</span>target<span class="token punctuation">)</span><span class="token punctuation">:</span>
    state<span class="token operator">=</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>hidden_size<span class="token punctuation">)</span>
    <span class="token keyword">for</span> x <span class="token keyword">in</span> train_seq<span class="token punctuation">:</span>
        output<span class="token punctuation">,</span>state<span class="token operator">=</span>my_rnn<span class="token punctuation">(</span>x<span class="token punctuation">,</span>state<span class="token punctuation">)</span>
    loss<span class="token operator">=</span>loss_func<span class="token punctuation">(</span>output<span class="token punctuation">,</span>target<span class="token punctuation">)</span>
    <span class="token comment"># ......</span>
18:T488,<span class="token keyword">def</span> <span class="token function">pred</span><span class="token punctuation">(</span>truth_seq<span class="token punctuation">,</span>net<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        state<span class="token operator">=</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>hidden_size<span class="token punctuation">)</span>
        <span class="token keyword">for</span> x <span class="token keyword">in</span> truth_seq<span class="token punctuation">:</span>
            output<span class="token punctuation">,</span>state<span class="token operator">=</span>net<span class="token punctuation">(</span>x<span class="token punctuation">,</span>state<span class="token punctuation">)</span>
        <span class="token keyword">return</span> output
19:T40e,<span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>train_seq<span class="token punctuation">,</span>target<span class="token punctuation">)</span><span class="token punctuation">:</span>
    state<span class="token operator">=</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>hidden_size<span class="token punctuation">)</span>
    output<span class="token punctuation">,</span>_<span class="token operator">=</span>my_rnn<span class="token punctuation">(</span>train_seq<span class="token punctuation">,</span>state<span class="token punctuation">)</span> <span class="token comment">#一次得到输出</span>
    loss<span class="token operator">=</span>loss_func<span class="token punctuation">(</span>output<span class="token punctuation">,</span>target<span class="token punctuation">)</span>
    <span class="token comment"># ......</span>
1a:T40e,<span class="token keyword">def</span> <span class="token function">pred</span><span class="token punctuation">(</span>truth_seq<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        state<span class="token operator">=</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>hidden_size<span class="token punctuation">)</span>
        output<span class="token punctuation">,</span>_<span class="token operator">=</span>my_rnn<span class="token punctuation">(</span>truth_seq<span class="token punctuation">,</span>state<span class="token punctuation">)</span> <span class="token comment">#一次得到输出</span>
        <span class="token keyword">return</span> output
1b:T536,式子中<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">⊗</span></span></span></span>表示按位置相乘，<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mclose">)</span></span></span></span>的每个元素输出范围是<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span>，某个元素接近<code class="x-inline-highlight">1</code>，<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span>对应位置保留的信息就越多，反之就越少。1c:T5a2,LSTM的第一步是决定什么应该被遗忘，也就是对上一个<span class="x-inline-strong">单元</span><code class="x-inline-highlight">(cell)</code>状态信息选择性的遗忘。<br/>这个操作由遗忘门<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>实现，将其<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span>范围的输出按位置与单元上一时刻状态相乘。1d:T93e,当模型看到<code class="x-inline-highlight">Alice是一名女教师，……</code>时，单元状态中可能存储了和主语<code class="x-inline-highlight">Alice</code>和<code class="x-inline-highlight">女教师</code>有关的语义信息，以便在后文输出合适的代词<code class="x-inline-highlight">她</code>；然后，当模型看到<code class="x-inline-highlight">Alice是一名女教师，她喜欢给学生讲课；Bob是一位男司机，……</code>时，我们希望在看到新主语<code class="x-inline-highlight">Bob</code>和<code class="x-inline-highlight">男司机</code>之后，忘记此前存储的旧主语的性别语义。也就是对旧单元状态<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8917em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span>乘上较小的<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。1e:T4d3,下一步就是决定要在单元中存入什么新的信息。这一部分有两路：<code class="x-inline-highlight">tanh</code>这一路与普通RNN很像，生成一个中间状态；<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span></span>这一路被称为输入门<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8095em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">i</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，控制这个中间状态有多少信息被存入单元。1f:T6c6,最后是决定新的隐藏状态，这个输出会基于单元状态，但会经过门控单元。输出门<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>决定经过<code class="x-inline-highlight">tanh</code>的单元状态<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>有多少被输出到下一时刻的隐藏状态。5:[["$","h2",null,{"className":"x-h1","children":"RNN：一个简单的例子"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"传统神经网络每次的输入是独立的，每次输出只依赖于当前的输入；但在某些任务中需要更好的处理序列信息，即前面的输入和后面的输入是有关系的；<span class=\"x-inline-strong\">循环神经网络</span><code class=\"x-inline-highlight\">(Recurrent Neural Networks, RNN)</code>通过使用带自反馈的神经元，能够处理任意长度的序列。"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"下面是一个非常常见的RNN结构描述图。它展示了RNN的自反馈机制和与时间的依赖关系，但是对网络结构的描述容易引起误解：右侧的展开形式并不意味着网络有<code class=\"x-inline-highlight\">t</code>层，而是反映了随着时间增加（有时也可以理解为随着程序中循环的迭代），上一次输出的隐藏状态，和<span class=\"katex\"><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>共同作为网络的下一次的输入。"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"或者，如果说CNN是从空间维度上堆叠卷积层，不断加深，RNN就是从时间维度上的延展，而其网络真正的参数是很少的。"}}],["$","$Ld",null,{"src":"rnn1.png","width":"600px","filterDarkTheme":true}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"下面以一个简单的正弦序列预测任务出发，结合代码理解RNN网络的部分细节。"}}],["$","h3",null,{"className":"x-h2","children":"预测一个正弦序列"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"这个例子中，我们对一个加了噪声的正弦序列进行预测。"}}],["$","div",null,{"className":"x-codeblock","children":[["$","div",null,{"className":"x-codeblock-header","children":[["$","div",null,{"className":"x-codeblock-header-language","children":"Python"}],["$","$Le",null,{"className":"x-codeblock-header-copy","text":"import numpy\nimport torch\nfrom torch import nn\nimport matplotlib.pyplot as plt\n\n#生成加噪声的正弦序列数据\nxlim=numpy.linspace(0,36,400)\ny=numpy.sin(xlim)+numpy.random.rand(*xlim.shape)*0.2\n\n#转换dtype和size，保持和后面的训练数据统一\ny=y.reshape(-1,1).astype(\"float32\")\n\nplt.plot(xlim,y)\nplt.show()\n"}]]}],["$","pre",null,{"style":{"background":null},"children":["$","code",null,{"dangerouslySetInnerHTML":{"__html":"$f"}}]}]]}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"代码中我们在区间<span class=\"katex\"><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord\">0</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">36</span><span class=\"mclose\">]</span></span></span></span>取了<code class=\"x-inline-highlight\">400</code>点数据，如果把横轴看成时间轴，可以认为数据集中有<code class=\"x-inline-highlight\">400</code>个连续时间点的数据。"}}],["$","$Ld",null,{"src":"fig1.png","width":"600px","filterDarkTheme":true}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"现在明确一下我们的方案："}}],["$","div",null,{"className":"x-oli","children":[["$","div",null,{"className":"x-oli-number","children":"1."}],["$","div",null,{"className":"x-oli-content-wrapper","children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"使用前<code class=\"x-inline-highlight\">80%</code>也就是前<code class=\"x-inline-highlight\">320</code>个数据作为训练集，剩余的作为测试集，观察预测结果。"}}]}]]}],["$","div",null,{"className":"x-oli","children":[["$","div",null,{"className":"x-oli-number","children":"2."}],["$","div",null,{"className":"x-oli-content-wrapper","children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"序列长度为<code class=\"x-inline-highlight\">10</code>，也就是模型根据前<code class=\"x-inline-highlight\">10</code>个时间点的数据去预测下一个时间点的数据。"}}]}]]}],["$","div",null,{"className":"x-oli","children":[["$","div",null,{"className":"x-oli-number","children":"3."}],["$","div",null,{"className":"x-oli-content-wrapper","children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"这个例子中输入特征的维度是<code class=\"x-inline-highlight\">1</code>，也就是只有<code class=\"x-inline-highlight\">y</code>值一个指标。此外，也不考虑批量大小。"}}]}]]}],["$","h3",null,{"className":"x-h2","children":"定义RNN网络"}],["$","div",null,{"className":"x-codeblock","children":[["$","div",null,{"className":"x-codeblock-header","children":[["$","div",null,{"className":"x-codeblock-header-language","children":"Python"}],["$","$Le",null,{"className":"x-codeblock-header-copy","text":"class MyRNN(nn.Module):\n    def __init__(self,input_size,hidden_size,output_size):\n        super().__init__()\n        self.linear_ih=nn.Linear(input_size,hidden_size)\n        self.linear_hh=nn.Linear(hidden_size,hidden_size)\n        self.linear_ho=nn.Linear(hidden_size,output_size)\n        self.tanh=nn.Tanh()\n    def forward(self,x,prev_state):\n        curr_state=self.tanh(\n            self.linear_ih(x)+self.linear_hh(prev_state)\n        )\n        output=self.linear_ho(curr_state).reshape(1)\n        return output,curr_state\n\nhidden_size=12\nmy_rnn=MyRNN(input_size=1,hidden_size=hidden_size,output_size=1)\nloss_func=nn.MSELoss()\noptimizer=torch.optim.SGD(my_rnn.parameters(),lr=0.01)\n"}]]}],["$","pre",null,{"style":{"background":null},"children":["$","code",null,{"dangerouslySetInnerHTML":{"__html":"$10"}}]}]]}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"这个是一个简单的RNN结构，从网络参数和结构来看很像一个<code class=\"x-inline-highlight\">输入层-隐藏层-输出层</code>的感知机，但是多了一步<code class=\"x-inline-highlight\">隐藏层-隐藏层</code>的连接，RNN的反馈结构就是由此体现的。"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"并且，注意到<code class=\"x-inline-highlight\">forward</code>函数的输入也需要两个参数：当前时刻输入<code class=\"x-inline-highlight\">x</code>和前一时刻状态<code class=\"x-inline-highlight\">prev_state</code>，同时也会把计算后的新状态<code class=\"x-inline-highlight\">curr_state</code>和<code class=\"x-inline-highlight\">output</code>一起返回，供下一次计算使用。在这里，经过<code class=\"x-inline-highlight\">linear_ho</code>后，<code class=\"x-inline-highlight\">output</code>的<code class=\"x-inline-highlight\">size</code>为<code class=\"x-inline-highlight\">(1,1)</code>，考虑到它仅仅是一个标量，我们把它<code class=\"x-inline-highlight\">resize</code>为<code class=\"x-inline-highlight\">(1)</code>。"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"接下来设置了一些超参数，隐藏层有<code class=\"x-inline-highlight\">12</code>个神经元，损失函数使用<code class=\"x-inline-highlight\">MSELoss()</code>。"}}],["$","h3",null,{"className":"x-h2","children":"训练"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"首先定义这样的训练函数：它传入一个序列<code class=\"x-inline-highlight\">train_seq</code>和目标<code class=\"x-inline-highlight\">target</code>。<code class=\"x-inline-highlight\">train_seq</code>的<code class=\"x-inline-highlight\">size</code>应该为<code class=\"x-inline-highlight\">(10,1)</code>，因为我们用前<code class=\"x-inline-highlight\">10</code>个时间点的数据去预测下一个，而输入特征维度是<code class=\"x-inline-highlight\">1</code>；<code class=\"x-inline-highlight\">target</code>的<code class=\"x-inline-highlight\">size</code>应该为<code class=\"x-inline-highlight\">(1)</code>，因为输出只是一个标量。注意我们循环依次输入<code class=\"x-inline-highlight\">train_seq</code>中的<code class=\"x-inline-highlight\">10</code>个数据，迭代更新<code class=\"x-inline-highlight\">state</code>，用最后一次的<code class=\"x-inline-highlight\">output</code>作为最终的输出计算损失。"}}],["$","div",null,{"className":"x-codeblock","children":[["$","div",null,{"className":"x-codeblock-header","children":[["$","div",null,{"className":"x-codeblock-header-language","children":"Python"}],["$","$Le",null,{"className":"x-codeblock-header-copy","text":"def train(train_seq,target):\n    #初始状态\n    state=torch.zeros(1,hidden_size)\n\n    for x in train_seq:\n        output,state=my_rnn(x,state)\n\n    loss=loss_func(output,target)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    #返回损失\n    return loss.detach().numpy().reshape(1)\n"}]]}],["$","pre",null,{"style":{"background":null},"children":["$","code",null,{"dangerouslySetInnerHTML":{"__html":"$11"}}]}]]}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"下面代码把真实数据划分成："}}],["$","div",null,{"className":"x-table-wrapper","children":["$","table",null,{"className":"x-table","children":["$","tbody",null,{"children":[["$","tr","0",{"children":[["$","th","0",{"className":null,"width":null,"children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">train_seq</code>"}}]}],["$","th","1",{"className":null,"width":null,"children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">target</code>"}}]}]]}],["$","tr","1",{"children":[["$","td","0",{"className":null,"width":null,"children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">y[0]</code>,<code class=\"x-inline-highlight\">y[1]</code>,<code class=\"x-inline-highlight\">y[2]</code>, ... ,<code class=\"x-inline-highlight\">y[9]</code>"}}]}],["$","td","1",{"className":null,"width":null,"children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">y[10]</code>"}}]}]]}],["$","tr","2",{"children":[["$","td","0",{"className":null,"width":null,"children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">y[1]</code>,<code class=\"x-inline-highlight\">y[2]</code>,<code class=\"x-inline-highlight\">y[3]</code>, ... ,<code class=\"x-inline-highlight\">y[10]</code>"}}]}],["$","td","1",{"className":null,"width":null,"children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">y[11]</code>"}}]}]]}],["$","tr","3",{"children":[["$","td","0",{"className":null,"width":null,"children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">y[2]</code>,<code class=\"x-inline-highlight\">y[3]</code>,<code class=\"x-inline-highlight\">y[4]</code>, ... ,<code class=\"x-inline-highlight\">y[11]</code>"}}]}],["$","td","1",{"className":null,"width":null,"children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">y[12]</code>"}}]}]]}],["$","tr","4",{"children":[["$","td","0",{"className":null,"width":null,"children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"..."}}]}],["$","td","1",{"className":null,"width":null,"children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"..."}}]}]]}],["$","tr","5",{"children":[["$","td","0",{"className":null,"width":null,"children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">y[309]</code>,<code class=\"x-inline-highlight\">y[310]</code>,<code class=\"x-inline-highlight\">y[311]</code>, ... ,<code class=\"x-inline-highlight\">y[318]</code>"}}]}],["$","td","1",{"className":null,"width":null,"children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">y[319]</code>"}}]}]]}]]}]}]}],["$","div",null,{"className":"x-codeblock","children":[["$","div",null,{"className":"x-codeblock-header","children":[["$","div",null,{"className":"x-codeblock-header-language","children":"Python"}],["$","$Le",null,{"className":"x-codeblock-header-copy","text":"train_datas=[]\nfor i in range(320-10):\n    train_seq=torch.from_numpy(y[i:i+10])\n    target=torch.from_numpy(y[i+10])\n    train_datas.append((train_seq,target))\n"}]]}],["$","pre",null,{"style":{"background":null},"children":["$","code",null,{"dangerouslySetInnerHTML":{"__html":"$12"}}]}]]}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"训练网络，绘制误差："}}],["$","div",null,{"className":"x-codeblock","children":[["$","div",null,{"className":"x-codeblock-header","children":[["$","div",null,{"className":"x-codeblock-header-language","children":"Python"}],["$","$Le",null,{"className":"x-codeblock-header-copy","text":"metrics=[]\nmy_rnn.train()\nfor train_seq,target in train_datas:\n    loss=train(train_seq,target)\n    metrics.append(loss)\n\nplt.subplot(211)\nplt.plot(metrics,label=\"loss\")\nplt.legend()\n"}]]}],["$","pre",null,{"style":{"background":null},"children":["$","code",null,{"dangerouslySetInnerHTML":{"__html":"$13"}}]}]]}],["$","h3",null,{"className":"x-h2","children":"预测"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"这个例子中我们用<span class=\"x-inline-strong\">单步预测</span>观察模型的效果。在单步预测时，每次预测都全部使用真实值；当然，我们可以这样做是因为验证集中本来就包含了真实的数据，换句话说，我们是在已知<code class=\"x-inline-highlight\">t+1</code>时刻的真实数据的情况下，去看看模型使用<code class=\"x-inline-highlight\">t-9</code>~<code class=\"x-inline-highlight\">t</code>时刻的数据，对<code class=\"x-inline-highlight\">t+1</code>时刻的预测值。"}}],["$","div",null,{"className":"x-table-wrapper","children":["$","table",null,{"className":"x-table","children":["$","tbody",null,{"children":[["$","tr","0",{"children":[["$","th","0",{"className":null,"width":null,"children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">input</code>"}}]}],["$","th","1",{"className":null,"width":null,"children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">prediction</code>"}}]}]]}],["$","tr","1",{"children":[["$","td","0",{"className":null,"width":null,"children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">y[310]</code>,<code class=\"x-inline-highlight\">y[311]</code>,<code class=\"x-inline-highlight\">y[312]</code>, ... ,<code class=\"x-inline-highlight\">y[319]</code>"}}]}],["$","td","1",{"className":null,"width":null,"children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">pred[320]</code>"}}]}]]}],["$","tr","2",{"children":[["$","td","0",{"className":null,"width":null,"children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">y[311]</code>,<code class=\"x-inline-highlight\">y[312]</code>,<code class=\"x-inline-highlight\">y[313]</code>, ... ,<code class=\"x-inline-highlight\">y[320]</code>"}}]}],["$","td","1",{"className":null,"width":null,"children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">pred[321]</code>"}}]}]]}],["$","tr","3",{"children":[["$","td","0",{"className":null,"width":null,"children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">y[312]</code>,<code class=\"x-inline-highlight\">y[313]</code>,<code class=\"x-inline-highlight\">y[314]</code>, ... ,<code class=\"x-inline-highlight\">y[321]</code>"}}]}],["$","td","1",{"className":null,"width":null,"children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">pred[322]</code>"}}]}]]}],["$","tr","4",{"children":[["$","td","0",{"className":null,"width":null,"children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"..."}}]}],["$","td","1",{"className":null,"width":null,"children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"..."}}]}]]}]]}]}]}],["$","div",null,{"className":"x-codeblock","children":[["$","div",null,{"className":"x-codeblock-header","children":[["$","div",null,{"className":"x-codeblock-header-language","children":"Python"}],["$","$Le",null,{"className":"x-codeblock-header-copy","text":"def pred(truth_seq):\n    with torch.no_grad():\n        state=torch.zeros(1,hidden_size)\n        for x in truth_seq:\n            output,state=my_rnn(x,state)\n        return output\n\npreds=[]\nfor i in range(320-10,400-10):\n    truth_seq=torch.from_numpy(y[i:i+10])\n    preds.append(pred(truth_seq).numpy())\npreds=numpy.array(preds).reshape(-1,1)\n\nplt.subplot(212)\nplt.plot(xlim,y,label=\"truth\")\nplt.plot(xlim[320:400],preds,\"red\",label=\"predict\")\nplt.legend()\nplt.show()\n"}]]}],["$","pre",null,{"style":{"background":null},"children":["$","code",null,{"dangerouslySetInnerHTML":{"__html":"$14"}}]}]]}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"如果我们不只是在测试集上评估模型性能，而是去预测真实生活中的问题，例如未来<code class=\"x-inline-highlight\">7</code>天的温度；或者假如我们的数据集到<code class=\"x-inline-highlight\">y[319]</code>就截止了，这时如果想得到后面多个时刻的数据，就需要<span class=\"x-inline-strong\">多步预测</span>，此时上一时刻的预测会被当做新的输入："}}],["$","div",null,{"className":"x-table-wrapper","children":["$","table",null,{"className":"x-table","children":["$","tbody",null,{"children":[["$","tr","0",{"children":[["$","th","0",{"className":null,"width":null,"children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">input</code>"}}]}],["$","th","1",{"className":null,"width":null,"children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">prediction</code>"}}]}]]}],["$","tr","1",{"children":[["$","td","0",{"className":null,"width":null,"children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">y[310]</code>,<code class=\"x-inline-highlight\">y[311]</code>,<code class=\"x-inline-highlight\">y[312]</code>, ... ,<code class=\"x-inline-highlight\">y[319]</code>"}}]}],["$","td","1",{"className":null,"width":null,"children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">pred[320]</code>"}}]}]]}],["$","tr","2",{"children":[["$","td","0",{"className":null,"width":null,"children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">y[311]</code>,<code class=\"x-inline-highlight\">y[312]</code>,<code class=\"x-inline-highlight\">y[313]</code>, ... ,<code class=\"x-inline-highlight\">pred[320]</code>"}}]}],["$","td","1",{"className":null,"width":null,"children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">pred[321]</code>"}}]}]]}],["$","tr","3",{"children":[["$","td","0",{"className":null,"width":null,"children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">y[312]</code>,<code class=\"x-inline-highlight\">y[313]</code>,<code class=\"x-inline-highlight\">y[314]</code>, ... ,<code class=\"x-inline-highlight\">pred[320]</code>,<code class=\"x-inline-highlight\">pred[321]</code>"}}]}],["$","td","1",{"className":null,"width":null,"children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">pred[322]</code>"}}]}]]}],["$","tr","4",{"children":[["$","td","0",{"className":null,"width":null,"children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"..."}}]}],["$","td","1",{"className":null,"width":null,"children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"..."}}]}]]}]]}]}]}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"多步预测会导致误差的累积。"}}],["$","h3",null,{"className":"x-h2","children":"看看效果"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"如果不执行训练步骤的代码，使用初始随机参数的模型预测结果是："}}],["$","$Ld",null,{"src":"fig2.png","width":"600px","filterDarkTheme":true}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"经过训练后，每次训练的<code class=\"x-inline-highlight\">loss</code>和最终的预测："}}],["$","$Ld",null,{"src":"fig3.png","width":"600px","filterDarkTheme":true}],["$","h3",null,{"className":"x-h2","children":"使用torch.nn.RNN"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"使用<code class=\"x-inline-highlight\">torch.nn.RNN</code>模块时，与上面例子中手动实现的RNN有几处细小的区别，下面给出了使用<code class=\"x-inline-highlight\">torch.nn.RNN</code>时需要做出的修改："}}],["$","div",null,{"className":"x-oli","children":[["$","div",null,{"className":"x-oli-number","children":"1."}],["$","div",null,{"className":"x-oli-content-wrapper","children":[["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"定义模型时，不再需要显式指定<code class=\"x-inline-highlight\">linear_ih</code>和<code class=\"x-inline-highlight\">linear_hh</code>两层，将由<code class=\"x-inline-highlight\">nn.RNN</code>模块实现；<code class=\"x-inline-highlight\">nn.RNN</code>模块没有定义输出层，因此输出层<code class=\"x-inline-highlight\">linear_ho</code>需要设置。<br/>在<code class=\"x-inline-highlight\">forward</code>函数中，手动实现时为了直观展示出RNN的迭代过程，只进行了一次隐藏状态的更新；而对于输入序列迭代更新隐藏状态是在训练和预测时实现的。而<code class=\"x-inline-highlight\">nn.RNN</code>模块的一次<code class=\"x-inline-highlight\">forward</code>就已经完成了迭代更新，其输入是整个序列<code class=\"x-inline-highlight\">seq</code>和<code class=\"x-inline-highlight\">prev_state</code>，返回值是<code class=\"x-inline-highlight\">output_hidden,curr_state</code>，对于不考虑批量大小的数据，它们的<code class=\"x-inline-highlight\">size</code>为："}}],["$","div",null,{"className":"x-uli","children":[["$","div",null,{"className":"x-uli-marker","children":["$","div",null,{"className":"x-uli-marker-dot"}]}],["$","div",null,{"className":"x-uli-content-wrapper","children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">seq</code>: <code class=\"x-inline-highlight\">(sequence_length, input_size)</code>"}}]}]]}],["$","div",null,{"className":"x-uli","children":[["$","div",null,{"className":"x-uli-marker","children":["$","div",null,{"className":"x-uli-marker-dot"}]}],["$","div",null,{"className":"x-uli-content-wrapper","children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">init_state</code>: <code class=\"x-inline-highlight\">(1, hidden_size)</code>"}}]}]]}],["$","div",null,{"className":"x-uli","children":[["$","div",null,{"className":"x-uli-marker","children":["$","div",null,{"className":"x-uli-marker-dot"}]}],["$","div",null,{"className":"x-uli-content-wrapper","children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">output_hidden</code>: <code class=\"x-inline-highlight\">(sequence_length, hidden_size)</code>"}}]}]]}],["$","div",null,{"className":"x-uli","children":[["$","div",null,{"className":"x-uli-marker","children":["$","div",null,{"className":"x-uli-marker-dot"}]}],["$","div",null,{"className":"x-uli-content-wrapper","children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">state</code>: <code class=\"x-inline-highlight\">(1, hidden_size)</code>"}}]}]]}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"最后在我们定义的<code class=\"x-inline-highlight\">MyRNN</code>模块中，用<code class=\"x-inline-highlight\">output_hidden</code>的最后一个时间点的输出，经过输出层得到最终的<code class=\"x-inline-highlight\">output</code>。"}}]]}]]}],["$","div",null,{"className":"x-highlightblock highlight-background-gray","children":[["$","h4",null,{"className":"x-h3","children":"手动实现"}],["$","div",null,{"className":"x-codeblock","children":[["$","div",null,{"className":"x-codeblock-header","children":[["$","div",null,{"className":"x-codeblock-header-language","children":"Python"}],["$","$Le",null,{"className":"x-codeblock-header-copy","text":"class MyRNN(nn.Module):\n    def __init__(self,input_size,hidden_size,output_size):\n        super().__init__()\n        self.linear_ih=nn.Linear(input_size,hidden_size)\n        self.linear_hh=nn.Linear(hidden_size,hidden_size)\n        self.linear_ho=nn.Linear(hidden_size,output_size)\n        self.tanh=nn.Tanh()\n    def forward(self,x,prev_state):\n        curr_state=self.tanh(\n            self.linear_ih(x)+self.linear_hh(prev_state)\n        )\n        output=self.linear_ho(curr_state).reshape(1)\n        return output,curr_state\n"}]]}],["$","pre",null,{"style":{"background":null},"children":["$","code",null,{"dangerouslySetInnerHTML":{"__html":"$15"}}]}]]}],["$","h4",null,{"className":"x-h3","children":"使用torch.nn.RNN"}],["$","div",null,{"className":"x-codeblock","children":[["$","div",null,{"className":"x-codeblock-header","children":[["$","div",null,{"className":"x-codeblock-header-language","children":"Python"}],["$","$Le",null,{"className":"x-codeblock-header-copy","text":"class MyRNN(nn.Module):\n    def __init__(self,input_size,hidden_size,output_size):\n        super().__init__()\n        self.rnn=nn.RNN(input_size=input_size,hidden_size=hidden_size,batch_first=True)\n        self.linear_ho=nn.Linear(hidden_size,output_size)\n    def forward(self,seq,init_state):\n        output_hidden,tate=self.rnn(seq,init_state)\n        output=self.linear_ho(output_hidden[-1,:]) #取最后一个时间点的输出\n        return output,state\n"}]]}],["$","pre",null,{"style":{"background":null},"children":["$","code",null,{"dangerouslySetInnerHTML":{"__html":"$16"}}]}]]}]]}],["$","div",null,{"className":"x-oli","children":[["$","div",null,{"className":"x-oli-number","children":"2."}],["$","div",null,{"className":"x-oli-content-wrapper","children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"训练和预测时，也不需要再遍历序列，迭代的过程已经在<code class=\"x-inline-highlight\">nn.RNN</code>模块内部实现。"}}]}]]}],["$","div",null,{"className":"x-highlightblock highlight-background-gray","children":[["$","h4",null,{"className":"x-h3","children":"手动实现"}],["$","div",null,{"className":"x-codeblock","children":[["$","div",null,{"className":"x-codeblock-header","children":[["$","div",null,{"className":"x-codeblock-header-language","children":"Python"}],["$","$Le",null,{"className":"x-codeblock-header-copy","text":"def train(train_seq,target):\n    state=torch.zeros(1,hidden_size)\n    for x in train_seq:\n        output,state=my_rnn(x,state)\n    loss=loss_func(output,target)\n    # ......\n"}]]}],["$","pre",null,{"style":{"background":null},"children":["$","code",null,{"dangerouslySetInnerHTML":{"__html":"$17"}}]}]]}],["$","div",null,{"className":"x-codeblock","children":[["$","div",null,{"className":"x-codeblock-header","children":[["$","div",null,{"className":"x-codeblock-header-language","children":"Python"}],["$","$Le",null,{"className":"x-codeblock-header-copy","text":"def pred(truth_seq,net):\n    with torch.no_grad():\n        state=torch.zeros(1,hidden_size)\n        for x in truth_seq:\n            output,state=net(x,state)\n        return output\n"}]]}],["$","pre",null,{"style":{"background":null},"children":["$","code",null,{"dangerouslySetInnerHTML":{"__html":"$18"}}]}]]}],["$","h4",null,{"className":"x-h3","children":"使用torch.nn.RNN"}],["$","div",null,{"className":"x-codeblock","children":[["$","div",null,{"className":"x-codeblock-header","children":[["$","div",null,{"className":"x-codeblock-header-language","children":"Python"}],["$","$Le",null,{"className":"x-codeblock-header-copy","text":"def train(train_seq,target):\n    state=torch.zeros(1,hidden_size)\n    output,_=my_rnn(train_seq,state) #一次得到输出\n    loss=loss_func(output,target)\n    # ......\n"}]]}],["$","pre",null,{"style":{"background":null},"children":["$","code",null,{"dangerouslySetInnerHTML":{"__html":"$19"}}]}]]}],["$","div",null,{"className":"x-codeblock","children":[["$","div",null,{"className":"x-codeblock-header","children":[["$","div",null,{"className":"x-codeblock-header-language","children":"Python"}],["$","$Le",null,{"className":"x-codeblock-header-copy","text":"def pred(truth_seq):\n    with torch.no_grad():\n        state=torch.zeros(1,hidden_size)\n        output,_=my_rnn(truth_seq,state) #一次得到输出\n        return output\n"}]]}],["$","pre",null,{"style":{"background":null},"children":["$","code",null,{"dangerouslySetInnerHTML":{"__html":"$1a"}}]}]]}]]}],["$","h3",null,{"className":"x-h2","children":"FAQ"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"初次了解RNN时，我在一些问题上困惑了很久。这个版块是对它们的再次整理。（尽管有些已经包含在上述例子中了！）"}}],["$","h4",null,{"className":"x-h3","children":"用10步预测下1步，为什么 input_size 不是10，而是1？"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">input_size</code>与序列长度并非同一个概念。用前<code class=\"x-inline-highlight\">10</code>个时间点的数据去预测下一个，这里的<code class=\"x-inline-highlight\">10</code>是序列长度；而<code class=\"x-inline-highlight\">input_size</code>是输入特征的维度。由于这个例子较为简单，只是用历史的<code class=\"x-inline-highlight\">y</code>值预测新的<code class=\"x-inline-highlight\">y</code>值，因此特征只有<code class=\"x-inline-highlight\">1</code>维。"}}],["$","h4",null,{"className":"x-h3","children":"什么时候 input_size 不是1？"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"例如我们在预测未来气温时，历史气温数据并不是唯一的参考，还可能参考历史的风速、气压、天气情况等等，此时输入数据将会是一个<code class=\"x-inline-highlight\">input_size</code>维的向量。"}}],["$","h4",null,{"className":"x-h3","children":"hidden_size=12，12是在哪里体现的？"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">12</code>只是模型的超参数，和MLP中隐藏层大小一样，并没有太多的物理含义。"}}],["$","h4",null,{"className":"x-h3","children":"训练时，每次迭代用哪些数据？应该遍历几遍数据集？每个 epoch 会使用哪些数据进行参数优化？"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"在训练一个CNN网络时（例如一个图片分类网络），策略通常是："}}],["$","div",null,{"className":"x-oli","children":[["$","div",null,{"className":"x-oli-number","children":"1."}],["$","div",null,{"className":"x-oli-content-wrapper","children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"指定超参数<code class=\"x-inline-highlight\">num_epoch</code>，在每个<code class=\"x-inline-highlight\">epoch</code>中随机遍历训练集中的所有图像进行参数优化；"}}]}]]}],["$","div",null,{"className":"x-oli","children":[["$","div",null,{"className":"x-oli-number","children":"2."}],["$","div",null,{"className":"x-oli-content-wrapper","children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"重复执行<code class=\"x-inline-highlight\">num_epoch</code>次。"}}]}]]}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"然而对于RNN来说这个概念似乎并不清晰，例如上述例子的训练策略是："}}],["$","div",null,{"className":"x-oli","children":[["$","div",null,{"className":"x-oli-number","children":"1."}],["$","div",null,{"className":"x-oli-content-wrapper","children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"从<code class=\"x-inline-highlight\">0</code>到<code class=\"x-inline-highlight\">(训练集大小 - 序列长度)</code>依次遍历起始时间<code class=\"x-inline-highlight\">t</code>；"}}]}]]}],["$","div",null,{"className":"x-oli","children":[["$","div",null,{"className":"x-oli-number","children":"2."}],["$","div",null,{"className":"x-oli-content-wrapper","children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"对于每个起始时间<code class=\"x-inline-highlight\">t</code>，将<code class=\"x-inline-highlight\">y[t]</code>~<code class=\"x-inline-highlight\">y[t+9]</code>为输入，<code class=\"x-inline-highlight\">y[t+10]</code>为真值作为一组训练样本。"}}]}]]}],["$","div",null,{"className":"x-oli","children":[["$","div",null,{"className":"x-oli-number","children":"3."}],["$","div",null,{"className":"x-oli-content-wrapper","children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"第<code class=\"x-inline-highlight\">1</code>步只遍历了一次！"}}]}]]}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"或者："}}],["$","div",null,{"className":"x-oli","children":[["$","div",null,{"className":"x-oli-number","children":"1."}],["$","div",null,{"className":"x-oli-content-wrapper","children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"..."}}]}]]}],["$","div",null,{"className":"x-oli","children":[["$","div",null,{"className":"x-oli-number","children":"2."}],["$","div",null,{"className":"x-oli-content-wrapper","children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"..."}}]}]]}],["$","div",null,{"className":"x-oli","children":[["$","div",null,{"className":"x-oli-number","children":"3."}],["$","div",null,{"className":"x-oli-content-wrapper","children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"前两步同上，但多次遍历训练集。"}}]}]]}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"另一个常用的策略是："}}],["$","div",null,{"className":"x-oli","children":[["$","div",null,{"className":"x-oli-number","children":"1."}],["$","div",null,{"className":"x-oli-content-wrapper","children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"指定超参数：训练轮次<code class=\"x-inline-highlight\">num_iter</code>；"}}]}]]}],["$","div",null,{"className":"x-oli","children":[["$","div",null,{"className":"x-oli-number","children":"2."}],["$","div",null,{"className":"x-oli-content-wrapper","children":["$","p",null,{"dangerouslySetInnerHTML":{"__html":"重复执行<code class=\"x-inline-highlight\">num_iter</code>次，每次随机抽取一个起始时间<code class=\"x-inline-highlight\">t</code>，并且将<code class=\"x-inline-highlight\">y[t]</code>~<code class=\"x-inline-highlight\">y[t+9]</code>为输入，<code class=\"x-inline-highlight\">y[t+10]</code>为真值作为一组训练样本。"}}]}]]}],["$","h2",null,{"className":"x-h1","children":"LSTM：一篇很好的博客"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"以下的内容和插图总结或翻译自这篇的英文博客：<a href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/\" target=\"_blank\" rel=\"noreferrer\" class=\"x-inline-link\">Understanding LSTM Networks</a>"}}],["$","h3",null,{"className":"x-h2","children":"长期依赖问题"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"RNN可以利用先前的信息理解当前的任务，这点非常不错；有时我们只需要短期的信息，例如一个语言模型预测下面的句子：<br/><code class=\"x-inline-highlight\">天空中飘着一朵白色的【云】</code>，这很简单。但有些时候我们需要更多背景信息，例如：<br/><code class=\"x-inline-highlight\">我出生在法国，…… ，我可以说流利的【法语】</code>，这个情况下，随着前后文距离变大，RNN对长期依赖关系的学习会变得困难。"}}],["$","h3",null,{"className":"x-h2","children":"LSTM"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">长短期记忆网络</span><code class=\"x-inline-highlight\">(Long Short-Term Memory, LSTM)</code>是一种特殊的RNN，可以学习长期依赖。以RNN为例，循环神经网络随时间展开通常具有如下的示意图："}}],["$","$Ld",null,{"src":"rnn2.png","width":"600px","filterDarkTheme":true}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"对于RNN来说，利用历史状态和输入得到新的状态，只经过一个简单的<code class=\"x-inline-highlight\">tanh</code>激活层，而对于LSTM来说，它的示意图略显复杂："}}],["$","$Ld",null,{"src":"lstm1.png","width":"600px","filterDarkTheme":true}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"在上图中，每条线表示一个向量，粉红色圆圈表示逐点式操作，黄色的方框是神经网络的层。这看起来很眼晕，不过我们接下来会一点点的解释图里的内容。"}}],["$","h3",null,{"className":"x-h2","children":"门控单元"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"下面的结构称为门控单元："}}],["$","$Ld",null,{"src":"lstm2.png","width":"100px","filterDarkTheme":true}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"门控单元控制信息量通过的多少，通过向量<span class=\"katex\"><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span></span></span></span>来控制<span class=\"katex\"><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">x</span></span></span></span>通过的信息量："}}],["$","div",null,{"className":"x-formula","dangerouslySetInnerHTML":{"__html":"<span class=\"katex\"><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">o</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">⊗</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">x</span></span></span></span>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"$1b"}}],["$","h3",null,{"className":"x-h2","children":"逐部分分析LSTM"}],["$","h4",null,{"className":"x-h3","children":"遗忘门"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"$1c"}}],["$","$Ld",null,{"src":"lstm3.png","width":"600px","filterDarkTheme":true}],["$","div",null,{"className":"x-highlightblock highlight-background-gray","children":[["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"举一个概念性的例子："}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"考虑一个语言模型，输入一个句子：<code class=\"x-inline-highlight\">Alice是一名女教师，她喜欢给学生讲课；Bob是一位男司机，他喝酒上瘾。</code>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"$1d"}}]]}],["$","h4",null,{"className":"x-h3","children":"输入门"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"$1e"}}],["$","$Ld",null,{"src":"lstm4.png","width":"600px","filterDarkTheme":true}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"经历这两步之后，便可以相加得到新的单元状态："}}],["$","$Ld",null,{"src":"lstm5.png","width":"600px","filterDarkTheme":true}],["$","div",null,{"className":"x-highlightblock highlight-background-gray","children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"同理，当模型看到<code class=\"x-inline-highlight\">Bob是一位男司机</code>时，我们可能会想丢掉此前的语义信息<code class=\"x-inline-highlight\">女性</code>，并把新的语义信息<code class=\"x-inline-highlight\">男性</code>存入单元状态，使得后文输出正确的代词<code class=\"x-inline-highlight\">他</code>。"}}]}],["$","h4",null,{"className":"x-h3","children":"输出门"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"$1f"}}],["$","$Ld",null,{"src":"lstm6.png","width":"600px","filterDarkTheme":true}],["$","div",null,{"className":"x-highlightblock highlight-background-gray","children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"当看到<code class=\"x-inline-highlight\">Bob是一位男司机，他……</code>时，由于出现了主语<code class=\"x-inline-highlight\">他</code>，模型可能会输出和<code class=\"x-inline-highlight\">谓语动词</code>有关的语义信息。"}}]}]]
c:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"学习RNN和LSTM - 铃木的网络日记"}],["$","link","3",{"rel":"canonical","href":"https://1kuzus.github.io/23d/learn-rnn-lstm/"}]]
1:null
