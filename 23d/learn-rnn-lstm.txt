1:HL["/_next/static/css/8ac0b0e7f508d966.css","style",{"crossOrigin":""}]
0:["jVXPjU0Xb3INthkZD7-vp",[[["",{"children":["(blogs)",{"children":["23d",{"children":["learn-rnn-lstm",{"children":["__PAGE__",{}]}]}]}]},"$undefined","$undefined",true],"$L2",[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/8ac0b0e7f508d966.css","precedence":"next","crossOrigin":""}]],"$L3"]]]]
4:HL["/_next/static/css/c84b60895b980f14.css","style",{"crossOrigin":""}]
5:HL["/_next/static/css/f70c26425a044d6b.css","style",{"crossOrigin":""}]
6:I[9226,["185","static/chunks/app/layout-5f2f1a9df5f5dd44.js"],"GlobalProvider"]
7:I[6954,[],""]
8:I[7264,[],""]
9:I[5652,["176","static/chunks/176-0514793c516ad9fb.js","135","static/chunks/app/(blogs)/layout-43c4c4a3da797474.js"],""]
b:I[4484,["954","static/chunks/d3ac728e-1e5d8b71e3d43fec.js","852","static/chunks/852-c88172e9e90c8c53.js","443","static/chunks/app/(blogs)/23d/learn-rnn-lstm/page-5bf6ead534598de0.js"],"Title"]
c:I[4484,["954","static/chunks/d3ac728e-1e5d8b71e3d43fec.js","852","static/chunks/852-c88172e9e90c8c53.js","443","static/chunks/app/(blogs)/23d/learn-rnn-lstm/page-5bf6ead534598de0.js"],"H1"]
d:I[413,["954","static/chunks/d3ac728e-1e5d8b71e3d43fec.js","852","static/chunks/852-c88172e9e90c8c53.js","443","static/chunks/app/(blogs)/23d/learn-rnn-lstm/page-5bf6ead534598de0.js"],"Image"]
e:I[4484,["954","static/chunks/d3ac728e-1e5d8b71e3d43fec.js","852","static/chunks/852-c88172e9e90c8c53.js","443","static/chunks/app/(blogs)/23d/learn-rnn-lstm/page-5bf6ead534598de0.js"],"H2"]
f:I[9708,["954","static/chunks/d3ac728e-1e5d8b71e3d43fec.js","852","static/chunks/852-c88172e9e90c8c53.js","443","static/chunks/app/(blogs)/23d/learn-rnn-lstm/page-5bf6ead534598de0.js"],""]
10:I[9841,["954","static/chunks/d3ac728e-1e5d8b71e3d43fec.js","852","static/chunks/852-c88172e9e90c8c53.js","443","static/chunks/app/(blogs)/23d/learn-rnn-lstm/page-5bf6ead534598de0.js"],"Oli"]
11:I[9841,["954","static/chunks/d3ac728e-1e5d8b71e3d43fec.js","852","static/chunks/852-c88172e9e90c8c53.js","443","static/chunks/app/(blogs)/23d/learn-rnn-lstm/page-5bf6ead534598de0.js"],"Uli"]
12:I[4484,["954","static/chunks/d3ac728e-1e5d8b71e3d43fec.js","852","static/chunks/852-c88172e9e90c8c53.js","443","static/chunks/app/(blogs)/23d/learn-rnn-lstm/page-5bf6ead534598de0.js"],"H3"]
13:I[848,["954","static/chunks/d3ac728e-1e5d8b71e3d43fec.js","852","static/chunks/852-c88172e9e90c8c53.js","443","static/chunks/app/(blogs)/23d/learn-rnn-lstm/page-5bf6ead534598de0.js"],""]
14:T536,式子中<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">⊗</span></span></span></span>表示按位置相乘，<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mclose">)</span></span></span></span>的每个元素输出范围是<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span>，某个元素接近<code class="x-inline-highlight">1</code>，<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span>对应位置保留的信息就越多，反之就越少。15:T5a3,LSTM的第一步是决定什么应该被遗忘，也就是对上一个<span class="x-inline-strong">单元</span><code class="x-inline-highlight">(cell)</code>状态信息选择性的遗忘。<br/> 这个操作由遗忘门<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>实现，将其<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span>范围的输出按位置与单元上一时刻状态相乘。16:T93e,当模型看到<code class="x-inline-highlight">Alice是一名女教师，……</code>时，单元状态中可能存储了和主语<code class="x-inline-highlight">Alice</code>和<code class="x-inline-highlight">女教师</code>有关的语义信息，以便在后文输出合适的代词<code class="x-inline-highlight">她</code>；然后，当模型看到<code class="x-inline-highlight">Alice是一名女教师，她喜欢给学生讲课；Bob是一位男司机，……</code>时，我们希望在看到新主语<code class="x-inline-highlight">Bob</code>和<code class="x-inline-highlight">男司机</code>之后，忘记此前存储的旧主语的性别语义。也就是对旧单元状态<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8917em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span>乘上较小的<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。17:T4d3,下一步就是决定要在单元中存入什么新的信息。这一部分有两路：<code class="x-inline-highlight">tanh</code>这一路与普通RNN很像，生成一个中间状态；<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span></span>这一路被称为输入门<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8095em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">i</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，控制这个中间状态有多少信息被存入单元。18:T6c6,最后是决定新的隐藏状态，这个输出会基于单元状态，但会经过门控单元。输出门<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>决定经过<code class="x-inline-highlight">tanh</code>的单元状态<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>有多少被输出到下一时刻的隐藏状态。2:[null,["$","html",null,{"lang":"en","children":["$","body",null,{"children":["$","$L6",null,{"children":["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children"],"loading":"$undefined","loadingStyles":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","notFound":[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":"404"}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],"notFoundStyles":[],"childProp":{"current":[null,["$","$L9",null,{"children":["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children","(blogs)","children"],"loading":"$undefined","loadingStyles":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","notFound":[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":"404"}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],"notFoundStyles":[],"childProp":{"current":["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children","(blogs)","children","23d","children"],"loading":"$undefined","loadingStyles":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","childProp":{"current":["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children","(blogs)","children","23d","children","learn-rnn-lstm","children"],"loading":"$undefined","loadingStyles":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","childProp":{"current":["$La",[["$","$Lb",null,{"children":"$undefined"}],["$","$Lc",null,{"children":"RNN：一个简单的例子"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"传统神经网络每次的输入是独立的，每次输出只依赖于当前的输入；但在某些任务中需要更好的处理序列信息，即前面的输入和后面的输入是有关系的；<span class=\"x-inline-strong\">循环神经网络</span><code class=\"x-inline-highlight\">(Recurrent Neural Networks, RNN)</code>通过使用带自反馈的神经元，能够处理任意长度的序列。"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"下面是一个非常常见的RNN结构描述图。它展示了RNN的自反馈机制和与时间的依赖关系，但是对网络结构的描述容易引起误解：右侧的展开形式并不意味着网络有<code class=\"x-inline-highlight\">t</code>层，而是反映了随着时间增加（有时也可以理解为随着程序中循环的迭代），上一次输出的隐藏状态，和<span class=\"katex\"><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>共同作为网络的下一次的输入。"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"或者，如果说CNN是从空间维度上堆叠卷积层，不断加深，RNN就是从时间维度上的延展，而其网络真正的参数是很少的。"}}],["$","div",null,{"className":"x-image-wrapper x-image-invert","children":["$","$Ld",null,{"alt":"img","src":{"default":{"src":"/_next/static/media/rnn1.ad2f936d.png","height":711,"width":2706,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAACCAYAAABllJ3tAAAATElEQVR4nGPcs29L6NsP7/bISEv9+v+Pwe7///9fGRkZeV69f3lETEDcinHb+V3+379+OcDFw/2XiYnF/t//f99ACt48f3ZYWELKGgCk1yEgUvIy7AAAAABJRU5ErkJggg==","blurWidth":8,"blurHeight":2}},"width":"600px"}]}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"下面以一个简单的正弦序列预测任务出发，结合代码理解RNN网络的部分细节。"}}],["$","$Le",null,{"children":"预测一个正弦序列"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"这个例子中，我们对一个加了噪声的正弦序列进行预测。"}}],["$","$Lf",null,{"language":"python","code":"\n                import numpy\n                import torch\n                from torch import nn\n                import matplotlib.pyplot as plt\n\n                #生成加噪声的正弦序列数据\n                xlim=numpy.linspace(0,36,400)\n                y=numpy.sin(xlim)+numpy.random.rand(*xlim.shape)*0.2\n\n                #转换dtype和size，保持和后面的训练数据统一\n                y=y.reshape(-1,1).astype(\"float32\")\n\n                plt.plot(xlim,y)\n                plt.show()\n                "}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"代码中我们在区间<span class=\"katex\"><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord\">0</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">36</span><span class=\"mclose\">]</span></span></span></span>取了<code class=\"x-inline-highlight\">400</code>点数据，如果把横轴看成时间轴，可以认为数据集中有<code class=\"x-inline-highlight\">400</code>个连续时间点的数据。"}}],["$","div",null,{"className":"x-image-wrapper x-image-invert","children":["$","$Ld",null,{"alt":"img","src":{"default":{"src":"/_next/static/media/fig1.2b0c9266.png","height":266,"width":840,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAADBAMAAABc5lN7AAAAGFBMVEX7/P37+/v6+/v6+vz6+vv5+vv5+fv5+fo/pXwpAAAAFUlEQVR42mModnMNYRBgAAKhUKcgABNsAl6NvkirAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":3}},"width":"600px"}]}],["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"现在明确一下我们的方案："}}],["$","$L10",null,{"children":"使用前`80%`也就是前`320`个数据作为训练集，剩余的作为测试集，观察预测结果。"}],["$","$L10",null,{"children":"序列长度为`10`，也就是模型根据前`10`个时间点的数据去预测下一个时间点的数据。"}],["$","$L10",null,{"children":"这个例子中输入特征的维度是`1`，也就是只有`y`值一个指标。此外，也不考虑批量大小。"}],["$","$Le",null,{"children":"定义RNN网络"}],["$","$Lf",null,{"language":"python","code":"\n                class MyRNN(nn.Module):\n                    def __init__(self,input_size,hidden_size,output_size):\n                        super().__init__()\n                        self.linear_ih=nn.Linear(input_size,hidden_size)\n                        self.linear_hh=nn.Linear(hidden_size,hidden_size)\n                        self.linear_ho=nn.Linear(hidden_size,output_size)\n                        self.tanh=nn.Tanh()\n                    def forward(self,x,prev_state):\n                        curr_state=self.tanh(\n                            self.linear_ih(x)+self.linear_hh(prev_state)\n                        )\n                        output=self.linear_ho(curr_state).reshape(1)\n                        return output,curr_state\n\n                hidden_size=12\n                my_rnn=MyRNN(input_size=1,hidden_size=hidden_size,output_size=1)\n                loss_func=nn.MSELoss()\n                optimizer=torch.optim.SGD(my_rnn.parameters(),lr=0.01)\n                "}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"这个是一个简单的RNN结构，从网络参数和结构来看很像一个<code class=\"x-inline-highlight\">输入层-隐藏层-输出层</code>的感知机，但是多了一步<code class=\"x-inline-highlight\">隐藏层-隐藏层</code>的连接，RNN的反馈结构就是由此体现的。"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"并且，注意到<code class=\"x-inline-highlight\">forward</code>函数的输入也需要两个参数：当前时刻输入<code class=\"x-inline-highlight\">x</code>和前一时刻状态<code class=\"x-inline-highlight\">prev_state</code>，同时也会把计算后的新状态<code class=\"x-inline-highlight\">curr_state</code>和<code class=\"x-inline-highlight\">output</code>一起返回，供下一次计算使用。在这里，经过<code class=\"x-inline-highlight\">linear_ho</code>后，<code class=\"x-inline-highlight\">output</code>的<code class=\"x-inline-highlight\">size</code>为<code class=\"x-inline-highlight\">(1,1)</code>，考虑到它仅仅是一个标量，我们把它<code class=\"x-inline-highlight\">resize</code>为<code class=\"x-inline-highlight\">(1)</code>。"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"接下来设置了一些超参数，隐藏层有<code class=\"x-inline-highlight\">12</code>个神经元，损失函数使用<code class=\"x-inline-highlight\">MSELoss()</code>。"}}],["$","$Le",null,{"children":"训练"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"首先定义这样的训练函数：它传入一个序列<code class=\"x-inline-highlight\">train_seq</code>和目标<code class=\"x-inline-highlight\">target</code>。<code class=\"x-inline-highlight\">train_seq</code>的<code class=\"x-inline-highlight\">size</code>应该为<code class=\"x-inline-highlight\">(10,1)</code>，因为我们用前<code class=\"x-inline-highlight\">10</code>个时间点的数据去预测下一个，而输入特征维度是<code class=\"x-inline-highlight\">1</code>；<code class=\"x-inline-highlight\">target</code>的<code class=\"x-inline-highlight\">size</code>应该为<code class=\"x-inline-highlight\">(1)</code>，因为输出只是一个标量。注意我们循环依次输入<code class=\"x-inline-highlight\">train_seq</code>中的<code class=\"x-inline-highlight\">10</code>个数据，迭代更新<code class=\"x-inline-highlight\">state</code>，用最后一次的<code class=\"x-inline-highlight\">output</code>作为最终的输出计算损失。"}}],["$","$Lf",null,{"language":"python","code":"\n                def train(train_seq,target):\n                    #初始状态\n                    state=torch.zeros(1,hidden_size)\n\n                    for x in train_seq:\n                        output,state=my_rnn(x,state)\n\n                    loss=loss_func(output,target)\n                    optimizer.zero_grad()\n                    loss.backward()\n                    optimizer.step()\n\n                    #返回损失\n                    return loss.detach().numpy().reshape(1)\n                "}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"下面代码把真实数据划分成："}}],["$","table",null,{"className":"x-table","children":["$","tbody",null,{"children":[["$","tr",null,{"children":[["$","th",null,{"children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">train_seq</code>"}}]}],["$","th",null,{"children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">target</code>"}}]}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">y[0]</code>,<code class=\"x-inline-highlight\">y[1]</code>,<code class=\"x-inline-highlight\">y[2]</code>, ... ,<code class=\"x-inline-highlight\">y[9]</code>"}}]}],["$","td",null,{"children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">y[10]</code>"}}]}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">y[1]</code>,<code class=\"x-inline-highlight\">y[2]</code>,<code class=\"x-inline-highlight\">y[3]</code>, ... ,<code class=\"x-inline-highlight\">y[10]</code>"}}]}],["$","td",null,{"children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">y[11]</code>"}}]}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">y[2]</code>,<code class=\"x-inline-highlight\">y[3]</code>,<code class=\"x-inline-highlight\">y[4]</code>, ... ,<code class=\"x-inline-highlight\">y[11]</code>"}}]}],["$","td",null,{"children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">y[12]</code>"}}]}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"..."}],["$","td",null,{"children":"..."}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">y[309]</code>,<code class=\"x-inline-highlight\">y[310]</code>,<code class=\"x-inline-highlight\">y[311]</code>, ... ,<code class=\"x-inline-highlight\">y[318]</code>"}}]}],["$","td",null,{"children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">y[319]</code>"}}]}]]}]]}]}],["$","$Lf",null,{"language":"python","code":"\n                train_datas=[]\n                for i in range(320-10):\n                    train_seq=torch.from_numpy(y[i:i+10])\n                    target=torch.from_numpy(y[i+10])\n                    train_datas.append((train_seq,target))\n                "}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"训练网络，绘制误差："}}],["$","$Lf",null,{"language":"python","code":"\n                metrics=[]\n                my_rnn.train()\n                for train_seq,target in train_datas:\n                    loss=train(train_seq,target)\n                    metrics.append(loss)\n\n                plt.subplot(211)\n                plt.plot(metrics,label=\"loss\")\n                plt.legend()\n                "}],["$","$Le",null,{"children":"预测"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"这个例子中我们用<span class=\"x-inline-strong\">单步预测</span>观察模型的效果。在单步预测时，每次预测都全部使用真实值；当然，我们可以这样做是因为验证集中本来就包含了真实的数据，换句话说，我们是在已知<code class=\"x-inline-highlight\">t+1</code>时刻的真实数据的情况下，去看看模型使用<code class=\"x-inline-highlight\">t-9</code>~<code class=\"x-inline-highlight\">t</code>时刻的数据，对<code class=\"x-inline-highlight\">t+1</code>时刻的预测值。"}}],["$","table",null,{"className":"x-table","children":["$","tbody",null,{"children":[["$","tr",null,{"children":[["$","th",null,{"children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">input</code>"}}]}],["$","th",null,{"children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">prediction</code>"}}]}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">y[310]</code>,<code class=\"x-inline-highlight\">y[311]</code>,<code class=\"x-inline-highlight\">y[312]</code>, ... ,<code class=\"x-inline-highlight\">y[319]</code>"}}]}],["$","td",null,{"children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">pred[320]</code>"}}]}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">y[311]</code>,<code class=\"x-inline-highlight\">y[312]</code>,<code class=\"x-inline-highlight\">y[313]</code>, ... ,<code class=\"x-inline-highlight\">y[320]</code>"}}]}],["$","td",null,{"children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">pred[321]</code>"}}]}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">y[312]</code>,<code class=\"x-inline-highlight\">y[313]</code>,<code class=\"x-inline-highlight\">y[314]</code>, ... ,<code class=\"x-inline-highlight\">y[321]</code>"}}]}],["$","td",null,{"children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">pred[322]</code>"}}]}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"..."}],["$","td",null,{"children":"..."}]]}]]}]}],["$","$Lf",null,{"language":"python","code":"\n                def pred(truth_seq):\n                    with torch.no_grad():\n                        state=torch.zeros(1,hidden_size)\n                        for x in truth_seq:\n                            output,state=my_rnn(x,state)\n                        return output\n\n                preds=[]\n                for i in range(320-10,400-10):\n                    truth_seq=torch.from_numpy(y[i:i+10])\n                    preds.append(pred(truth_seq).numpy())\n                preds=numpy.array(preds).reshape(-1,1)\n\n                plt.subplot(212)\n                plt.plot(xlim,y,label=\"truth\")\n                plt.plot(xlim[320:400],preds,\"red\",label=\"predict\")\n                plt.legend()\n                plt.show()\n                "}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"如果我们不只是在测试集上评估模型性能，而是去预测真实生活中的问题，例如未来<code class=\"x-inline-highlight\">7</code>天的温度；或者假如我们的数据集到<code class=\"x-inline-highlight\">y[319]</code>就截止了，这时如果想得到后面多个时刻的数据，就需要<span class=\"x-inline-strong\">多步预测</span>，此时上一时刻的预测会被当做新的输入："}}],["$","table",null,{"className":"x-table","children":["$","tbody",null,{"children":[["$","tr",null,{"children":[["$","th",null,{"children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">input</code>"}}]}],["$","th",null,{"children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">prediction</code>"}}]}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">y[310]</code>,<code class=\"x-inline-highlight\">y[311]</code>,<code class=\"x-inline-highlight\">y[312]</code>, ... ,<code class=\"x-inline-highlight\">y[319]</code>"}}]}],["$","td",null,{"children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">pred[320]</code>"}}]}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">y[311]</code>,<code class=\"x-inline-highlight\">y[312]</code>,<code class=\"x-inline-highlight\">y[313]</code>, ... ,<code class=\"x-inline-highlight\">pred[320]</code>"}}]}],["$","td",null,{"children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">pred[321]</code>"}}]}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">y[312]</code>,<code class=\"x-inline-highlight\">y[313]</code>,<code class=\"x-inline-highlight\">y[314]</code>, ... ,<code class=\"x-inline-highlight\">pred[320]</code>,<code class=\"x-inline-highlight\">pred[321]</code>"}}]}],["$","td",null,{"children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">pred[322]</code>"}}]}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"..."}],["$","td",null,{"children":"..."}]]}]]}]}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"多步预测会导致误差的累积。"}}],["$","$Le",null,{"children":"看看效果"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"如果不执行训练步骤的代码，使用初始随机参数的模型预测结果是："}}],["$","div",null,{"className":"x-image-wrapper x-image-invert","children":["$","$Ld",null,{"alt":"img","src":{"default":{"src":"/_next/static/media/fig2.51ee0782.png","height":261,"width":840,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAACBAMAAACXuoDeAAAAGFBMVEX7+/z6+/z6+vv6+vr5+vr6+fr5+fr5+fkEDyOPAAAAEklEQVR42mNwFBRUZShjVAwGAAagAXSZPcFiAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":2}},"width":"600px"}]}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"经过训练后，每次训练的<code class=\"x-inline-highlight\">loss</code>和最终的预测："}}],["$","div",null,{"className":"x-image-wrapper x-image-invert","children":["$","$Ld",null,{"alt":"img","src":{"default":{"src":"/_next/static/media/fig3.6f6f5b35.png","height":543,"width":840,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAMAAABPT11nAAAAQlBMVEX///7//v7+/v79/f38/f38/P38/Pz7/P36+/z8+vr7+vv6+vv5+vv5+vr6+fr6+Pn4+fn5+Pj4+Pn3+Pn2+Pr29/mY1zi4AAAALklEQVR42g3BhQEAIAwDsDIYVlz+fxUSVIWIGA+eoqrOgvXub4DhczlhrsjC3h4eogGS5iURrwAAAABJRU5ErkJggg==","blurWidth":8,"blurHeight":5}},"width":"600px"}]}],["$","$Le",null,{"children":"使用torch.nn.RNN"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"使用<code class=\"x-inline-highlight\">torch.nn.RNN</code>模块时，与上面例子中手动实现的RNN有几处细小的区别，下面给出了使用<code class=\"x-inline-highlight\">torch.nn.RNN</code>时需要做出的修改："}}],["$","$L10",null,{"reset":true,"children":[["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"定义模型时，不再需要显式指定<code class=\"x-inline-highlight\">linear_ih</code>和<code class=\"x-inline-highlight\">linear_hh</code>两层，将由<code class=\"x-inline-highlight\">nn.RNN</code>模块实现；<code class=\"x-inline-highlight\">nn.RNN</code>模块没有定义输出层，因此输出层<code class=\"x-inline-highlight\">linear_ho</code>需要设置。<br/> 在<code class=\"x-inline-highlight\">forward</code>函数中，手动实现时为了直观展示出RNN的迭代过程，只进行了一次隐藏状态的更新；而对于输入序列迭代更新隐藏状态是在训练和预测时实现的。而<code class=\"x-inline-highlight\">nn.RNN</code>模块的一次<code class=\"x-inline-highlight\">forward</code>就已经完成了迭代更新，其输入是整个序列<code class=\"x-inline-highlight\">seq</code>和<code class=\"x-inline-highlight\">prev_state</code>，返回值是<code class=\"x-inline-highlight\">output_hidden,curr_state</code>，对于不考虑批量大小的数据，它们的<code class=\"x-inline-highlight\">size</code>为："}}],["$","$L11",null,{"children":"`seq`: `(sequence_length, input_size)`"}],["$","$L11",null,{"children":"`init_state`: `(1, hidden_size)`"}],["$","$L11",null,{"children":"`output_hidden`: `(sequence_length, hidden_size)`"}],["$","$L11",null,{"children":"`state`: `(1, hidden_size)`"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"最后在我们定义的<code class=\"x-inline-highlight\">MyRNN</code>模块中，用<code class=\"x-inline-highlight\">output_hidden</code>的最后一个时间点的输出，经过输出层得到最终的<code class=\"x-inline-highlight\">output</code>。"}}]]}],["$","div",null,{"className":"x-highlightblock highlight-background-gray","children":[["$","$L12",null,{"children":"手动实现"}],["$","$Lf",null,{"language":"python","code":"\n                    class MyRNN(nn.Module):\n                        def __init__(self,input_size,hidden_size,output_size):\n                            super().__init__()\n                            self.linear_ih=nn.Linear(input_size,hidden_size)\n                            self.linear_hh=nn.Linear(hidden_size,hidden_size)\n                            self.linear_ho=nn.Linear(hidden_size,output_size)\n                            self.tanh=nn.Tanh()\n                        def forward(self,x,prev_state):\n                            curr_state=self.tanh(\n                                self.linear_ih(x)+self.linear_hh(prev_state)\n                            )\n                            output=self.linear_ho(curr_state).reshape(1)\n                            return output,curr_state\n                    "}],["$","$L12",null,{"children":"使用torch.nn.RNN"}],["$","$Lf",null,{"language":"python","code":"\n                    class MyRNN(nn.Module):\n                        def __init__(self,input_size,hidden_size,output_size):\n                            super().__init__()\n                            self.rnn=nn.RNN(input_size=input_size,hidden_size=hidden_size,batch_first=True)\n                            self.linear_ho=nn.Linear(hidden_size,output_size)\n                        def forward(self,seq,init_state):\n                            output_hidden,tate=self.rnn(seq,init_state)\n                            output=self.linear_ho(output_hidden[-1,:]) #取最后一个时间点的输出\n                            return output,state\n                    "}]]}],["$","$L10",null,{"children":"训练和预测时，也不需要再遍历序列，迭代的过程已经在`nn.RNN`模块内部实现。"}],["$","div",null,{"className":"x-highlightblock highlight-background-gray","children":[["$","$L12",null,{"children":"手动实现"}],["$","$Lf",null,{"language":"python","code":"\n                    def train(train_seq,target):\n                        state=torch.zeros(1,hidden_size)\n                        for x in train_seq:\n                            output,state=my_rnn(x,state)\n                        loss=loss_func(output,target)\n                        # ......\n                    "}],["$","$Lf",null,{"language":"python","code":"\n                    def pred(truth_seq,net):\n                        with torch.no_grad():\n                            state=torch.zeros(1,hidden_size)\n                            for x in truth_seq:\n                                output,state=net(x,state)\n                            return output\n                    "}],["$","$L12",null,{"children":"使用torch.nn.RNN"}],["$","$Lf",null,{"language":"python","code":"\n                    def train(train_seq,target):\n                        state=torch.zeros(1,hidden_size)\n                        output,_=my_rnn(train_seq,state) #一次得到输出\n                        loss=loss_func(output,target)\n                        # ......\n                    "}],["$","$Lf",null,{"language":"python","code":"\n                    def pred(truth_seq):\n                        with torch.no_grad():\n                            state=torch.zeros(1,hidden_size)\n                            output,_=my_rnn(truth_seq,state) #一次得到输出\n                            return output\n                    "}]]}],["$","$Le",null,{"children":"FAQ"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"初次了解RNN时，我在一些问题上困惑了很久。这个版块是对它们的再次整理。（尽管有些已经包含在上述例子中了！）"}}],["$","$L12",null,{"children":"用10步预测下1步，为什么 input_size 不是10，而是1？"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">input_size</code>与序列长度并非同一个概念。用前<code class=\"x-inline-highlight\">10</code>个时间点的数据去预测下一个，这里的<code class=\"x-inline-highlight\">10</code>是序列长度；而<code class=\"x-inline-highlight\">input_size</code>是输入特征的维度。由于这个例子较为简单，只是用历史的<code class=\"x-inline-highlight\">y</code>值预测新的<code class=\"x-inline-highlight\">y</code>值，因此特征只有<code class=\"x-inline-highlight\">1</code>维。"}}],["$","$L12",null,{"children":"什么时候 input_size 不是1？"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"例如我们在预测未来气温时，历史气温数据并不是唯一的参考，还可能参考历史的风速、气压、天气情况等等，此时输入数据将会是一个<code class=\"x-inline-highlight\">input_size</code>维的向量。"}}],["$","$L12",null,{"children":"hidden_size=12，12是在哪里体现的？"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<code class=\"x-inline-highlight\">12</code>只是模型的超参数，和MLP中隐藏层大小一样，并没有太多的物理含义。"}}],["$","$L12",null,{"children":"训练时，每次迭代用哪些数据？应该遍历几遍数据集？每个 epoch 会使用哪些数据进行参数优化？"}],["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"在训练一个CNN网络时（例如一个图片分类网络），策略通常是："}}],["$","$L10",null,{"reset":true,"children":"指定超参数`num_epoch`，在每个`epoch`中随机遍历训练集中的所有图像进行参数优化；"}],["$","$L10",null,{"children":"重复执行`num_epoch`次。"}],["$","p",null,{"className":"x-p with-margin-top no-margin-bottom","dangerouslySetInnerHTML":{"__html":"然而对于RNN来说这个概念似乎并不清晰，例如上述例子的训练策略是："}}],["$","$L10",null,{"reset":true,"children":"从`0`到`(训练集大小 - 序列长度)`依次遍历起始时间`t`；"}],["$","$L10",null,{"children":"对于每个起始时间`t`，将`y[t]`~`y[t+9]`为输入，`y[t+10]`为真值作为一组训练样本。"}],["$","$L10",null,{"children":"第`1`步只遍历了一次！"}],["$","p",null,{"className":"x-p with-margin-top no-margin-bottom","dangerouslySetInnerHTML":{"__html":"或者："}}],["$","p",null,{"className":"x-p no-margin-bottom","dangerouslySetInnerHTML":{"__html":"..."}}],["$","$L10",null,{"reset":3,"children":"前两步同上，但多次遍历训练集。"}],["$","p",null,{"className":"x-p with-margin-top no-margin-bottom","dangerouslySetInnerHTML":{"__html":"另一个常用的策略是："}}],["$","$L10",null,{"reset":true,"children":"指定超参数：训练轮次`num_iter`；"}],["$","$L10",null,{"children":"重复执行`num_iter`次，每次随机抽取一个起始时间`t`，并且将`y[t]`~`y[t+9]`为输入，`y[t+10]`为真值作为一组训练样本。"}],["$","$Lc",null,{"children":"LSTM：一篇很好的博客"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"以下的内容和插图总结或翻译自这篇的英文博客：<a href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/\" target=\"_blank\" rel=\"noreferrer\" class=\"x-inline-link\">Understanding LSTM Networks</a>"}}],["$","$Le",null,{"children":"长期依赖问题"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"RNN可以利用先前的信息理解当前的任务，这点非常不错；有时我们只需要短期的信息，例如一个语言模型预测下面的句子：<br/> <code class=\"x-inline-highlight\">天空中飘着一朵白色的【云】</code>，这很简单。但有些时候我们需要更多背景信息，例如：<br/><code class=\"x-inline-highlight\">我出生在法国，…… ，我可以说流利的【法语】</code>，这个情况下，随着前后文距离变大，RNN对长期依赖关系的学习会变得困难。"}}],["$","$Le",null,{"children":"LSTM"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"<span class=\"x-inline-strong\">长短期记忆网络</span><code class=\"x-inline-highlight\">(Long Short-Term Memory, LSTM)</code>是一种特殊的RNN，可以学习长期依赖。以RNN为例，循环神经网络随时间展开通常具有如下的示意图："}}],["$","div",null,{"className":"x-image-wrapper x-image-invert","children":["$","$Ld",null,{"alt":"img","src":{"default":{"src":"/_next/static/media/rnn2.d1dd71bb.png","height":839,"width":2242,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAADCAYAAACuyE5IAAAAbklEQVR4nAFjAJz/Adv0yjT47v0Q7+P3/SA6D+/iyvEcBg4D6RMdCgfm0fURAdzxyv8AAQIA/P37uAH+/0gBAwLsAAAA1wIBAz0AAf/EAcffxmoaFwTg3+L6BA4MARASEQfn3+P5EhkWAgEBAAHidwIqvbX06uAAAAAASUVORK5CYII=","blurWidth":8,"blurHeight":3}},"width":"600px"}]}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"对于RNN来说，利用历史状态和输入得到新的状态，只经过一个简单的<code class=\"x-inline-highlight\">tanh</code>激活层，而对于LSTM来说，它的示意图略显复杂："}}],["$","div",null,{"className":"x-image-wrapper x-image-invert","children":["$","$Ld",null,{"alt":"img","src":{"default":{"src":"/_next/static/media/lstm1.83284ab3.png","height":839,"width":2233,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAADCAYAAACuyE5IAAAAbklEQVR4nAFjAJz/AdjwxzP47/0Q8OP5/h85Ee7mz/IdAwr/6BQgCwXn0PUUAdvxyv8AAAEA+vj2uwD8/UX9/gDvBQoH1AQEBj3///7MAcffxmwaFwTh29z0BQsH+QwNDgfo6O4EFB4aB//8/P3nf8QzvBHgmCgAAAAASUVORK5CYII=","blurWidth":8,"blurHeight":3}},"width":"600px"}]}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"在上图中，每条线表示一个向量，粉红色圆圈表示逐点式操作，黄色的方框是神经网络的层。这看起来很眼晕，不过我们接下来会一点点的解释图里的内容。"}}],["$","$Le",null,{"children":"门控单元"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"下面的结构称为门控单元："}}],["$","div",null,{"className":"x-image-wrapper x-image-invert","children":["$","$Ld",null,{"alt":"img","src":{"default":{"src":"/_next/static/media/lstm2.96d0a9a3.png","height":242,"width":198,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAcAAAAICAYAAAA1BOUGAAAAoklEQVR42mNQlpb2YGBgcK1OS4u9tX37/3UTJzYB+fYmWlqeDDrKyoJADn9nUZH4ieXLO7fNmKEL5PM4mpoKMcBATTcDK5BaDsSyDEiA8fmjfkYoOxAhLMLI8P//HrDEzctduUf31+y7frGjCchlYgACoOReMOPu9Z5DV8+3/39wq+8ekMvKgAxcnbXYHOxUI60slIRQJMJDTGB2egIxM0wcAHV1L781GzudAAAAAElFTkSuQmCC","blurWidth":7,"blurHeight":8}},"width":"100px"}]}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"门控单元控制信息量通过的多少，通过向量<span class=\"katex\"><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.04398em;\">z</span></span></span></span>来控制<span class=\"katex\"><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">x</span></span></span></span>通过的信息量："}}],["$","$L13",null,{"text":"o=\\sigma(z) \\otimes x"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"$14"}}],["$","$Le",null,{"children":"逐部分分析LSTM"}],["$","$L12",null,{"children":"遗忘门"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"$15"}}],["$","div",null,{"className":"x-image-wrapper x-image-invert","children":["$","$Ld",null,{"alt":"img","src":{"default":{"src":"/_next/static/media/lstm3.ac9b9ff5.png","height":564,"width":1826,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAACCAYAAABllJ3tAAAATUlEQVR4nGO8/eFkDjMzcyIDIyP7v7//Wvj4+FZsXreDPTe+/Pe3Lz8YGC8+3lPLzcebzczMyvzv378SIV7BhStXrucoTK7+/f3rz/8AtBQfB6GMnsQAAAAASUVORK5CYII=","blurWidth":8,"blurHeight":2}},"width":"600px"}]}],["$","div",null,{"className":"x-highlightblock highlight-background-gray","children":[["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"举一个概念性的例子："}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"考虑一个语言模型，输入一个句子：<code class=\"x-inline-highlight\">Alice是一名女教师，她喜欢给学生讲课；Bob是一位男司机，他喝酒上瘾。</code>"}}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"$16"}}]]}],["$","$L12",null,{"children":"输入门"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"$17"}}],["$","div",null,{"className":"x-image-wrapper x-image-invert","children":["$","$Ld",null,{"alt":"img","src":{"default":{"src":"/_next/static/media/lstm4.00cef2df.png","height":564,"width":1826,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAACCAYAAABllJ3tAAAATUlEQVR4nGO8/+Vc1r+/f+MZmZjY//371ybDJ7Wqs38SV0tF76/fv/78Z7zy4nA9E8PfXA4ubub/jEyFSjyaCypaynl7m6b9/PP7738AfRAiC3RDuCcAAAAASUVORK5CYII=","blurWidth":8,"blurHeight":2}},"width":"600px"}]}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"经历这两步之后，便可以相加得到新的单元状态："}}],["$","div",null,{"className":"x-image-wrapper x-image-invert","children":["$","$Ld",null,{"alt":"img","src":{"default":{"src":"/_next/static/media/lstm5.3f95c1ad.png","height":564,"width":1826,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAACCAYAAABllJ3tAAAATUlEQVR4nAFCAL3/AdToxGkDAgJeAwQDB/n6+ogXFxaqxcHIBVNCYP3+/v/+AdrvyX///v9uBAUEA////3EWDhWfsbS2B2VXcfz49vn9JsEjA+HlngsAAAAASUVORK5CYII=","blurWidth":8,"blurHeight":2}},"width":"600px"}]}],["$","div",null,{"className":"x-highlightblock highlight-background-gray","children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"同理，当模型看到<code class=\"x-inline-highlight\">Bob是一位男司机</code>时，我们可能会想丢掉此前的语义信息<code class=\"x-inline-highlight\">女性</code>，并把新的语义信息<code class=\"x-inline-highlight\">男性</code>存入单元状态，使得后文输出正确的代词<code class=\"x-inline-highlight\">他</code>。"}}]}],["$","$L12",null,{"children":"输出门"}],["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"$18"}}],["$","div",null,{"className":"x-image-wrapper x-image-invert","children":["$","$Ld",null,{"alt":"img","src":{"default":{"src":"/_next/static/media/lstm6.7eca3c69.png","height":564,"width":1826,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAACCAYAAABllJ3tAAAATUlEQVR4nAFCAL3/Ad70zWgCAQFd+fb5B/n5+okYFxWrk4ycCYh/kvv7+vz+AdTpxIAHBwVv/Pn6A/j5/HAoHSWegYWKDIl9kvn//wD8vColBmKlkBoAAAAASUVORK5CYII=","blurWidth":8,"blurHeight":2}},"width":"600px"}]}],["$","div",null,{"className":"x-highlightblock highlight-background-gray","children":["$","p",null,{"className":"x-p","dangerouslySetInnerHTML":{"__html":"当看到<code class=\"x-inline-highlight\">Bob是一位男司机，他……</code>时，由于出现了主语<code class=\"x-inline-highlight\">他</code>，模型可能会输出和<code class=\"x-inline-highlight\">谓语动词</code>有关的语义信息。"}}]}]],null],"segment":"__PAGE__"},"styles":[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/f70c26425a044d6b.css","precedence":"next","crossOrigin":""}]]}],"segment":"learn-rnn-lstm"},"styles":[]}],"segment":"23d"},"styles":[]}],"params":{}}],null],"segment":"(blogs)"},"styles":[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/c84b60895b980f14.css","precedence":"next","crossOrigin":""}]]}]}]}]}],null]
3:[["$","meta","0",{"charSet":"utf-8"}],["$","title","1",{"children":"Create Next App"}],["$","meta","2",{"name":"description","content":"Generated by create next app"}],["$","meta","3",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","link","4",{"rel":"icon","href":"/favicon.ico","type":"image/x-icon","sizes":"16x16"}]]
a:null
