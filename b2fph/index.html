<!doctype html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><script async src="https://www.googletagmanager.com/gtag/js?id=G-45BYSZ6WPY"></script><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-45BYSZ6WPY")</script><title>Blogs</title><script defer="defer" src="/static/js/main.a638868c.js"></script><link href="/static/css/main.68423a56.css" rel="stylesheet"></head><body><noscript><div><h1>「模式识别」线性学习器与线性分类器</h1><h2>背景知识</h2><h3>Sigmoid函数</h3><p>Sigmoid函数通常表示为$[backslash]sigma(x)=[backslash]frac{1}{1+e^{-x}}$，值域为$[0,1]$，有以下两个常用的性质：</p>\sigma(x)+\sigma(-x)=1\sigma'(x)=\sigma(x)\cdot\sigma(-x)=\sigma(x)\cdot[1-\sigma(x)]<h3>常数问题</h3><p>你可能看到过$y=[backslash]bm{W}^T[backslash]bm{X}+b$和$y=[backslash]bm{W}^T[backslash]bm{X}$两种计算分类结果的方式。以二维空间上的分类为例，前者的分类线可以是任意直线，而后者只能是过原点的直线。事实上，可以通过将问题空间升一个维度来统一两种形式。</p><p>令$[backslash]bm{W} [backslash]leftarrow [[backslash]bm{W} [backslash]; b]$，$[backslash]bm{X} [backslash]leftarrow [[backslash]bm{X} [backslash]; 1]$，两种形式即可统一。</p><p>上述操作相当于，仍然以二维空间上的分类为例，若希望分类线不受过原点的限制，考虑把问题放入三维空间：<br/>$xy$坐标不变，令$z=1$，此时可以在三维空间中用一过原点的平面作为分类面，这个分类面与平面$z=1$的交线可以是任意（平面$z=1$上的）直线。</p><h2>线性回归</h2><h3>动机</h3><p>用下式：</p>y=w_1x_1+w_2x_2 \dots w_dx_d=\bm{W}^T\bm{X}<p>来估计带标签样本$([backslash]bm{X}_i,y_i)$</p><h3>损失函数</h3><p>线性回归模型使用最小平方误差：</p>\min L(\bm{W})=\frac{1}{N}\sum_{i=1}^N(\bm{W}^T\bm{X}_i-y_i)^2<p>记$[backslash]bm{[backslash]chi}_{(N [backslash]times d)}=[[backslash]bm{X}_1, [backslash]bm{X}_2, [backslash]dots, [backslash]bm{X}_N]^T$，$[backslash]bm{y}_{(N [backslash]times 1)}=[y_1, y_2, [backslash]dots, y_N]^T$，可以将损失写作：</p><div>\min L(\bm{W})=\frac{1}{N}(\bm{\chi W}-\bm{y})^T(\bm{\chi W}-\bm{y})</div><h3>最优解</h3><div>\bm{W}^*=(\bm{\chi}^T\bm{\chi})^{-1}\bm{\chi}^T\bm{y}</div><h2>Fisher线性判别</h2><h3>动机</h3><p>线性二分类问题都可以看作把样本投影到某个方向上，再在这个一维空间中确定一个阈值，将两类样本分开。现在希望找到的方向满足：两类间的距离尽可能大，但每一类内部的样本尽可能聚集。</p><h3>准则函数</h3><p>假设两类问题中每类的均值向量分别为$[backslash]bm{m}_1$和$[backslash]bm{m}_2$，定义类内离散度矩阵：</p>\bm{S}_w=\bm{S}_1+\bm{S}_2=\sum_{\bm{X} \in \bm{\chi}_1}(\bm{X}-\bm{m}_1)(\bm{X}-\bm{m}_1)^T+\sum_{\bm{X} \in \bm{\chi}_2}(\bm{X}-\bm{m}_2)(\bm{X}-\bm{m}_2)^T<p>类间离散度矩阵：</p>\bm{S}_b=(\bm{m}_1-\bm{m}_2)(\bm{m}_1-\bm{m}_2)^T<p>判别准则为</p><div>\max J_F(\bm{W})=\frac{\bm{W}^T\bm{S}_b\bm{W}}{\bm{W}^T\bm{S}_w\bm{W}}</div><h3>最优解</h3><p>Fisher判别准则下的最优投影方向为：</p><div>\bm{W}^*=\bm{S}_w^{-1}(\bm{m}_1-\bm{m}_2)</div><p>确定了方向后，分类的阈值可以取两类均值的中心的投影，即：</p><div>threshold=\bm{W}^{*T}\frac{\bm{m}_1+\bm{m}_2}{2}</div><h2>感知器</h2><h3>动机</h3><p>感知器用$[backslash]bm{W}^T[backslash]bm{X}$的正负来判断样本的分类。</p><h3>损失函数</h3><p>感知器的损失函数可以理解为错分样本的个数，数学形式如下：</p><div>\min L(\bm{W})=\frac{1}{N}\sum_{i=1}^Nl(i)，其中l(i)=\begin{cases}1, \quad \text{sign}(y_i\bm{W}^T\bm{X_i}) < 0 \\0, \quad 其他\end{cases}</div><h3>更新规则</h3><p>如果所有样本都分类正确，算法结束；否则对于分错的样本$([backslash]bm{X}_i,y_i)$执行：</p>\bm{W}_{t+1}=\bm{W}_t+y_i\bm{X}_i<h2>两类Logistic回归</h2><h3>动机</h3><p>使用Sigmoid函数将$[backslash]bm{W}^T[backslash]bm{X}$归约到区间$[0,1]$，可以把这个输出结果看作概率，那么输出将从单纯的以$[backslash]bm{W}^T[backslash]bm{X}$的符号进行二分类变成连续的概率。</p><h3>损失函数</h3><p>考虑最大似然估计，样本集出现的概率为：</p>P=\prod_1^NP(y_i|\bm{X}_i)<p>使用对数似然函数$[backslash]ln P=[backslash]sum_{i=1}^N[backslash]ln P(y_i|[backslash]bm{X}_i)$，考虑单个样本出现的概率：</p><p>假设Sigmoid函数的输出是$p$，其含义应为“样本是正类”的概率。因此一个正样本对似然函数的贡献是$p$，而负样本要代入$1-p$。换句话说，上述对数似然函数中的$P(y_i|[backslash]bm{X}_i)$可以有如下数学表示：</p>P(y_i|\bm{X}_i)=\begin{cases}\sigma(\bm{W}^T\bm{X_i}), \quad y_i=1 \\1-\sigma(\bm{W}^T\bm{X_i}), \quad y_i=-1\end{cases}<p>考虑到Sigmoid函数的性质$1-\sigma(x)=\sigma(-x)$，上式可以统一写作：</p>P(y_i|\bm{X}_i)=\sigma(y_i\bm{W}^T\bm{X_i})<p>希望似然函数取最大，那么定义损失函数为如下形式：</p>\min L(\bm{W})=-\frac{1}{N}\ln P=-\frac{1}{N}\sum_{i=1}^N\ln \sigma(y_i\bm{W}^T\bm{X_i})<p>进而化简为：</p><div>\min L(\bm{W})=\frac{1}{N}\sum_{i=1}^N\ln(1+e^{-y_i\bm{W}^T\bm{X_i}})</div><h3>更新规则</h3><p>使用梯度下降法，计算出梯度：</p>\nabla L=-\frac{1}{N}\sum_{i=1}^N\sigma(-y_i\bm{W}^T\bm{X_i}) \cdot y_i\bm{X_i}=-\frac{1}{N}\sum_{i=1}^N\frac{y_i\bm{X_i}}{1+e^{y_i\bm{W}^T\bm{X_i}}}<p>然后按照学习率$\eta$更新下一时刻参数：</p>\bm{W}_{t+1}=\bm{W}_t-\eta\nabla L</div></noscript><div id="root"></div></body></html>